{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement\n",
    "In this notebook, we are going to be focusing on the QAgent and the PQAgent in the reinforcement module in PAI-Utils."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from paiutils import reinforcement as rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be testing with the Taxi-v3 environment. For more information on this environment, click [this](https://gym.openai.com/envs/Taxi-v3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Discrete(500) Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "genv = gym.make('Taxi-v3')\n",
    "max_steps = genv._max_episode_steps\n",
    "print(max_steps)\n",
    "print(genv.observation_space, genv.action_space)\n",
    "\n",
    "env = rl.GymWrapper(genv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = rl.StochasticPolicy(\n",
    "    rl.GreedyPolicy(), rl.ExponentialDecay(1, 0.001, 0.1),\n",
    "    0, env.action_size\n",
    ")\n",
    "discounted_rate = .9\n",
    "agent = rl.QAgent(\n",
    "    env.discrete_state_space, env.action_size,\n",
    "    policy, discounted_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_playing_data(\n",
    "    training=True, learning_rate=.5, verbose=False\n",
    ")\n",
    "num_episodes = 10000\n",
    "result = env.play_episodes(\n",
    "    agent, num_episodes, max_steps,\n",
    "    verbose=False, episode_verbose=False,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "save_dir = ''\n",
    "path = agent.save(save_dir, note=f'QAgent_{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: |\u001b[42m_\u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "agent.set_playing_data(training=False)\n",
    "result = env.play_episodes(\n",
    "    agent, 1, max_steps,\n",
    "    verbose=False, episode_verbose=False,\n",
    "    render=True\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Trials\n",
    "By testing the agent once we are not guaranteed a true representation of how good the agent is. Therefore, we will conduct mutliple trials of 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 7.914860000000001 - Median: 7.91 - Std Dev.: 0.2511067111807249 - Max: 8.6 - Min: 7.21\n"
     ]
    }
   ],
   "source": [
    "num_trials = 1000\n",
    "num_episodes = 100\n",
    "results = []\n",
    "for _ in range(num_trials):\n",
    "    result = env.play_episodes(\n",
    "        agent, num_episodes, max_steps,\n",
    "        verbose=False, episode_verbose=False,\n",
    "        render=False\n",
    "    )\n",
    "    results.append(result)\n",
    "print(f'Mean: {np.mean(results)} - '\n",
    "      f'Median: {np.median(results)} - '\n",
    "      f'Std Dev.: {np.std(results)} - '\n",
    "      f'Max: {np.max(results)} - '\n",
    "      f'Min: {np.min(results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = rl.StochasticPolicy(\n",
    "    rl.GreedyPolicy(), rl.ExponentialDecay(1, .1, .3),\n",
    "    0, env.action_size\n",
    ")\n",
    "discounted_rates = [.9, .8, .91, .92, .93, .94, .95, .96, .97, .98, .99]\n",
    "learning_rates = [.4, .1, .2]\n",
    "agent = rl.PQAgent(\n",
    "    env.discrete_state_space, env.action_size,\n",
    "    policy, discounted_rates, learning_rates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_playing_data(\n",
    "    training=True, verbose=False\n",
    ")\n",
    "num_episodes = 20000\n",
    "result = env.play_episodes(\n",
    "    agent, num_episodes, max_steps,\n",
    "    verbose=False, episode_verbose=False,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "save_dir = ''\n",
    "path = agent.save(save_dir, note=f'PQAgent_{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Step: 1 - Reward: -1 - Action: 4\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "Step: 2 - Reward: -1 - Action: 0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Step: 3 - Reward: -1 - Action: 0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Step: 4 - Reward: -1 - Action: 0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Step: 5 - Reward: -1 - Action: 0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Step: 6 - Reward: 20 - Action: 5\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "agent.set_playing_data(training=False)\n",
    "result = env.play_episodes(\n",
    "    agent, 1, max_steps,\n",
    "    verbose=False, episode_verbose=True,\n",
    "    render=True\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Trials\n",
    "By testing the agent once we are not guaranteed a true representation of how good the agent is. Therefore, we will conduct mutliple trials of 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "Mean: 7.898299999999998 - Median: 7.93 - Std Dev.: 0.24607338336358114 - Max: 8.49 - Min: 7.26\n",
      "0 1\n",
      "Mean: -64.0718 - Median: -63.845 - Std Dev.: 9.903752054650802 - Max: -36.93 - Min: -86.89\n",
      "0 2\n",
      "Mean: -3.3857999999999997 - Median: -2.2249999999999996 - Std Dev.: 4.727937008886646 - Max: 4.07 - Min: -17.21\n",
      "1 0\n",
      "Mean: 7.482500000000001 - Median: 7.855 - Std Dev.: 1.0020113522311012 - Max: 8.72 - Min: 3.89\n",
      "1 1\n",
      "Mean: -148.78570000000002 - Median: -149.26 - Std Dev.: 9.365496169984803 - Max: -113.33 - Min: -170.45\n",
      "1 2\n",
      "Mean: -93.8293 - Median: -93.065 - Std Dev.: 11.819510671343377 - Max: -65.72 - Min: -128.48\n",
      "2 0\n",
      "Mean: 7.960599999999999 - Median: 7.925 - Std Dev.: 0.22539662819128417 - Max: 8.43 - Min: 7.46\n",
      "2 1\n",
      "Mean: -49.073199999999986 - Median: -48.44 - Std Dev.: 8.442863954843759 - Max: -24.47 - Min: -70.51\n",
      "2 2\n",
      "Mean: 1.9801000000000004 - Median: 1.815 - Std Dev.: 3.3045815151089863 - Max: 8.29 - Min: -8.61\n",
      "3 0\n",
      "Mean: 7.9664 - Median: 7.97 - Std Dev.: 0.23389536121949916 - Max: 8.55 - Min: 7.4\n",
      "3 1\n",
      "Mean: -27.946100000000005 - Median: -28.785 - Std Dev.: 6.90012476626329 - Max: -14.04 - Min: -45.52\n",
      "3 2\n",
      "Mean: 6.2067 - Median: 6.215 - Std Dev.: 1.9401278591886668 - Max: 8.56 - Min: 1.44\n",
      "4 0\n",
      "Mean: 7.9155999999999995 - Median: 7.875 - Std Dev.: 0.24119834161950615 - Max: 8.46 - Min: 7.31\n",
      "4 1\n",
      "Mean: -14.8486 - Median: -14.629999999999999 - Std Dev.: 7.484673141827904 - Max: 0.01 - Min: -37.49\n",
      "4 2\n",
      "Mean: 7.941999999999998 - Median: 7.945 - Std Dev.: 0.24186773244895646 - Max: 8.56 - Min: 7.36\n",
      "5 0\n",
      "Mean: 7.954299999999998 - Median: 7.975 - Std Dev.: 0.25143291351770164 - Max: 8.53 - Min: 7.29\n",
      "5 1\n",
      "Mean: -6.130699999999999 - Median: -6.48 - Std Dev.: 5.168221600318624 - Max: 4.59 - Min: -21.1\n",
      "5 2\n",
      "Mean: 7.921900000000002 - Median: 7.945 - Std Dev.: 0.2648308705570405 - Max: 8.62 - Min: 7.17\n",
      "6 0\n",
      "Mean: 7.949899999999999 - Median: 7.955 - Std Dev.: 0.2718694355752408 - Max: 8.59 - Min: 7.39\n",
      "6 1\n",
      "Mean: 4.001900000000001 - Median: 3.895 - Std Dev.: 2.8217624616540635 - Max: 8.61 - Min: -4.43\n",
      "6 2\n",
      "Mean: 7.9314 - Median: 7.95 - Std Dev.: 0.23574995227995277 - Max: 8.76 - Min: 7.4\n",
      "7 0\n",
      "Mean: 7.9011000000000005 - Median: 7.89 - Std Dev.: 0.26057971908803645 - Max: 8.52 - Min: 7.32\n",
      "7 1\n",
      "Mean: 7.5367 - Median: 7.925 - Std Dev.: 1.0497981282132294 - Max: 8.72 - Min: 3.45\n",
      "7 2\n",
      "Mean: 7.9487999999999985 - Median: 7.96 - Std Dev.: 0.2560713962940805 - Max: 8.47 - Min: 7.34\n",
      "8 0\n",
      "Mean: 7.916099999999999 - Median: 7.945 - Std Dev.: 0.2592253652712249 - Max: 8.47 - Min: 7.28\n",
      "8 1\n",
      "Mean: 7.9234 - Median: 7.9350000000000005 - Std Dev.: 0.23335046603767476 - Max: 8.45 - Min: 7.27\n",
      "8 2\n",
      "Mean: 7.9052999999999995 - Median: 7.91 - Std Dev.: 0.25921595244120293 - Max: 8.49 - Min: 7.03\n",
      "9 0\n",
      "Mean: 7.958600000000001 - Median: 7.95 - Std Dev.: 0.2661203487146369 - Max: 8.61 - Min: 7.27\n",
      "9 1\n",
      "Mean: 7.965400000000002 - Median: 7.955 - Std Dev.: 0.24194387778987095 - Max: 8.52 - Min: 7.22\n",
      "9 2\n",
      "Mean: 7.894499999999999 - Median: 7.91 - Std Dev.: 0.26391049619141715 - Max: 8.55 - Min: 7.2\n",
      "10 0\n",
      "Mean: 7.9249 - Median: 7.94 - Std Dev.: 0.26483389133568225 - Max: 8.53 - Min: 7.28\n",
      "10 1\n",
      "Mean: 7.9924 - Median: 7.995 - Std Dev.: 0.26657126626851585 - Max: 8.67 - Min: 7.41\n",
      "10 2\n",
      "Mean: 7.919699999999999 - Median: 7.945 - Std Dev.: 0.2677291728594402 - Max: 8.53 - Min: 7.3\n"
     ]
    }
   ],
   "source": [
    "num_trials = 100\n",
    "num_episodes = 100\n",
    "for dndx in range(len(discounted_rates)):\n",
    "    for lndx in range(len(learning_rates)):\n",
    "        agent.set_playing_data(\n",
    "            training=False,\n",
    "            discounted_rate_ndx=dndx,\n",
    "            learning_rate_ndx=lndx\n",
    "        )\n",
    "        results = []\n",
    "        for _ in range(num_trials):\n",
    "            result = env.play_episodes(\n",
    "                agent, num_episodes, max_steps,\n",
    "                verbose=False, episode_verbose=False,\n",
    "                render=False\n",
    "            )\n",
    "            results.append(result)\n",
    "        print(dndx, lndx)\n",
    "        print(f'Mean: {np.mean(results)} - '\n",
    "              f'Median: {np.median(results)} - '\n",
    "              f'Std Dev.: {np.std(results)} - '\n",
    "              f'Max: {np.max(results)} - '\n",
    "              f'Min: {np.min(results)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
