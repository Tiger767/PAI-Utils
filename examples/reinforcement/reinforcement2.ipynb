{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement\n",
    "In this notebook, we are going to be focusing on the DQNAgent and the PGAgent in the reinforcement module in PAI-Utils."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from paiutils import neural_network as nn\n",
    "from paiutils import reinforcement as rl\n",
    "\n",
    "\n",
    "# see if using GPU and if so enable memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "We are going to be using the CartPole-v0 environment. For more information on this environment, click [this](https://github.com/openai/gym/wiki/CartPole-v0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32) Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "genv = gym.make('CartPole-v0')\n",
    "max_steps = genv._max_episode_steps\n",
    "print(max_steps)\n",
    "print(genv.observation_space, genv.action_space)\n",
    "\n",
    "env = rl.GymWrapper(genv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32)           128         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           528         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           528         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            17          batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            34          batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q_output (Lambda)               (None, 2)            0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,523\n",
      "Trainable params: 1,395\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x0 = keras.layers.Input(shape=env.state_shape)\n",
    "x = nn.dense(32)(x0)\n",
    "x1 = nn.dense(16)(x)\n",
    "x2 = nn.dense(16)(x)\n",
    "#outputs = keras.layers.Dense(action_shape[0])(x)\n",
    "outputs = rl.DQNAgent.get_dueling_output_layer(\n",
    "    env.action_size, dueling_type='avg'\n",
    ")(x1, x2)\n",
    "qmodel = keras.Model(inputs=x0,\n",
    "                     outputs=outputs)\n",
    "qmodel.compile(optimizer=keras.optimizers.Adam(.001),\n",
    "               loss='mse')\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = rl.StochasticPolicy(\n",
    "    rl.GreedyPolicy(),\n",
    "    rl.ExponentialDecay(.5, .1, .01, step_every_call=False),\n",
    "    0, env.action_size\n",
    ")\n",
    "discounted_rate = .99\n",
    "agent = rl.DQNAgent(\n",
    "    policy, qmodel, discounted_rate,\n",
    "    enable_target=False, enable_double=False,\n",
    "    enable_per=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17:02:39 - Episode: 1 - Steps: 33 - Total Reward: 33.0 - Best Total Reward: 33.0 - Average Total Reward: 33.0 - Memory Size: 33\n",
      "Time: 17:02:39 - Episode: 2 - Steps: 24 - Total Reward: 24.0 - Best Total Reward: 33.0 - Average Total Reward: 28.5 - Memory Size: 57\n",
      "Time: 17:02:39 - Episode: 3 - Steps: 33 - Total Reward: 33.0 - Best Total Reward: 33.0 - Average Total Reward: 30.0 - Memory Size: 90\n",
      "Time: 17:02:39 - Episode: 4 - Steps: 13 - Total Reward: 13.0 - Best Total Reward: 33.0 - Average Total Reward: 25.75 - Memory Size: 103\n",
      "Time: 17:02:39 - Episode: 5 - Steps: 31 - Total Reward: 31.0 - Best Total Reward: 33.0 - Average Total Reward: 26.8 - Memory Size: 134\n",
      "Time: 17:02:39 - Episode: 6 - Steps: 18 - Total Reward: 18.0 - Best Total Reward: 33.0 - Average Total Reward: 25.333333333333332 - Memory Size: 152\n",
      "Time: 17:02:39 - Episode: 7 - Steps: 17 - Total Reward: 17.0 - Best Total Reward: 33.0 - Average Total Reward: 24.142857142857142 - Memory Size: 169\n",
      "Time: 17:02:39 - Episode: 8 - Steps: 11 - Total Reward: 11.0 - Best Total Reward: 33.0 - Average Total Reward: 22.5 - Memory Size: 180\n",
      "Save Loop: 0\n",
      "Time: 17:02:41 - Episode: 1 - Steps: 10 - Total Reward: 10.0 - Best Total Reward: 10.0 - Average Total Reward: 10.0 - Memory Size: 190\n",
      "Time: 17:02:43 - Episode: 2 - Steps: 64 - Total Reward: 64.0 - Best Total Reward: 64.0 - Average Total Reward: 37.0 - Memory Size: 254\n",
      "Time: 17:02:46 - Episode: 3 - Steps: 83 - Total Reward: 83.0 - Best Total Reward: 83.0 - Average Total Reward: 52.333333333333336 - Memory Size: 337\n",
      "Time: 17:02:50 - Episode: 4 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 89.25 - Memory Size: 537\n",
      "Time: 17:02:55 - Episode: 5 - Steps: 150 - Total Reward: 150.0 - Best Total Reward: 200.0 - Average Total Reward: 101.4 - Memory Size: 687\n",
      "Time: 17:03:01 - Episode: 6 - Steps: 76 - Total Reward: 76.0 - Best Total Reward: 200.0 - Average Total Reward: 97.16666666666667 - Memory Size: 763\n"
     ]
    }
   ],
   "source": [
    "# Warmup\n",
    "agent.set_playing_data(memorizing=True, verbose=True)\n",
    "env.play_episodes(agent, 8, max_steps, random=True,\n",
    "                  verbose=True, episode_verbose=False,\n",
    "                  render=False)\n",
    "\n",
    "agent.set_playing_data(\n",
    "    training=True, memorizing=True,\n",
    "    learns_in_episode=False, batch_size=16,\n",
    "    mini_batch=0, epochs=1, repeat=50,\n",
    "    target_update_interval=1, tau=1.0,\n",
    "    verbose=False\n",
    ")\n",
    "save_dir = ''\n",
    "num_episodes = 6\n",
    "for ndx in range(1):\n",
    "    print(f'Save Loop: {ndx}')\n",
    "    result = env.play_episodes(\n",
    "        agent, num_episodes, max_steps,\n",
    "        verbose=True, episode_verbose=False,\n",
    "        render=False\n",
    "    )\n",
    "    agent.save(save_dir, note=f'DQN_{ndx}_{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 - Reward: 1.0 - Action: 0\n",
      "Step: 2 - Reward: 1.0 - Action: 1\n",
      "Step: 3 - Reward: 1.0 - Action: 1\n",
      "Step: 4 - Reward: 1.0 - Action: 0\n",
      "Step: 5 - Reward: 1.0 - Action: 0\n",
      "Step: 6 - Reward: 1.0 - Action: 1\n",
      "Step: 7 - Reward: 1.0 - Action: 0\n",
      "Step: 8 - Reward: 1.0 - Action: 1\n",
      "Step: 9 - Reward: 1.0 - Action: 1\n",
      "Step: 10 - Reward: 1.0 - Action: 0\n",
      "Step: 11 - Reward: 1.0 - Action: 0\n",
      "Step: 12 - Reward: 1.0 - Action: 1\n",
      "Step: 13 - Reward: 1.0 - Action: 0\n",
      "Step: 14 - Reward: 1.0 - Action: 1\n",
      "Step: 15 - Reward: 1.0 - Action: 1\n",
      "Step: 16 - Reward: 1.0 - Action: 0\n",
      "Step: 17 - Reward: 1.0 - Action: 0\n",
      "Step: 18 - Reward: 1.0 - Action: 1\n",
      "Step: 19 - Reward: 1.0 - Action: 0\n",
      "Step: 20 - Reward: 1.0 - Action: 1\n",
      "Step: 21 - Reward: 1.0 - Action: 0\n",
      "Step: 22 - Reward: 1.0 - Action: 1\n",
      "Step: 23 - Reward: 1.0 - Action: 1\n",
      "Step: 24 - Reward: 1.0 - Action: 0\n",
      "Step: 25 - Reward: 1.0 - Action: 0\n",
      "Step: 26 - Reward: 1.0 - Action: 1\n",
      "Step: 27 - Reward: 1.0 - Action: 0\n",
      "Step: 28 - Reward: 1.0 - Action: 1\n",
      "Step: 29 - Reward: 1.0 - Action: 1\n",
      "Step: 30 - Reward: 1.0 - Action: 0\n",
      "Step: 31 - Reward: 1.0 - Action: 0\n",
      "Step: 32 - Reward: 1.0 - Action: 1\n",
      "Step: 33 - Reward: 1.0 - Action: 0\n",
      "Step: 34 - Reward: 1.0 - Action: 1\n",
      "Step: 35 - Reward: 1.0 - Action: 1\n",
      "Step: 36 - Reward: 1.0 - Action: 0\n",
      "Step: 37 - Reward: 1.0 - Action: 0\n",
      "Step: 38 - Reward: 1.0 - Action: 1\n",
      "Step: 39 - Reward: 1.0 - Action: 0\n",
      "Step: 40 - Reward: 1.0 - Action: 1\n",
      "Step: 41 - Reward: 1.0 - Action: 1\n",
      "Step: 42 - Reward: 1.0 - Action: 0\n",
      "Step: 43 - Reward: 1.0 - Action: 0\n",
      "Step: 44 - Reward: 1.0 - Action: 1\n",
      "Step: 45 - Reward: 1.0 - Action: 0\n",
      "Step: 46 - Reward: 1.0 - Action: 1\n",
      "Step: 47 - Reward: 1.0 - Action: 1\n",
      "Step: 48 - Reward: 1.0 - Action: 0\n",
      "Step: 49 - Reward: 1.0 - Action: 0\n",
      "Step: 50 - Reward: 1.0 - Action: 1\n",
      "Step: 51 - Reward: 1.0 - Action: 0\n",
      "Step: 52 - Reward: 1.0 - Action: 1\n",
      "Step: 53 - Reward: 1.0 - Action: 1\n",
      "Step: 54 - Reward: 1.0 - Action: 0\n",
      "Step: 55 - Reward: 1.0 - Action: 0\n",
      "Step: 56 - Reward: 1.0 - Action: 1\n",
      "Step: 57 - Reward: 1.0 - Action: 1\n",
      "Step: 58 - Reward: 1.0 - Action: 0\n",
      "Step: 59 - Reward: 1.0 - Action: 0\n",
      "Step: 60 - Reward: 1.0 - Action: 1\n",
      "Step: 61 - Reward: 1.0 - Action: 1\n",
      "Step: 62 - Reward: 1.0 - Action: 0\n",
      "Step: 63 - Reward: 1.0 - Action: 0\n",
      "Step: 64 - Reward: 1.0 - Action: 1\n",
      "Step: 65 - Reward: 1.0 - Action: 1\n",
      "Step: 66 - Reward: 1.0 - Action: 1\n",
      "Step: 67 - Reward: 1.0 - Action: 0\n",
      "Step: 68 - Reward: 1.0 - Action: 0\n",
      "Step: 69 - Reward: 1.0 - Action: 0\n",
      "Step: 70 - Reward: 1.0 - Action: 1\n",
      "Step: 71 - Reward: 1.0 - Action: 0\n",
      "Step: 72 - Reward: 1.0 - Action: 1\n",
      "Step: 73 - Reward: 1.0 - Action: 1\n",
      "Step: 74 - Reward: 1.0 - Action: 1\n",
      "Step: 75 - Reward: 1.0 - Action: 0\n",
      "Step: 76 - Reward: 1.0 - Action: 0\n",
      "Step: 77 - Reward: 1.0 - Action: 0\n",
      "Step: 78 - Reward: 1.0 - Action: 1\n",
      "Step: 79 - Reward: 1.0 - Action: 0\n",
      "Step: 80 - Reward: 1.0 - Action: 1\n",
      "Step: 81 - Reward: 1.0 - Action: 1\n",
      "Step: 82 - Reward: 1.0 - Action: 1\n",
      "Step: 83 - Reward: 1.0 - Action: 0\n",
      "Step: 84 - Reward: 1.0 - Action: 0\n",
      "Step: 85 - Reward: 1.0 - Action: 0\n",
      "Step: 86 - Reward: 1.0 - Action: 1\n",
      "Step: 87 - Reward: 1.0 - Action: 1\n",
      "Step: 88 - Reward: 1.0 - Action: 1\n",
      "Step: 89 - Reward: 1.0 - Action: 0\n",
      "Step: 90 - Reward: 1.0 - Action: 0\n",
      "Step: 91 - Reward: 1.0 - Action: 0\n",
      "Step: 92 - Reward: 1.0 - Action: 1\n",
      "Step: 93 - Reward: 1.0 - Action: 1\n",
      "Step: 94 - Reward: 1.0 - Action: 1\n",
      "Step: 95 - Reward: 1.0 - Action: 0\n",
      "Step: 96 - Reward: 1.0 - Action: 0\n",
      "Step: 97 - Reward: 1.0 - Action: 0\n",
      "Step: 98 - Reward: 1.0 - Action: 1\n",
      "Step: 99 - Reward: 1.0 - Action: 1\n",
      "Step: 100 - Reward: 1.0 - Action: 1\n",
      "Step: 101 - Reward: 1.0 - Action: 0\n",
      "Step: 102 - Reward: 1.0 - Action: 0\n",
      "Step: 103 - Reward: 1.0 - Action: 0\n",
      "Step: 104 - Reward: 1.0 - Action: 1\n",
      "Step: 105 - Reward: 1.0 - Action: 1\n",
      "Step: 106 - Reward: 1.0 - Action: 0\n",
      "Step: 107 - Reward: 1.0 - Action: 1\n",
      "Step: 108 - Reward: 1.0 - Action: 0\n",
      "Step: 109 - Reward: 1.0 - Action: 1\n",
      "Step: 110 - Reward: 1.0 - Action: 0\n",
      "Step: 111 - Reward: 1.0 - Action: 1\n",
      "Step: 112 - Reward: 1.0 - Action: 0\n",
      "Step: 113 - Reward: 1.0 - Action: 1\n",
      "Step: 114 - Reward: 1.0 - Action: 0\n",
      "Step: 115 - Reward: 1.0 - Action: 1\n",
      "Step: 116 - Reward: 1.0 - Action: 0\n",
      "Step: 117 - Reward: 1.0 - Action: 1\n",
      "Step: 118 - Reward: 1.0 - Action: 0\n",
      "Step: 119 - Reward: 1.0 - Action: 1\n",
      "Step: 120 - Reward: 1.0 - Action: 0\n",
      "Step: 121 - Reward: 1.0 - Action: 1\n",
      "Step: 122 - Reward: 1.0 - Action: 0\n",
      "Step: 123 - Reward: 1.0 - Action: 1\n",
      "Step: 124 - Reward: 1.0 - Action: 0\n",
      "Step: 125 - Reward: 1.0 - Action: 1\n",
      "Step: 126 - Reward: 1.0 - Action: 1\n",
      "Step: 127 - Reward: 1.0 - Action: 1\n",
      "Step: 128 - Reward: 1.0 - Action: 0\n",
      "Step: 129 - Reward: 1.0 - Action: 0\n",
      "Step: 130 - Reward: 1.0 - Action: 0\n",
      "Step: 131 - Reward: 1.0 - Action: 1\n",
      "Step: 132 - Reward: 1.0 - Action: 1\n",
      "Step: 133 - Reward: 1.0 - Action: 0\n",
      "Step: 134 - Reward: 1.0 - Action: 1\n",
      "Step: 135 - Reward: 1.0 - Action: 0\n",
      "Step: 136 - Reward: 1.0 - Action: 1\n",
      "Step: 137 - Reward: 1.0 - Action: 0\n",
      "Step: 138 - Reward: 1.0 - Action: 1\n",
      "Step: 139 - Reward: 1.0 - Action: 0\n",
      "Step: 140 - Reward: 1.0 - Action: 1\n",
      "Step: 141 - Reward: 1.0 - Action: 0\n",
      "Step: 142 - Reward: 1.0 - Action: 1\n",
      "Step: 143 - Reward: 1.0 - Action: 0\n",
      "Step: 144 - Reward: 1.0 - Action: 1\n",
      "Step: 145 - Reward: 1.0 - Action: 0\n",
      "Step: 146 - Reward: 1.0 - Action: 1\n",
      "Step: 147 - Reward: 1.0 - Action: 0\n",
      "Step: 148 - Reward: 1.0 - Action: 0\n",
      "Step: 149 - Reward: 1.0 - Action: 1\n",
      "Step: 150 - Reward: 1.0 - Action: 1\n",
      "Step: 151 - Reward: 1.0 - Action: 0\n",
      "Step: 152 - Reward: 1.0 - Action: 1\n",
      "Step: 153 - Reward: 1.0 - Action: 0\n",
      "Step: 154 - Reward: 1.0 - Action: 1\n",
      "Step: 155 - Reward: 1.0 - Action: 0\n",
      "Step: 156 - Reward: 1.0 - Action: 0\n",
      "Step: 157 - Reward: 1.0 - Action: 1\n",
      "Step: 158 - Reward: 1.0 - Action: 1\n",
      "Step: 159 - Reward: 1.0 - Action: 0\n",
      "Step: 160 - Reward: 1.0 - Action: 1\n",
      "Step: 161 - Reward: 1.0 - Action: 0\n",
      "Step: 162 - Reward: 1.0 - Action: 0\n",
      "Step: 163 - Reward: 1.0 - Action: 1\n",
      "Step: 164 - Reward: 1.0 - Action: 1\n",
      "Step: 165 - Reward: 1.0 - Action: 0\n",
      "Step: 166 - Reward: 1.0 - Action: 0\n",
      "Step: 167 - Reward: 1.0 - Action: 1\n",
      "Step: 168 - Reward: 1.0 - Action: 1\n",
      "Step: 169 - Reward: 1.0 - Action: 0\n",
      "Step: 170 - Reward: 1.0 - Action: 0\n",
      "Step: 171 - Reward: 1.0 - Action: 1\n",
      "Step: 172 - Reward: 1.0 - Action: 1\n",
      "Step: 173 - Reward: 1.0 - Action: 0\n",
      "Step: 174 - Reward: 1.0 - Action: 0\n",
      "Step: 175 - Reward: 1.0 - Action: 1\n",
      "Step: 176 - Reward: 1.0 - Action: 1\n",
      "Step: 177 - Reward: 1.0 - Action: 0\n",
      "Step: 178 - Reward: 1.0 - Action: 0\n",
      "Step: 179 - Reward: 1.0 - Action: 1\n",
      "Step: 180 - Reward: 1.0 - Action: 0\n",
      "Step: 181 - Reward: 1.0 - Action: 1\n",
      "Step: 182 - Reward: 1.0 - Action: 0\n",
      "Step: 183 - Reward: 1.0 - Action: 1\n",
      "Step: 184 - Reward: 1.0 - Action: 1\n",
      "Step: 185 - Reward: 1.0 - Action: 0\n",
      "Step: 186 - Reward: 1.0 - Action: 0\n",
      "Step: 187 - Reward: 1.0 - Action: 1\n",
      "Step: 188 - Reward: 1.0 - Action: 0\n",
      "Step: 189 - Reward: 1.0 - Action: 1\n",
      "Step: 190 - Reward: 1.0 - Action: 1\n",
      "Step: 191 - Reward: 1.0 - Action: 0\n",
      "Step: 192 - Reward: 1.0 - Action: 0\n",
      "Step: 193 - Reward: 1.0 - Action: 1\n",
      "Step: 194 - Reward: 1.0 - Action: 0\n",
      "Step: 195 - Reward: 1.0 - Action: 1\n",
      "Step: 196 - Reward: 1.0 - Action: 0\n",
      "Step: 197 - Reward: 1.0 - Action: 1\n",
      "Step: 198 - Reward: 1.0 - Action: 0\n",
      "Step: 199 - Reward: 1.0 - Action: 1\n",
      "Step: 200 - Reward: 1.0 - Action: 0\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "agent.set_playing_data(training=False,\n",
    "                       memorizing=False)\n",
    "step, total_reward = env.play_episode(\n",
    "    agent, max_steps,\n",
    "    verbose=True, render=False\n",
    ")\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solved?\n",
    "The CartPole environment is considered solved if we get an average reward of 195.0 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17:03:25 - Episode: 1 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:25 - Episode: 2 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:25 - Episode: 3 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:26 - Episode: 4 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:26 - Episode: 5 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:27 - Episode: 6 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:27 - Episode: 7 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:28 - Episode: 8 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:28 - Episode: 9 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:29 - Episode: 10 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:29 - Episode: 11 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:30 - Episode: 12 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:30 - Episode: 13 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:31 - Episode: 14 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:31 - Episode: 15 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:32 - Episode: 16 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:32 - Episode: 17 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:32 - Episode: 18 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:33 - Episode: 19 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:33 - Episode: 20 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:34 - Episode: 21 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:34 - Episode: 22 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:35 - Episode: 23 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:35 - Episode: 24 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:36 - Episode: 25 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:36 - Episode: 26 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:37 - Episode: 27 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:37 - Episode: 28 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:38 - Episode: 29 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:38 - Episode: 30 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 763\n",
      "Time: 17:03:38 - Episode: 31 - Steps: 198 - Total Reward: 198.0 - Best Total Reward: 200.0 - Average Total Reward: 199.93548387096774 - Memory Size: 763\n",
      "Time: 17:03:39 - Episode: 32 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.9375 - Memory Size: 763\n",
      "Time: 17:03:39 - Episode: 33 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.93939393939394 - Memory Size: 763\n",
      "Time: 17:03:40 - Episode: 34 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94117647058823 - Memory Size: 763\n",
      "Time: 17:03:40 - Episode: 35 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94285714285715 - Memory Size: 763\n",
      "Time: 17:03:41 - Episode: 36 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94444444444446 - Memory Size: 763\n",
      "Time: 17:03:41 - Episode: 37 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94594594594594 - Memory Size: 763\n",
      "Time: 17:03:42 - Episode: 38 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94736842105263 - Memory Size: 763\n",
      "Time: 17:03:42 - Episode: 39 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.94871794871796 - Memory Size: 763\n",
      "Time: 17:03:43 - Episode: 40 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.95 - Memory Size: 763\n",
      "Time: 17:03:43 - Episode: 41 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.9512195121951 - Memory Size: 763\n",
      "Time: 17:03:44 - Episode: 42 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.95238095238096 - Memory Size: 763\n",
      "Time: 17:03:44 - Episode: 43 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.95348837209303 - Memory Size: 763\n",
      "Time: 17:03:44 - Episode: 44 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.95454545454547 - Memory Size: 763\n",
      "Time: 17:03:45 - Episode: 45 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.95555555555555 - Memory Size: 763\n",
      "Time: 17:03:45 - Episode: 46 - Steps: 148 - Total Reward: 148.0 - Best Total Reward: 200.0 - Average Total Reward: 198.82608695652175 - Memory Size: 763\n",
      "Time: 17:03:46 - Episode: 47 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.85106382978722 - Memory Size: 763\n",
      "Time: 17:03:46 - Episode: 48 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.875 - Memory Size: 763\n",
      "Time: 17:03:47 - Episode: 49 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.89795918367346 - Memory Size: 763\n",
      "Time: 17:03:47 - Episode: 50 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.92 - Memory Size: 763\n",
      "Time: 17:03:48 - Episode: 51 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.94117647058823 - Memory Size: 763\n",
      "Time: 17:03:48 - Episode: 52 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.96153846153845 - Memory Size: 763\n",
      "Time: 17:03:49 - Episode: 53 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 198.9811320754717 - Memory Size: 763\n",
      "Time: 17:03:49 - Episode: 54 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.0 - Memory Size: 763\n",
      "Time: 17:03:49 - Episode: 55 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.01818181818183 - Memory Size: 763\n",
      "Time: 17:03:50 - Episode: 56 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.03571428571428 - Memory Size: 763\n",
      "Time: 17:03:50 - Episode: 57 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.05263157894737 - Memory Size: 763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17:03:51 - Episode: 58 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.06896551724137 - Memory Size: 763\n",
      "Time: 17:03:51 - Episode: 59 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.08474576271186 - Memory Size: 763\n",
      "Time: 17:03:52 - Episode: 60 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.1 - Memory Size: 763\n",
      "Time: 17:03:52 - Episode: 61 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.11475409836066 - Memory Size: 763\n",
      "Time: 17:03:53 - Episode: 62 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.1290322580645 - Memory Size: 763\n",
      "Time: 17:03:53 - Episode: 63 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.14285714285714 - Memory Size: 763\n",
      "Time: 17:03:54 - Episode: 64 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.15625 - Memory Size: 763\n",
      "Time: 17:03:54 - Episode: 65 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.16923076923078 - Memory Size: 763\n",
      "Time: 17:03:55 - Episode: 66 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.1818181818182 - Memory Size: 763\n",
      "Time: 17:03:55 - Episode: 67 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.19402985074626 - Memory Size: 763\n",
      "Time: 17:03:55 - Episode: 68 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.2058823529412 - Memory Size: 763\n",
      "Time: 17:03:56 - Episode: 69 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.2173913043478 - Memory Size: 763\n",
      "Time: 17:03:56 - Episode: 70 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.22857142857143 - Memory Size: 763\n",
      "Time: 17:03:57 - Episode: 71 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.2394366197183 - Memory Size: 763\n",
      "Time: 17:03:57 - Episode: 72 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.25 - Memory Size: 763\n",
      "Time: 17:03:58 - Episode: 73 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.26027397260273 - Memory Size: 763\n",
      "Time: 17:03:58 - Episode: 74 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.27027027027026 - Memory Size: 763\n",
      "Time: 17:03:59 - Episode: 75 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.28 - Memory Size: 763\n",
      "Time: 17:03:59 - Episode: 76 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.28947368421052 - Memory Size: 763\n",
      "Time: 17:04:00 - Episode: 77 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.2987012987013 - Memory Size: 763\n",
      "Time: 17:04:00 - Episode: 78 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.30769230769232 - Memory Size: 763\n",
      "Time: 17:04:01 - Episode: 79 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.31645569620252 - Memory Size: 763\n",
      "Time: 17:04:01 - Episode: 80 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.325 - Memory Size: 763\n",
      "Time: 17:04:02 - Episode: 81 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.33333333333334 - Memory Size: 763\n",
      "Time: 17:04:02 - Episode: 82 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.34146341463415 - Memory Size: 763\n",
      "Time: 17:04:03 - Episode: 83 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.34939759036143 - Memory Size: 763\n",
      "Time: 17:04:03 - Episode: 84 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.35714285714286 - Memory Size: 763\n",
      "Time: 17:04:03 - Episode: 85 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.36470588235295 - Memory Size: 763\n",
      "Time: 17:04:04 - Episode: 86 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.37209302325581 - Memory Size: 763\n",
      "Time: 17:04:04 - Episode: 87 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.3793103448276 - Memory Size: 763\n",
      "Time: 17:04:05 - Episode: 88 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.38636363636363 - Memory Size: 763\n",
      "Time: 17:04:05 - Episode: 89 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.3932584269663 - Memory Size: 763\n",
      "Time: 17:04:06 - Episode: 90 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.4 - Memory Size: 763\n",
      "Time: 17:04:06 - Episode: 91 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.4065934065934 - Memory Size: 763\n",
      "Time: 17:04:07 - Episode: 92 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.41304347826087 - Memory Size: 763\n",
      "Time: 17:04:07 - Episode: 93 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.41935483870967 - Memory Size: 763\n",
      "Time: 17:04:08 - Episode: 94 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.4255319148936 - Memory Size: 763\n",
      "Time: 17:04:08 - Episode: 95 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.43157894736842 - Memory Size: 763\n",
      "Time: 17:04:09 - Episode: 96 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.4375 - Memory Size: 763\n",
      "Time: 17:04:09 - Episode: 97 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.44329896907217 - Memory Size: 763\n",
      "Time: 17:04:09 - Episode: 98 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.44897959183675 - Memory Size: 763\n",
      "Time: 17:04:10 - Episode: 99 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.45454545454547 - Memory Size: 763\n",
      "Time: 17:04:10 - Episode: 100 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.46 - Memory Size: 763\n",
      "Solved: True\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "agent.set_playing_data(training=False,\n",
    "                       memorizing=False)\n",
    "result = env.play_episodes(\n",
    "    agent, num_episodes, max_steps,\n",
    "    verbose=True, episode_verbose=False,\n",
    "    render=False\n",
    ")\n",
    "print(f'Solved: {result > 195}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Actor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,538\n",
      "Trainable params: 1,410\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=env.state_shape)\n",
    "x = nn.dense(32)(inputs)\n",
    "x = nn.dense(32)(x)\n",
    "outputs = nn.dense(env.action_size,\n",
    "                   activation='softmax',\n",
    "                   batch_norm=False)(x)\n",
    "\n",
    "amodel = keras.Model(inputs=inputs,\n",
    "                     outputs=outputs)\n",
    "amodel.compile(optimizer=keras.optimizers.Adam(.01),\n",
    "               loss='mse')\n",
    "amodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_rate = .99\n",
    "agent = rl.PGAgent(\n",
    "    amodel, discounted_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Loop: 0\n",
      "Time: 20:18:00 - Episode: 1 - Steps: 13 - Total Reward: 13.0 - Best Total Reward: 13.0 - Average Total Reward: 13.0 - Memory Size: 13\n",
      "Time: 20:18:00 - Episode: 2 - Steps: 26 - Total Reward: 26.0 - Best Total Reward: 26.0 - Average Total Reward: 19.5 - Memory Size: 39\n",
      "Time: 20:18:00 - Episode: 3 - Steps: 27 - Total Reward: 27.0 - Best Total Reward: 27.0 - Average Total Reward: 22.0 - Memory Size: 66\n",
      "Time: 20:18:00 - Episode: 4 - Steps: 10 - Total Reward: 10.0 - Best Total Reward: 27.0 - Average Total Reward: 19.0 - Memory Size: 76\n",
      "Time: 20:18:00 - Episode: 5 - Steps: 8 - Total Reward: 8.0 - Best Total Reward: 27.0 - Average Total Reward: 16.8 - Memory Size: 84\n",
      "Time: 20:18:00 - Episode: 6 - Steps: 10 - Total Reward: 10.0 - Best Total Reward: 27.0 - Average Total Reward: 15.666666666666666 - Memory Size: 94\n",
      "Time: 20:18:01 - Episode: 7 - Steps: 46 - Total Reward: 46.0 - Best Total Reward: 46.0 - Average Total Reward: 20.0 - Memory Size: 140\n",
      "Time: 20:18:01 - Episode: 8 - Steps: 50 - Total Reward: 50.0 - Best Total Reward: 50.0 - Average Total Reward: 23.75 - Memory Size: 190\n",
      "Time: 20:18:01 - Episode: 9 - Steps: 67 - Total Reward: 67.0 - Best Total Reward: 67.0 - Average Total Reward: 28.555555555555557 - Memory Size: 257\n",
      "Time: 20:18:01 - Episode: 10 - Steps: 49 - Total Reward: 49.0 - Best Total Reward: 67.0 - Average Total Reward: 30.6 - Memory Size: 306\n",
      "Time: 20:18:02 - Episode: 11 - Steps: 125 - Total Reward: 125.0 - Best Total Reward: 125.0 - Average Total Reward: 39.18181818181818 - Memory Size: 431\n",
      "Time: 20:18:02 - Episode: 12 - Steps: 112 - Total Reward: 112.0 - Best Total Reward: 125.0 - Average Total Reward: 45.25 - Memory Size: 543\n",
      "Time: 20:18:02 - Episode: 13 - Steps: 67 - Total Reward: 67.0 - Best Total Reward: 125.0 - Average Total Reward: 46.92307692307692 - Memory Size: 610\n",
      "Time: 20:18:03 - Episode: 14 - Steps: 116 - Total Reward: 116.0 - Best Total Reward: 125.0 - Average Total Reward: 51.857142857142854 - Memory Size: 726\n",
      "Time: 20:18:03 - Episode: 15 - Steps: 116 - Total Reward: 116.0 - Best Total Reward: 125.0 - Average Total Reward: 56.13333333333333 - Memory Size: 842\n",
      "Time: 20:18:04 - Episode: 16 - Steps: 14 - Total Reward: 14.0 - Best Total Reward: 125.0 - Average Total Reward: 53.5 - Memory Size: 856\n",
      "Time: 20:18:04 - Episode: 17 - Steps: 94 - Total Reward: 94.0 - Best Total Reward: 125.0 - Average Total Reward: 55.88235294117647 - Memory Size: 950\n",
      "Time: 20:18:05 - Episode: 18 - Steps: 138 - Total Reward: 138.0 - Best Total Reward: 138.0 - Average Total Reward: 60.44444444444444 - Memory Size: 1088\n",
      "Time: 20:18:06 - Episode: 19 - Steps: 71 - Total Reward: 71.0 - Best Total Reward: 138.0 - Average Total Reward: 61.0 - Memory Size: 1159\n",
      "Time: 20:18:06 - Episode: 20 - Steps: 177 - Total Reward: 177.0 - Best Total Reward: 177.0 - Average Total Reward: 66.8 - Memory Size: 1336\n",
      "Time: 20:18:07 - Episode: 21 - Steps: 82 - Total Reward: 82.0 - Best Total Reward: 177.0 - Average Total Reward: 67.52380952380952 - Memory Size: 1418\n",
      "Time: 20:18:08 - Episode: 22 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 73.54545454545455 - Memory Size: 1618\n",
      "Time: 20:18:09 - Episode: 23 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 79.04347826086956 - Memory Size: 1818\n",
      "Time: 20:18:10 - Episode: 24 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 84.08333333333333 - Memory Size: 2018\n",
      "Time: 20:18:12 - Episode: 25 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 88.72 - Memory Size: 2218\n",
      "Time: 20:18:13 - Episode: 26 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 93.0 - Memory Size: 2418\n",
      "Time: 20:18:14 - Episode: 27 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 96.96296296296296 - Memory Size: 2618\n",
      "Time: 20:18:16 - Episode: 28 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 100.64285714285714 - Memory Size: 2818\n",
      "Time: 20:18:18 - Episode: 29 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 104.06896551724138 - Memory Size: 3018\n",
      "Time: 20:18:19 - Episode: 30 - Steps: 129 - Total Reward: 129.0 - Best Total Reward: 200.0 - Average Total Reward: 104.9 - Memory Size: 3147\n"
     ]
    }
   ],
   "source": [
    "# No warmup needed\n",
    "#agent.set_playing_data(memorizing=True, verbose=True)\n",
    "#env.play_episodes(agent, 1, max_steps, random=True,\n",
    "#                  verbose=True, episode_verbose=False,\n",
    "#                  render=False)\n",
    "\n",
    "agent.set_playing_data(\n",
    "    training=True, memorizing=True,\n",
    "    batch_size=16, mini_batch=0,\n",
    "    epochs=5, repeat=1,\n",
    "    entropy_coef=0,\n",
    "    verbose=False\n",
    ")\n",
    "save_dir = ''\n",
    "num_episodes = 30\n",
    "for ndx in range(1):\n",
    "    print(f'Save Loop: {ndx}')\n",
    "    result = env.play_episodes(\n",
    "        agent, num_episodes, max_steps,\n",
    "        verbose=True, episode_verbose=False,\n",
    "        render=False\n",
    "    )\n",
    "    agent.save(save_dir, note=f'PG_{ndx}_{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 - Reward: 1.0 - Action: 1\n",
      "Step: 2 - Reward: 1.0 - Action: 0\n",
      "Step: 3 - Reward: 1.0 - Action: 1\n",
      "Step: 4 - Reward: 1.0 - Action: 0\n",
      "Step: 5 - Reward: 1.0 - Action: 0\n",
      "Step: 6 - Reward: 1.0 - Action: 1\n",
      "Step: 7 - Reward: 1.0 - Action: 0\n",
      "Step: 8 - Reward: 1.0 - Action: 1\n",
      "Step: 9 - Reward: 1.0 - Action: 0\n",
      "Step: 10 - Reward: 1.0 - Action: 1\n",
      "Step: 11 - Reward: 1.0 - Action: 0\n",
      "Step: 12 - Reward: 1.0 - Action: 1\n",
      "Step: 13 - Reward: 1.0 - Action: 0\n",
      "Step: 14 - Reward: 1.0 - Action: 1\n",
      "Step: 15 - Reward: 1.0 - Action: 0\n",
      "Step: 16 - Reward: 1.0 - Action: 1\n",
      "Step: 17 - Reward: 1.0 - Action: 0\n",
      "Step: 18 - Reward: 1.0 - Action: 1\n",
      "Step: 19 - Reward: 1.0 - Action: 0\n",
      "Step: 20 - Reward: 1.0 - Action: 1\n",
      "Step: 21 - Reward: 1.0 - Action: 0\n",
      "Step: 22 - Reward: 1.0 - Action: 1\n",
      "Step: 23 - Reward: 1.0 - Action: 0\n",
      "Step: 24 - Reward: 1.0 - Action: 0\n",
      "Step: 25 - Reward: 1.0 - Action: 1\n",
      "Step: 26 - Reward: 1.0 - Action: 0\n",
      "Step: 27 - Reward: 1.0 - Action: 1\n",
      "Step: 28 - Reward: 1.0 - Action: 0\n",
      "Step: 29 - Reward: 1.0 - Action: 1\n",
      "Step: 30 - Reward: 1.0 - Action: 0\n",
      "Step: 31 - Reward: 1.0 - Action: 1\n",
      "Step: 32 - Reward: 1.0 - Action: 0\n",
      "Step: 33 - Reward: 1.0 - Action: 1\n",
      "Step: 34 - Reward: 1.0 - Action: 0\n",
      "Step: 35 - Reward: 1.0 - Action: 1\n",
      "Step: 36 - Reward: 1.0 - Action: 0\n",
      "Step: 37 - Reward: 1.0 - Action: 1\n",
      "Step: 38 - Reward: 1.0 - Action: 0\n",
      "Step: 39 - Reward: 1.0 - Action: 1\n",
      "Step: 40 - Reward: 1.0 - Action: 0\n",
      "Step: 41 - Reward: 1.0 - Action: 1\n",
      "Step: 42 - Reward: 1.0 - Action: 0\n",
      "Step: 43 - Reward: 1.0 - Action: 1\n",
      "Step: 44 - Reward: 1.0 - Action: 0\n",
      "Step: 45 - Reward: 1.0 - Action: 1\n",
      "Step: 46 - Reward: 1.0 - Action: 0\n",
      "Step: 47 - Reward: 1.0 - Action: 1\n",
      "Step: 48 - Reward: 1.0 - Action: 0\n",
      "Step: 49 - Reward: 1.0 - Action: 1\n",
      "Step: 50 - Reward: 1.0 - Action: 0\n",
      "Step: 51 - Reward: 1.0 - Action: 1\n",
      "Step: 52 - Reward: 1.0 - Action: 0\n",
      "Step: 53 - Reward: 1.0 - Action: 1\n",
      "Step: 54 - Reward: 1.0 - Action: 0\n",
      "Step: 55 - Reward: 1.0 - Action: 1\n",
      "Step: 56 - Reward: 1.0 - Action: 0\n",
      "Step: 57 - Reward: 1.0 - Action: 1\n",
      "Step: 58 - Reward: 1.0 - Action: 0\n",
      "Step: 59 - Reward: 1.0 - Action: 1\n",
      "Step: 60 - Reward: 1.0 - Action: 0\n",
      "Step: 61 - Reward: 1.0 - Action: 1\n",
      "Step: 62 - Reward: 1.0 - Action: 0\n",
      "Step: 63 - Reward: 1.0 - Action: 1\n",
      "Step: 64 - Reward: 1.0 - Action: 0\n",
      "Step: 65 - Reward: 1.0 - Action: 1\n",
      "Step: 66 - Reward: 1.0 - Action: 0\n",
      "Step: 67 - Reward: 1.0 - Action: 1\n",
      "Step: 68 - Reward: 1.0 - Action: 0\n",
      "Step: 69 - Reward: 1.0 - Action: 1\n",
      "Step: 70 - Reward: 1.0 - Action: 0\n",
      "Step: 71 - Reward: 1.0 - Action: 1\n",
      "Step: 72 - Reward: 1.0 - Action: 1\n",
      "Step: 73 - Reward: 1.0 - Action: 0\n",
      "Step: 74 - Reward: 1.0 - Action: 1\n",
      "Step: 75 - Reward: 1.0 - Action: 0\n",
      "Step: 76 - Reward: 1.0 - Action: 1\n",
      "Step: 77 - Reward: 1.0 - Action: 0\n",
      "Step: 78 - Reward: 1.0 - Action: 1\n",
      "Step: 79 - Reward: 1.0 - Action: 0\n",
      "Step: 80 - Reward: 1.0 - Action: 1\n",
      "Step: 81 - Reward: 1.0 - Action: 0\n",
      "Step: 82 - Reward: 1.0 - Action: 1\n",
      "Step: 83 - Reward: 1.0 - Action: 0\n",
      "Step: 84 - Reward: 1.0 - Action: 1\n",
      "Step: 85 - Reward: 1.0 - Action: 0\n",
      "Step: 86 - Reward: 1.0 - Action: 1\n",
      "Step: 87 - Reward: 1.0 - Action: 0\n",
      "Step: 88 - Reward: 1.0 - Action: 1\n",
      "Step: 89 - Reward: 1.0 - Action: 0\n",
      "Step: 90 - Reward: 1.0 - Action: 1\n",
      "Step: 91 - Reward: 1.0 - Action: 0\n",
      "Step: 92 - Reward: 1.0 - Action: 1\n",
      "Step: 93 - Reward: 1.0 - Action: 0\n",
      "Step: 94 - Reward: 1.0 - Action: 1\n",
      "Step: 95 - Reward: 1.0 - Action: 0\n",
      "Step: 96 - Reward: 1.0 - Action: 1\n",
      "Step: 97 - Reward: 1.0 - Action: 0\n",
      "Step: 98 - Reward: 1.0 - Action: 1\n",
      "Step: 99 - Reward: 1.0 - Action: 1\n",
      "Step: 100 - Reward: 1.0 - Action: 0\n",
      "Step: 101 - Reward: 1.0 - Action: 1\n",
      "Step: 102 - Reward: 1.0 - Action: 0\n",
      "Step: 103 - Reward: 1.0 - Action: 1\n",
      "Step: 104 - Reward: 1.0 - Action: 0\n",
      "Step: 105 - Reward: 1.0 - Action: 1\n",
      "Step: 106 - Reward: 1.0 - Action: 0\n",
      "Step: 107 - Reward: 1.0 - Action: 1\n",
      "Step: 108 - Reward: 1.0 - Action: 0\n",
      "Step: 109 - Reward: 1.0 - Action: 1\n",
      "Step: 110 - Reward: 1.0 - Action: 0\n",
      "Step: 111 - Reward: 1.0 - Action: 1\n",
      "Step: 112 - Reward: 1.0 - Action: 0\n",
      "Step: 113 - Reward: 1.0 - Action: 0\n",
      "Step: 114 - Reward: 1.0 - Action: 1\n",
      "Step: 115 - Reward: 1.0 - Action: 0\n",
      "Step: 116 - Reward: 1.0 - Action: 1\n",
      "Step: 117 - Reward: 1.0 - Action: 0\n",
      "Step: 118 - Reward: 1.0 - Action: 1\n",
      "Step: 119 - Reward: 1.0 - Action: 0\n",
      "Step: 120 - Reward: 1.0 - Action: 1\n",
      "Step: 121 - Reward: 1.0 - Action: 0\n",
      "Step: 122 - Reward: 1.0 - Action: 1\n",
      "Step: 123 - Reward: 1.0 - Action: 0\n",
      "Step: 124 - Reward: 1.0 - Action: 1\n",
      "Step: 125 - Reward: 1.0 - Action: 0\n",
      "Step: 126 - Reward: 1.0 - Action: 1\n",
      "Step: 127 - Reward: 1.0 - Action: 0\n",
      "Step: 128 - Reward: 1.0 - Action: 1\n",
      "Step: 129 - Reward: 1.0 - Action: 0\n",
      "Step: 130 - Reward: 1.0 - Action: 1\n",
      "Step: 131 - Reward: 1.0 - Action: 0\n",
      "Step: 132 - Reward: 1.0 - Action: 1\n",
      "Step: 133 - Reward: 1.0 - Action: 0\n",
      "Step: 134 - Reward: 1.0 - Action: 0\n",
      "Step: 135 - Reward: 1.0 - Action: 1\n",
      "Step: 136 - Reward: 1.0 - Action: 0\n",
      "Step: 137 - Reward: 1.0 - Action: 1\n",
      "Step: 138 - Reward: 1.0 - Action: 0\n",
      "Step: 139 - Reward: 1.0 - Action: 1\n",
      "Step: 140 - Reward: 1.0 - Action: 0\n",
      "Step: 141 - Reward: 1.0 - Action: 1\n",
      "Step: 142 - Reward: 1.0 - Action: 0\n",
      "Step: 143 - Reward: 1.0 - Action: 1\n",
      "Step: 144 - Reward: 1.0 - Action: 0\n",
      "Step: 145 - Reward: 1.0 - Action: 1\n",
      "Step: 146 - Reward: 1.0 - Action: 0\n",
      "Step: 147 - Reward: 1.0 - Action: 1\n",
      "Step: 148 - Reward: 1.0 - Action: 0\n",
      "Step: 149 - Reward: 1.0 - Action: 1\n",
      "Step: 150 - Reward: 1.0 - Action: 0\n",
      "Step: 151 - Reward: 1.0 - Action: 1\n",
      "Step: 152 - Reward: 1.0 - Action: 0\n",
      "Step: 153 - Reward: 1.0 - Action: 1\n",
      "Step: 154 - Reward: 1.0 - Action: 0\n",
      "Step: 155 - Reward: 1.0 - Action: 1\n",
      "Step: 156 - Reward: 1.0 - Action: 0\n",
      "Step: 157 - Reward: 1.0 - Action: 1\n",
      "Step: 158 - Reward: 1.0 - Action: 0\n",
      "Step: 159 - Reward: 1.0 - Action: 1\n",
      "Step: 160 - Reward: 1.0 - Action: 0\n",
      "Step: 161 - Reward: 1.0 - Action: 0\n",
      "Step: 162 - Reward: 1.0 - Action: 1\n",
      "Step: 163 - Reward: 1.0 - Action: 0\n",
      "Step: 164 - Reward: 1.0 - Action: 1\n",
      "Step: 165 - Reward: 1.0 - Action: 0\n",
      "Step: 166 - Reward: 1.0 - Action: 1\n",
      "Step: 167 - Reward: 1.0 - Action: 0\n",
      "Step: 168 - Reward: 1.0 - Action: 1\n",
      "Step: 169 - Reward: 1.0 - Action: 0\n",
      "Step: 170 - Reward: 1.0 - Action: 1\n",
      "Step: 171 - Reward: 1.0 - Action: 0\n",
      "Step: 172 - Reward: 1.0 - Action: 1\n",
      "Step: 173 - Reward: 1.0 - Action: 1\n",
      "Step: 174 - Reward: 1.0 - Action: 0\n",
      "Step: 175 - Reward: 1.0 - Action: 1\n",
      "Step: 176 - Reward: 1.0 - Action: 0\n",
      "Step: 177 - Reward: 1.0 - Action: 1\n",
      "Step: 178 - Reward: 1.0 - Action: 0\n",
      "Step: 179 - Reward: 1.0 - Action: 1\n",
      "Step: 180 - Reward: 1.0 - Action: 0\n",
      "Step: 181 - Reward: 1.0 - Action: 1\n",
      "Step: 182 - Reward: 1.0 - Action: 0\n",
      "Step: 183 - Reward: 1.0 - Action: 1\n",
      "Step: 184 - Reward: 1.0 - Action: 0\n",
      "Step: 185 - Reward: 1.0 - Action: 1\n",
      "Step: 186 - Reward: 1.0 - Action: 0\n",
      "Step: 187 - Reward: 1.0 - Action: 1\n",
      "Step: 188 - Reward: 1.0 - Action: 0\n",
      "Step: 189 - Reward: 1.0 - Action: 1\n",
      "Step: 190 - Reward: 1.0 - Action: 0\n",
      "Step: 191 - Reward: 1.0 - Action: 1\n",
      "Step: 192 - Reward: 1.0 - Action: 0\n",
      "Step: 193 - Reward: 1.0 - Action: 1\n",
      "Step: 194 - Reward: 1.0 - Action: 0\n",
      "Step: 195 - Reward: 1.0 - Action: 1\n",
      "Step: 196 - Reward: 1.0 - Action: 0\n",
      "Step: 197 - Reward: 1.0 - Action: 1\n",
      "Step: 198 - Reward: 1.0 - Action: 0\n",
      "Step: 199 - Reward: 1.0 - Action: 1\n",
      "Step: 200 - Reward: 1.0 - Action: 0\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "agent.set_playing_data(training=False,\n",
    "                       memorizing=False)\n",
    "step, total_reward = env.play_episode(\n",
    "    agent, max_steps,\n",
    "    verbose=True, render=False\n",
    ")\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solved?\n",
    "As previously mentioned, the CartPole environment is considered solved if we get an average reward of 195.0 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20:18:35 - Episode: 1 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:35 - Episode: 2 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:35 - Episode: 3 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:35 - Episode: 4 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:36 - Episode: 5 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:36 - Episode: 6 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:36 - Episode: 7 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:37 - Episode: 8 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:37 - Episode: 9 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:37 - Episode: 10 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:37 - Episode: 11 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:38 - Episode: 12 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:38 - Episode: 13 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:38 - Episode: 14 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:38 - Episode: 15 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:39 - Episode: 16 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:39 - Episode: 17 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:39 - Episode: 18 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:40 - Episode: 19 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:40 - Episode: 20 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:40 - Episode: 21 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:40 - Episode: 22 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:41 - Episode: 23 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:41 - Episode: 24 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:41 - Episode: 25 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:41 - Episode: 26 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:42 - Episode: 27 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:42 - Episode: 28 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:42 - Episode: 29 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:42 - Episode: 30 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:43 - Episode: 31 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:43 - Episode: 32 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:43 - Episode: 33 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:44 - Episode: 34 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:44 - Episode: 35 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:44 - Episode: 36 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:44 - Episode: 37 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:45 - Episode: 38 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:45 - Episode: 39 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:45 - Episode: 40 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:45 - Episode: 41 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:46 - Episode: 42 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:46 - Episode: 43 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:46 - Episode: 44 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:47 - Episode: 45 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:47 - Episode: 46 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:47 - Episode: 47 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:47 - Episode: 48 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:48 - Episode: 49 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:48 - Episode: 50 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:48 - Episode: 51 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:48 - Episode: 52 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:49 - Episode: 53 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:49 - Episode: 54 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:49 - Episode: 55 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:50 - Episode: 56 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:50 - Episode: 57 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:50 - Episode: 58 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:50 - Episode: 59 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20:18:51 - Episode: 60 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:51 - Episode: 61 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:51 - Episode: 62 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:51 - Episode: 63 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:52 - Episode: 64 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:52 - Episode: 65 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:52 - Episode: 66 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:52 - Episode: 67 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:53 - Episode: 68 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:53 - Episode: 69 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:53 - Episode: 70 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:54 - Episode: 71 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:54 - Episode: 72 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:54 - Episode: 73 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:54 - Episode: 74 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:55 - Episode: 75 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:55 - Episode: 76 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:55 - Episode: 77 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:55 - Episode: 78 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:56 - Episode: 79 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:56 - Episode: 80 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:56 - Episode: 81 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:57 - Episode: 82 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:57 - Episode: 83 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:57 - Episode: 84 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:57 - Episode: 85 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:58 - Episode: 86 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:58 - Episode: 87 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:58 - Episode: 88 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:58 - Episode: 89 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:59 - Episode: 90 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:59 - Episode: 91 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:18:59 - Episode: 92 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 200.0 - Memory Size: 3147\n",
      "Time: 20:19:00 - Episode: 93 - Steps: 197 - Total Reward: 197.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96774193548387 - Memory Size: 3147\n",
      "Time: 20:19:00 - Episode: 94 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96808510638297 - Memory Size: 3147\n",
      "Time: 20:19:00 - Episode: 95 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96842105263158 - Memory Size: 3147\n",
      "Time: 20:19:00 - Episode: 96 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96875 - Memory Size: 3147\n",
      "Time: 20:19:01 - Episode: 97 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96907216494844 - Memory Size: 3147\n",
      "Time: 20:19:01 - Episode: 98 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96938775510205 - Memory Size: 3147\n",
      "Time: 20:19:01 - Episode: 99 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.96969696969697 - Memory Size: 3147\n",
      "Time: 20:19:01 - Episode: 100 - Steps: 200 - Total Reward: 200.0 - Best Total Reward: 200.0 - Average Total Reward: 199.97 - Memory Size: 3147\n",
      "Solved: True\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "agent.set_playing_data(training=False,\n",
    "                       memorizing=False)\n",
    "result = env.play_episodes(\n",
    "    agent, num_episodes, max_steps,\n",
    "    verbose=True, episode_verbose=False,\n",
    "    render=False\n",
    ")\n",
    "print(f'Solved: {result > 195}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gameplay\n",
    "[![Gameplay of the DQN and PG Agents](http://img.youtube.com/vi/0d3U2tkhEkM/0.jpg)](https://www.youtube.com/watch?v=0d3U2tkhEkM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
