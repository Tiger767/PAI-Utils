<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>paiutils.reinforcement API documentation</title>
<meta name="description" content="Author: Travis Hammond
Version: 12_21_2020" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>paiutils.reinforcement</code></h1>
</header>
<section id="section-intro">
<p>Author: Travis Hammond
Version: 12_21_2020</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Author: Travis Hammond
Version: 12_21_2020
&#34;&#34;&#34;


import os
from datetime import datetime

import h5py
import gym
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow import keras
from tensorflow.keras.models import model_from_json


class Environment:
    &#34;&#34;&#34;This class handles the environment in which the Agent
       performs actions in and can get rewards from.
    &#34;&#34;&#34;

    def __init__(self, state_shape, action_size):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            state_shape: A tuple of integers, which is the
                         expected state shape for the agent,
                         or an integer of the discrete state
                         space
            action_size: An integer which is the discrete size
                         of the action space
        &#34;&#34;&#34;
        if isinstance(state_shape, int):
            self.discrete_state_space = state_shape
            state_shape = 1
        else:
            self.discrete_state_space = None
        self.state_shape = state_shape
        self.action_size = action_size

    def reset(self):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = None
        return self.state

    def step(self, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the action.

        Args:
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state = None
        return self.state, 0, False

    def play_episode(self, agent, max_steps,
                     random=False, random_bounds=None,
                     render=False, verbose=True):
        &#34;&#34;&#34;Plays a single complete episode with the agent.

        Args:
            agent: An instance of Agent, which will be used to
                   interact in the environment
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen

        Returns:
            A tuple of an integer (last step) and a float (total reward)
        &#34;&#34;&#34;
        if not isinstance(agent, Agent):
            raise TypeError(&#39;The instance agent is not a child of Agent.&#39;)
        if not isinstance(agent.playing_data, PlayingData):
            raise ValueError(&#39;Invalid playing_data value. &#39;
                             &#39;(Forgot to set playing_data?)&#39;)
        total_reward = 0
        state = self.reset()
        if render:
            self.render()
        for step in range(1, max_steps + 1):
            if random:
                if random_bounds is None:
                    action = np.random.randint(0, self.action_size)
                else:
                    action = np.random.uniform(*random_bounds,
                                               size=self.action_size)
            else:
                action = agent.select_action(
                    state, training=agent.playing_data.training
                )
            new_state, reward, terminal = self.step(action)

            total_reward += reward

            if agent.playing_data.memorizing:
                agent.add_memory(state, action, new_state, reward, terminal)

            state = new_state

            if verbose:
                print(f&#39;Step: {step} - Reward: {reward} &#39;
                      f&#39;- Action: {action}&#39;)
            if render:
                self.render()
            if (agent.playing_data.training
                    and agent.playing_data.learns_in_episode
                    and agent.playing_data.epochs &gt; 0):
                agent.learn(**agent.playing_data.learning_params)
            if terminal:
                break
        agent.end_episode()
        if (agent.playing_data.training
                and not agent.playing_data.learns_in_episode
                and agent.playing_data.epochs &gt; 0):
            agent.learn(**agent.playing_data.learning_params)
        return step, total_reward

    def play_episodes(self, agent, num_episodes, max_steps,
                      random=False, random_bounds=None,
                      render=False, verbose=True,
                      episode_verbose=None,
                      end_episode_callback=None):
        &#34;&#34;&#34;Plays atleast 1 complete episode with the agent.

        Args:
            agent: An instance of Agent, which will be used to
                   interact in the environment
            num_episodes: An integer, which is the number of episodes to play
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen
            episode_verbose: A boolean, which determines if single episode
                             information should be printed to the screen
            end_episode_callback: A function called at the end of each episode
                                  with episode count, steps, and total reward
                                  from the most recent episode. If True
                                  is returned, play_episodes will stop
                                  early.

        Returns:
            A float, which is the average total reward of all episodes
        &#34;&#34;&#34;
        if episode_verbose is None:
            episode_verbose = verbose
        total_rewards = 0
        best_reward = &#39;Unknown&#39;
        for episode in range(1, num_episodes + 1):
            step, total_reward = self.play_episode(
                agent, max_steps, random=random, random_bounds=random_bounds,
                render=render, verbose=episode_verbose,
            )
            total_rewards += total_reward
            if best_reward == &#39;Unknown&#39; or total_reward &gt; best_reward:
                best_reward = total_reward
            if verbose:
                str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
                if isinstance(agent, MemoryAgent):
                    mem_len = len(next(iter(agent.memory.values())))
                    mem_str = f&#39; - Memory Size: {mem_len}&#39;
                else:
                    mem_str = &#39;&#39;
                print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                      f&#39;Steps: {step} - &#39;
                      f&#39;Total Reward: {total_reward} - &#39;
                      f&#39;Best Total Reward: {best_reward} - &#39;
                      f&#39;Average Total Reward: {total_rewards / episode}&#39;
                      f&#39;{mem_str}&#39;)
            if end_episode_callback is not None:
                end = end_episode_callback(
                    episode, step, total_reward
                )
                if end:
                    break
        return total_rewards / episode

    def close(self):
        &#34;&#34;&#34;Closes any threads or loose ends of the environment.
        &#34;&#34;&#34;
        pass

    def render(self):
        &#34;&#34;&#34;Renders the environment.
        &#34;&#34;&#34;
        pass


class GymWrapper(Environment):
    &#34;&#34;&#34;This class is a environment wrapper for OpenAI Gyms.&#34;&#34;&#34;

    def __init__(self, genv):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            genv: An OpenAI Gym
        &#34;&#34;&#34;
        self.genv = genv
        if isinstance(self.genv.observation_space, gym.spaces.Discrete):
            self.discrete_state_space = self.genv.observation_space.n
            self.state_shape = 1
        elif isinstance(self.genv.observation_space, gym.spaces.Box):
            self.discrete_state_space = None
            self.state_shape = self.genv.observation_space.shape
        else:
            raise NotImplementedError(&#39;Only Discrete and Box &#39;
                                      &#39;observation spaces &#39;
                                      &#39;are supported&#39;)

        if isinstance(self.genv.action_space, gym.spaces.Discrete):
            self.action_size = self.genv.action_space.n
        elif isinstance(self.genv.observation_space, gym.spaces.Box):
            if len(self.genv.action_space.shape) &gt; 1:
                raise NotImplementedError(&#39;Box action spaces with more &#39;
                                          &#39;than one dimension are not &#39;
                                          &#39;supported&#39;)
            self.action_size = self.genv.action_space.shape[0]
        else:
            raise NotImplementedError(&#39;Only Discrete action &#39;
                                      &#39;spaces are supported&#39;)

    def reset(self):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = self.genv.reset()
        if self.discrete_state_space is None:
            return self.state
        else:
            return [self.state]

    def step(self, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the action.

        Args:
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state, reward, terminal, _ = self.genv.step(action)
        if self.discrete_state_space is None:
            return self.state, reward, terminal
        else:
            return [self.state], reward, terminal

    def close(self):
        &#34;&#34;&#34;Closes any threads or loose ends of the environment.
        &#34;&#34;&#34;
        self.genv.close()

    def render(self):
        &#34;&#34;&#34;Renders the environment.
        &#34;&#34;&#34;
        self.genv.render()


class MultiSeqAgentEnvironment(Environment):
    &#34;&#34;&#34;This class handles the environment in which multiple agents
       can perform actions against eachother in a sequential manner.
    &#34;&#34;&#34;

    def __init__(self, state_shape, action_size):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            state_shape: A tuple of integers, which is the
                         expected state shape for the agent
            action_size: An integer which is the discrete size
                         of the action space
        &#34;&#34;&#34;
        if isinstance(state_shape, int):
            self.discrete_state_space = state_shape
            state_shape = 1
        else:
            self.discrete_state_space = None
        self.state_shape = state_shape
        self.action_size = action_size

    def reset(self, num_agents):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Args:
            num_agents: An integer, which is the number of states needed

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = None
        return [self.state] * num_agents

    def step(self, agent_ndx, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the agent&#39;s action.

        Args:
            agent_ndx: An integer, which is the index of the
                       agent taking a step
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state = None
        return self.state, 0, False

    def play_episode(self, agents, max_steps, shuffle=True,
                     random=False, random_bounds=None,
                     render=False, verbose=True):
        &#34;&#34;&#34;Plays a single complete episode with the agents.

        Args:
            agents: A list of Agent instances, which will be used to
                    interact in the environment
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            shuffle: A boolean, which determines if the agents&#39; positions
                     should be shuffled
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen

        Returns:
            A tuple of a list of integers (last steps)
                and a list of floats (total rewards)
        &#34;&#34;&#34;
        num_agents = len(agents)
        ndxs = np.arange(num_agents)
        if shuffle:
            np.random.shuffle(ndxs)
        for ndx in ndxs:
            if not isinstance(agents[ndx], Agent):
                raise TypeError(f&#39;The instance agent ({ndx}) is &#39;
                                f&#39;not a child of Agent.&#39;)
            if not isinstance(agents[ndx].playing_data, PlayingData):
                raise ValueError(f&#39;Invalid playing_data value for agent &#39;
                                 f&#39;{ndx}. (Forgot to set playing_data?)&#39;)
        total_rewards = [0] * num_agents
        states = self.reset(num_agents)
        if render:
            self.render()
        break_loop = [False] * num_agents
        for step in range(1, max_steps + 1):
            for ndx in ndxs:
                if random:
                    if random_bounds is None:
                        action = np.random.randint(0, self.action_size)
                    else:
                        action = np.random.uniform(*random_bounds,
                                                   size=self.action_size)
                else:
                    action = agents[ndx].select_action(
                        states[ndx], training=agents[ndx].playing_data.training
                    )
                new_state, reward, terminal = self.step(ndx, action)
                total_rewards[ndx] += reward

                if agents[ndx].playing_data.memorizing:
                    agents[ndx].add_memory(states[ndx], action, new_state,
                                           reward, terminal)
                states[ndx] = new_state

                if verbose:
                    print(f&#39;Step: {step} - Agent: {ndx} - &#39;
                          f&#39;Reward: {reward} - Action: {action}&#39;)
                if render:
                    self.render()
                if (agents[ndx].playing_data.training
                        and agents[ndx].playing_data.learns_in_episode
                        and agents[ndx].playing_data.epochs &gt; 0):
                    agents[ndx].learn(
                        **agents[ndx].playing_data.learning_params
                    )
                if terminal:
                    break_loop[ndx] = True
                    if False not in break_loop:
                        break
            if False not in break_loop:
                break
        for ndx in ndxs:
            agents[ndx].end_episode()
            if (agents[ndx].playing_data.training
                    and not agents[ndx].playing_data.learns_in_episode
                    and agents[ndx].playing_data.epochs &gt; 0):
                agents[ndx].learn(**agents[ndx].playing_data.learning_params)
        return step, total_rewards

    def play_episodes(self, agents, num_episodes, max_steps, shuffle=True,
                      random=False, random_bounds=None, render=False,
                      verbose=True, episode_verbose=None,
                      end_episode_callback=None):
        &#34;&#34;&#34;Plays at least 1 complete episode with the agents.

        Args:
            agents: A list of Agent instances, which will be used to
                    interact in the environment
            num_episodes: An integer, which is the number of episodes to play
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            shuffle: A boolean, which determines if the agents&#39; positions
                     should be shuffled
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen
            episode_verbose: A boolean, which determines if single episode
                             information should be printed to the screen
            end_episode_callback: A function called at the end of each episode
                                  with episode count, steps, and total reward
                                  from the most recent episode. If True
                                  is returned, play_episodes will stop
                                  early.

        Returns:
            A list of floats, which are the average total reward of all
                episodes for each agent
        &#34;&#34;&#34;
        if episode_verbose is None:
            episode_verbose = verbose
        num_agents = len(agents)
        total_rewards = [0] * num_agents
        best_rewards = [&#39;Unknown&#39;] * num_agents
        for episode in range(1, num_episodes + 1):
            step, total_reward = self.play_episode(
                agents, max_steps, shuffle=shuffle, random=random,
                random_bounds=random_bounds, render=render,
                verbose=episode_verbose,
            )
            for ndx in range(num_agents):
                total_rewards[ndx] += total_reward[ndx]
                if (best_rewards[ndx] == &#39;Unknown&#39;
                        or total_reward[ndx] &gt; best_rewards[ndx]):
                    best_rewards[ndx] = total_reward[ndx]
            if verbose:
                str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
                print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                      f&#39;Steps: {step}&#39;)
                for ndx in range(num_agents):
                    avg_total_reward = total_rewards[ndx] / episode
                    print(f&#39;Agent {ndx}. &#39;
                          f&#39;Total Reward: {total_reward[ndx]} - &#39;
                          f&#39;Best Total Reward: {best_rewards[ndx]} - &#39;
                          f&#39;Average Total Reward: {avg_total_reward}&#39;)
            if end_episode_callback is not None:
                end = end_episode_callback(
                    episode, step, total_reward
                )
                if end:
                    break
        return [tr / episode for tr in total_rewards]


class Policy:
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initalizes the Policy.&#34;&#34;&#34;
        pass

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        return action_func()

    def reset(self):
        &#34;&#34;&#34;Resets any states.&#34;&#34;&#34;
        pass

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended.&#34;&#34;&#34;
        pass


class GreedyPolicy(Policy):
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function and
       selecting the greediest action.
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        action = np.argmax(action_func())
        return action


class AsceticPolicy(Policy):
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function and
       selecting the most ascetic action.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initalizes the Policy.&#34;&#34;&#34;
        super().__init__()

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        action = np.argmin(action_func())
        return action


class StochasticPolicy(Policy):
    def __init__(self, policy, stochasticity_decay_training,
                 stochasticity_testing, action_size):
        &#34;&#34;&#34;Initalizes the Policy&#39;s states.

        Args:
            policy: A policy instance
            stochasticity_decay_training: A decay instance which decays
                                          the stochasticity of the policy
            stochasticity_testing: A float, which is the stochasticity
                                   of the policy when the agent is not
                                   training
            action_size: An integer, which is the size of the action ndarray
        &#34;&#34;&#34;
        super().__init__()
        self.policy = policy
        self.stochasticity_decay_training = stochasticity_decay_training
        self.stochasticity_testing = stochasticity_testing
        self.action_size = action_size

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        if training:
            stochasticity = self.stochasticity_decay_training()
        else:
            stochasticity = self.stochasticity_testing
        if np.random.uniform() &lt; stochasticity:
            return np.random.randint(0, self.action_size)
        else:
            return self.policy.select_action(action_func, training)

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
        self.stochasticity_decay_training.step()

    def reset(self):
        &#34;&#34;&#34;Resets state of the stochasticity decay instance.&#34;&#34;&#34;
        self.stochasticity_decay_training.reset()


class NoisePolicy(Policy):
    &#34;&#34;&#34;This class is used for adding normal noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds):
        &#34;&#34;&#34;Initalizes the Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
        &#34;&#34;&#34;
        self.noise_scale_decay_training = noise_scale_decay_training
        self.noise_scale_testing = noise_scale_testing
        self.action_bounds = action_bounds

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        noise = np.random.normal(scale=noise_scale, size=actions.shape)
        return np.clip(actions + noise, *self.action_bounds)

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
        self.noise_scale_decay_training.step()

    def reset(self):
        &#34;&#34;&#34;Resets decay state.&#34;&#34;&#34;
        self.noise_scale_decay_training.reset()


class UniformNoisePolicy(NoisePolicy):
    &#34;&#34;&#34;This class is used for adding noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds, additive=False):
        &#34;&#34;&#34;Initalizes the Uniform Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
            additive: A boolean, which determines if the noise should be
                      added or replace the action value completely
        &#34;&#34;&#34;
        super().__init__(noise_scale_decay_training,
                         noise_scale_testing, action_bounds)
        self.additive = additive

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        noise = np.random.uniform(*self.action_bounds,
                                  size=actions.shape)
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        if self.additive:
            return np.clip(actions + noise * noise_scale, *self.action_bounds)
        else:
            return np.where(
                np.random.uniform(size=actions.shape) &lt; noise_scale,
                noise, actions
            )


class TemporalNoisePolicy(NoisePolicy):
    &#34;&#34;&#34;This class is used for adding temporal noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds,
                 sigma=.3, theta=.15, dt=.01, init_noise=None):
        &#34;&#34;&#34;Initalizes the Temporal Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
        &#34;&#34;&#34;
        super().__init__(noise_scale_decay_training,
                         noise_scale_testing, action_bounds)
        self.sigma = sigma
        self.theta = theta
        self.dt = dt
        self.sqrt_dt = np.sqrt(self.dt)
        if init_noise is None:
            self.init_noise = None
        else:
            self.init_noise = init_noise
        self.last_noise = None

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        if self.init_noise is None:
            self.init_noise = np.full(actions.shape,
                                      np.mean(self.action_bounds))
            self.last_noise = self.init_noise
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        if noise_scale == 0:
            return actions
        noise = np.random.normal(scale=noise_scale, size=actions.shape)
        noise = (self.last_noise +
                 self.theta * -self.last_noise * self.dt +
                 self.sigma * self.sqrt_dt * noise)
        self.last_noise = noise
        return np.clip(actions + noise, *self.action_bounds)

    def reset(self):
        &#34;&#34;&#34;Resets decay state and initial actions.&#34;&#34;&#34;
        super().reset()
        self.last_noise = self.init_noise


class Decay:
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value through a given number of steps.
       (formula: max(initial_value - constant * steps, 0))
    &#34;&#34;&#34;

    def __init__(self, initial_value, constant,
                 min_value=0, step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            constant: A float, which is the slope/rate that the decay occurs
            min_value: A float, which is the minimum value the decay can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        self.initial_value = initial_value
        self.constant = constant
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.steps = 0

    def reset(self):
        &#34;&#34;&#34;Resets the steps.&#34;&#34;&#34;
        self.steps = 0

    def step(self):
        &#34;&#34;&#34;Steps the decay forward.&#34;&#34;&#34;
        self.steps += 1

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.initial_value - self.constant * self.steps
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)


class ExponentialDecay(Decay):
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value exponentially through a given number of steps.
       (formula: inital_value * (1 - rate)^steps + min_value)
    &#34;&#34;&#34;

    def __init__(self, initial_value, rate, min_value=0,
                 step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            rate: A float, which is the slope/rate that the decay occurs
            min_value: A float, which is the minimum value the decay can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        self.initial_value = initial_value
        self.rate = rate
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.steps = 0

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.initial_value * (1 - self.rate)**self.steps
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)


class LinearDecay(Decay):
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value linearly through a given number of steps.
       (formula: max(initial_value - (inital_value - min_value)
                     / total_steps * steps, min_value))
    &#34;&#34;&#34;

    def __init__(self, initial_value, total_steps,
                 min_value=0, step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            total_steps: An integer, which is the number of steps until
                         min_value would be reach
            min_value: A float, which is the minimum value the decay
                       can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        if not isinstance(total_steps, int):
            raise TypeError(&#39;total_steps should be an integer&#39;)
        self.initial_value = initial_value
        self.total_steps = total_steps
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.a = (-1 * (self.initial_value - self.min_value) /
                  self.total_steps)
        self.steps = 0

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.a * self.steps + self.initial_value
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)


class Memory:
    &#34;&#34;&#34;This class is used by agents to store episode information.
       (uses a normal python list)
    &#34;&#34;&#34;

    def __init__(self, max_len=None):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        self.max_len = max_len
        self.buffer = []

    def __len__(self):
        &#34;&#34;&#34;Returns the number of entries in the memory.

        Returns:
            An integer
        &#34;&#34;&#34;
        return len(self.buffer)

    def add(self, x):
        &#34;&#34;&#34;Adds a entry to memory.

        Args:
            x: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer.append(x)
        if (self.max_len is not None
                and len(self.buffer) &gt; self.max_len):
            del self.buffer[0]

    def __getitem__(self, key):
        &#34;&#34;&#34;Returns an item given a key.

        Args:
            key: A valid key or index for a memory entry
        &#34;&#34;&#34;
        return self.buffer[key]

    def __setitem__(self, key, value):
        &#34;&#34;&#34;Sets a entry to a given key.

        Args:
            key: A valid key or index for a memory entry
            value: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer[key] = value

    def array(self):
        &#34;&#34;&#34;Returns a copy of the memory.

        Returns:
            A numpy ndarray
        &#34;&#34;&#34;
        return np.array(self.buffer)

    def reset(self):
        &#34;&#34;&#34;Resets or clears the memory.
        &#34;&#34;&#34;
        self.buffer.clear()

    def end_episode(self):
        &#34;&#34;&#34;Tells memory an episode ended.
        &#34;&#34;&#34;
        pass

    def save(self, file, name):
        &#34;&#34;&#34;Creates a h5py dataset with the memory data.

        Args:
            file: A h5py open file for writing
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        file.create_dataset(name, data=self.array())

    def load(self, file, name):
        &#34;&#34;&#34;Loads a h5py dataset with the saved memory data.

        Args:
            file: A h5py open file for reading
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        for element in file[name]:
            self.add(element)

    @staticmethod
    def create_shuffled_subset(memories, subset_size, weights=None):
        &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

        Args:
            memories: A list of Memeory Objects
            subset_size: A integer, which is the size of the
                         outer dimension of each ndarray
            weights: A list of probabilities that add up to 1

        Returns:
            arrays and shuffled indexes
        &#34;&#34;&#34;
        length = len(memories[0])
        if subset_size &gt; length:
            raise ValueError(f&#39;Subset size {subset_size} is &#39;
                             f&#39;greater than memory length {length}&#39;)
        for memory in memories:
            if len(memory) != length:
                raise ValueError(&#39;Memories are not all the same length.&#39;)
            if not isinstance(memory, Memory):
                raise TypeError(&#39;Memories must also be Memory &#39;
                                &#39;or subclass instances&#39;)
        indexes = np.random.choice(np.arange(length),
                                   size=subset_size, replace=False,
                                   p=weights)
        arrays = [np.empty((subset_size, *memory[0].shape))
                  if isinstance(memory[0], np.ndarray)
                  else np.empty(subset_size)
                  for memory in memories]
        for ndx, rndx in enumerate(indexes):
            for array, memory in zip(arrays, memories):
                array[ndx] = memory[rndx]
        return arrays, indexes


class ETDMemory(Memory):
    &#34;&#34;&#34;This class is for the efficient storage of time distributed states.
       This type of memory should only be used for states.
    &#34;&#34;&#34;

    def __init__(self, num_time_steps, void_state, max_len=None):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            num_time_steps: An integer, which is the number of
                            states that make up a complete state
            void_state: A ndarray, which is used when there is not
                        enough states to create a complete state
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        if max_len is not None:
            raise NotImplementedError(&#39;max_len is not yet implemented&#39;)
        self.num_time_steps = num_time_steps
        self.max_len = max_len
        self.buffer = [void_state]
        self.ndxs = []
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def __len__(self):
        &#34;&#34;&#34;Returns the number of entries in the memory.

        Returns:
            An integer
        &#34;&#34;&#34;
        return len(self.ndxs)

    def add(self, x):
        &#34;&#34;&#34;Adds a entry to memory.

        Args:
            x: A entry similar to other entries
        &#34;&#34;&#34;
        self.step_ndxs = np.roll(self.step_ndxs, -1)
        self.step_ndxs[-1] = len(self.buffer)
        self.ndxs.append(self.step_ndxs)
        self.buffer.append(x)

    def __getitem__(self, key):
        &#34;&#34;&#34;Returns an item given a key.

        Args:
            key: A valid key or index for a memory entry
        &#34;&#34;&#34;
        return self.buffer[key + 1 if key &gt;= 0 else key]

    def __setitem__(self, key, value):
        &#34;&#34;&#34;Sets a entry to a given key.

        Args:
            key: A valid key or index for a memory entry
            value: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer[key + 1 if key &gt;= 0 else key] = value

    def array(self):
        &#34;&#34;&#34;Returns a copy of the memory.

        Returns:
            A numpy ndarray
        &#34;&#34;&#34;
        return np.array(self.buffer)[np.array(self.ndxs)]

    def reset(self):
        &#34;&#34;&#34;Resets or clears the memory.
        &#34;&#34;&#34;
        void_state = self.buffer[0]
        self.buffer.clear()
        self.buffer.append(void_state)
        self.ndxs.clear()
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def end_episode(self):
        &#34;&#34;&#34;Tells memory an episode ended.
        &#34;&#34;&#34;
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def save(self, file, name):
        &#34;&#34;&#34;Creates a h5py dataset with the memory data.

        Args:
            file: A h5py open file for writing
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        file.create_dataset(f&#39;{name}_buffer&#39;, data=np.array(self.buffer))
        file.create_dataset(f&#39;{name}_ndxs&#39;, data=np.array(self.ndxs))

    def load(self, file, name):
        &#34;&#34;&#34;Loads a h5py dataset with the saved memory data.

        Args:
            file: A h5py open file for reading
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        for element in file[f&#39;{name}_ndxs&#39;]:
            self.ndxs.append(element)
            if element.shape != (self.num_time_steps,):
                raise ValueError(&#39;Cannot load dataset: &#39;
                                 &#39;invalid number of time steps&#39;)

        for element in file[f&#39;{name}_buffer&#39;]:
            self.buffer.append(element)

    @staticmethod
    def create_shuffled_subset(memories, subset_size, weights=None):
        &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

        Args:
            memories: A list of Memeory Objects (not asserted but assumed)
            subset_size: A integer, which is the size of the
                         outer dimension of each ndarray
            weights: A list of probabilities that add up to 1

        Returns:
            arrays and shuffled indexes
        &#34;&#34;&#34;
        length = len(memories[0])
        if subset_size &gt; length:
            raise ValueError(f&#39;Subset size {subset_size} is &#39;
                             f&#39;greater than memory length {length}&#39;)
        for memory in memories:
            if len(memory) != length:
                raise ValueError(&#39;Memories are not all the same length.&#39;)
            if not isinstance(memory, Memory):
                raise TypeError(&#39;Memories must also be Memory &#39;
                                &#39;or subclass instances&#39;)
        indexes = np.random.choice(np.arange(len(memories[0])),
                                   size=subset_size, replace=False,
                                   p=weights)
        arrays = []
        for memory in memories:
            if isinstance(memory, ETDMemory):
                arrays.append(np.empty((subset_size,
                                        memory.num_time_steps,
                                        *memory.buffer[0].shape)))
            elif isinstance(memory[0], np.ndarray):
                arrays.append(np.empty((subset_size, *memory[0].shape)))
            else:
                arrays.append(np.empty(subset_size))
        for ndx, rndx in enumerate(indexes):
            for array, memory in zip(arrays, memories):
                if isinstance(memory, ETDMemory):
                    for andx, sndx in enumerate(memory.ndxs[rndx]):
                        array[ndx, andx] = memory.buffer[sndx]
                else:
                    array[ndx] = memory[rndx]
        return arrays, indexes


class RingMemory(Memory):
    def __init__(self, max_len):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        Memory.__init__(self, max_len=max_len)


class PlayingData:
    &#34;&#34;&#34;This class is used for containing data
       that the environment needs to know, but the agent has.
    &#34;&#34;&#34;

    def __init__(self, training, memorizing, epochs,
                 learns_in_episode, learning_params):
        &#34;&#34;&#34;Initalizes the data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            learning_Args: A dictionary of parameters for the agent&#39;s
                             learn method
        &#34;&#34;&#34;
        if not (training is True or training is False):
            raise ValueError(&#39;Invalid training value. Must be True or False.&#39;)
        if not (memorizing is True or memorizing is False):
            raise ValueError(
                &#39;Invalid memorizing value. Must be True or False.&#39;)
        if epochs &lt; 0:
            raise ValueError(&#39;Invalid epoch value. Must &#39;
                             &#39;be greater or equal to zero.&#39;)
        if not (learns_in_episode is True or learns_in_episode is False):
            raise ValueError(&#39;Invalid learns_in_episode value. &#39;
                             &#39;Must be True or False.&#39;)
        if not isinstance(learning_params, dict):
            raise TypeError(&#39;Invalid learning_params value. &#39;
                            &#39;Must be a dictionary.&#39;)
        self.training = training
        self.memorizing = memorizing
        self.epochs = epochs
        self.learns_in_episode = learns_in_episode
        self.learning_params = learning_params


class Agent:
    &#34;&#34;&#34;This class is the base class for all agent classes,
       and essentially is a random agent.
    &#34;&#34;&#34;

    def __init__(self, action_size, policy):
        &#34;&#34;&#34;Initalizes the agent.

        Args:
            action_size: An integer which is the discrete size
                         of the action space
            policy: A policy instance
        &#34;&#34;&#34;
        if not isinstance(action_size, int):
            raise TypeError(&#39;action_size must be an integer&#39;)
        if not isinstance(policy, Policy):
            raise TypeError(&#39;policy must be a Policy instance&#39;)
        self.action_size = action_size
        self.policy = policy
        self.playing_data = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to get the action for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            return np.random.random(self.action_size)
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self):
        &#34;&#34;&#34;Sets the episode data.&#34;&#34;&#34;
        self.playing_data = PlayingData(False, False, 0, False, {})

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.
           (For this agent all memory is discarded)

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        pass

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        pass

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode for the agent.&#34;&#34;&#34;
        self.policy.end_episode()

    def learn(self, verbose=True):
        &#34;&#34;&#34;Trains the agent on a batch of its experiences.
           (For this agent no learning is needed)

        Args:
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        with open(os.path.join(path, &#39;note.txt&#39;), &#39;r&#39;) as file:
            note = file.read()
        return note

    def save(self, path, note):
        &#34;&#34;&#34;Saves a note to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        time = datetime.now()
        path = os.path.join(path, time.strftime(r&#39;%Y%m%d_%H%M%S_%f&#39;))
        os.mkdir(path)
        with open(os.path.join(path, &#39;note.txt&#39;), &#39;w&#39;) as file:
            file.write(note)
        return path


class QAgent(Agent):
    &#34;&#34;&#34;This class is a Q-learning Agent. It does not uses a neural network,
       but instead uses a table.
    &#34;&#34;&#34;

    def __init__(self, discrete_state_space, action_size,
                 policy, discounted_rate):
        &#34;&#34;&#34;Initalizes the Q-learning agent.

        Args:
            discrete_state_space: An integer, which is the size of
                                  the state space
            action_size: An integers, which is the
                         action size of the environment
            policy: A policy instance
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.discrete_state_space = discrete_state_space
        self.discounted_rate = discounted_rate
        self.qtable = np.zeros((self.discrete_state_space,
                                self.action_size))
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to look up the action for in the table
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            nonlocal state
            if isinstance(state, list) and len(state) == 1:
                state = state[0]
            return self.qtable[state]
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, learning_rate=None,
                         verbose=False):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            learning_rate: A float, which is the rate that the table
                           is updated with the currect Q reward
                           (Must be provided if training is True)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.playing_data = PlayingData(training, training, 1, True,
                                        {&#39;learning_rate&#39;: learning_rate,
                                         &#39;verbose&#39;: verbose})

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        if isinstance(state, list) and len(state) == 1:
            state = state[0]
        if isinstance(new_state, list) and len(new_state) == 1:
            new_state = new_state[0]
        self.state = state
        self.action = action
        self.new_state = new_state
        self.reward = reward
        self.terminal = terminal

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def learn(self, learning_rate, verbose=True):
        &#34;&#34;&#34;Trains the agent on its last experience.

        Args:
            learning_rate: A float, which is the rate that the table
                           is updated with the currect Q reward
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        if self.state is None:
            raise ValueError(&#39;Memory is empty&#39;)
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (self.discounted_rate *
                                 np.amax(self.qtable[self.new_state]))
        self.qtable[self.state, self.action] = (
            (1 - learning_rate) * self.qtable[self.state, self.action] +
            learning_rate * (self.reward + discounted_reward)
        )
        if verbose:
            pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        self.qtable = np.load(os.path.join(path, &#39;qtable.npy&#39;))
        return note

    def save(self, path, note=&#39;QAgent Save&#39;):
        &#34;&#34;&#34;Saves a note and qtable to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        np.save(os.path.join(path, &#39;qtable.npy&#39;), self.qtable)
        return path


class PQAgent(QAgent):
    &#34;&#34;&#34;This class is like QAgent, but it uses multiple variables at once,
       hince Parallel Q Agent.
    &#34;&#34;&#34;

    def __init__(self, discrete_state_space, action_size,
                 policy, discounted_rates, learning_rates):
        &#34;&#34;&#34;Initalizes the Q-learning agent.

        Args:
            discrete_state_space: An integer, which
            action_size: An integers, which is the
                         action size of the environment
            policy: A policy instance
            discounted_rates: A list of floats within 0.0-1.0, which
                              are the rates that future rewards should
                              be counted for the current reward
            learning_rate: A list of floats, which are the rates
                           that the table is updated with the currect
                           Q reward
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.discrete_state_space = discrete_state_space
        self.discounted_rates = np.array(discounted_rates)
        self.learning_rates = np.array(learning_rates)
        self.inv_learning_rates = 1 - self.learning_rates
        self.qtables = np.zeros((len(self.learning_rates),
                                 len(self.discounted_rates),
                                 self.discrete_state_space,
                                 self.action_size))
        self.selected_qtable = lambda: self.qtables[0, 0]
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to look up the action for in the table
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            nonlocal state
            if isinstance(state, list) and len(state) == 1:
                state = state[0]
            return self.selected_qtable()[state]
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, learning_rate_ndx=None,
                         discounted_rate_ndx=None, verbose=False):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            learning_rate_ndx: An integer, which is a ndx for
                               the learning rates
            discounted_rate_ndx: An integer, which is a ndx for
                                 the discounted rate
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        lrn = 0
        drn = 0
        if learning_rate_ndx is not None:
            lrn = learning_rate_ndx
        if discounted_rate_ndx is not None:
            drn = discounted_rate_ndx
        self.selected_qtable = lambda: self.qtables[lrn, drn]
        self.playing_data = PlayingData(
            training, training, 1, True,
            {&#39;learning_rate_ndx&#39;: learning_rate_ndx,
             &#39;discounted_rate_ndx&#39;: discounted_rate_ndx,
             &#39;verbose&#39;: verbose}
        )

    def learn(self, learning_rate_ndx=None,
              discounted_rate_ndx=None, verbose=True):
        &#34;&#34;&#34;Trains the agent on its last experience.

        Args:
            learning_rate_ndx: An integer, which is a ndx for
                               the learning rates
            discounted_rate_ndx: An integer, which is a ndx for
                                 the discounted rate
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        if self.state is None:
            raise ValueError(&#39;Memory is empty&#39;)
        lrn = learning_rate_ndx
        drn = discounted_rate_ndx
        if lrn is None and drn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates *
                    np.amax(self.qtables[:, :, self.new_state], axis=-1)
                )
            self.qtables[:, :, self.state, self.action] = (
                self.inv_learning_rates[:, np.newaxis] *
                self.qtables[:, :, self.state, self.action] +
                self.learning_rates[:, np.newaxis] *
                (self.reward + discounted_reward)
            )
        elif drn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates *
                    np.amax(self.qtables[lrn, :, self.new_state], axis=-1)
                )
            self.qtables[lrn, :, self.state, self.action] = (
                self.inv_learning_rates[lrn, np.newaxis] *
                self.qtables[lrn, :, self.state, self.action] +
                self.learning_rates[lrn, np.newaxis] *
                (self.reward + discounted_reward)
            )
        elif lrn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates[drn] *
                    np.amax(self.qtables[:, drn, self.new_state], axis=-1)
                )
            self.qtables[:, drn, self.state, self.action] = (
                self.inv_learning_rates[:, np.newaxis] *
                self.qtables[:, drn, self.state, self.action] +
                self.learning_rates[:, np.newaxis] *
                (self.reward + discounted_reward)
            )
        else:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates[drn] *
                    np.amax(self.qtables[lrn, drn, self.new_state], axis=-1)
                )
            self.qtables[lrn, drn, self.state, self.action] = (
                self.inv_learning_rates[lrn] *
                self.qtables[lrn, drn, self.state, self.action] +
                self.learning_rates[lrn] *
                (self.reward + discounted_reward)
            )

        if verbose:
            pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
                  from

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        self.qtables = np.load(os.path.join(path, &#39;qtables.npy&#39;))
        return note

    def save(self, path, note=&#39;PQAgent Save&#39;):
        &#34;&#34;&#34;Saves a note and qtables to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        np.save(os.path.join(path, &#39;qtables.npy&#39;), self.qtables)
        return path


class MemoryAgent(Agent):
    &#34;&#34;&#34;This class is the base class for all agent that use memory.
    &#34;&#34;&#34;

    def __init__(self, action_size, policy):
        &#34;&#34;&#34;Initalizes the agent.

        Args:
            action_size: An integer which is the discrete size
                         of the action space
            policy: A policy instance
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.memory = {}
        self.time_distributed_states = None

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        Agent.end_episode(self)
        for memory in self.memory.values():
            memory.reset()

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode for the agent.&#34;&#34;&#34;
        Agent.end_episode(self)
        for memory in self.memory.values():
            memory.end_episode()
        if (&#39;states&#39; in self.memory
                and isinstance(self.memory[&#39;states&#39;], ETDMemory)):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])

    def load(self, path, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

            Returns:
                A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        if load_data:
            with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;r&#39;) as file:
                for name, memory in self.memory.items():
                    memory.load(file, name)
        return note

    def save(self, path, save_data=True, note=&#39;MemoryAgent&#39;):
        &#34;&#34;&#34;Saves a note and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        if save_data:
            with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;w&#39;) as file:
                for name, memory in self.memory.items():
                    memory.save(file, name)
        return path


class DQNAgent(MemoryAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Deep Q Network instead of
       a table like the QAgent. This allows for generalizations and
       large environment states.
    &#34;&#34;&#34;

    @staticmethod
    def get_dueling_output_layer(action_size, dueling_type=&#39;avg&#39;):
        assert dueling_type in [&#39;avg&#39;, &#39;max&#39;, &#39;naive&#39;], (
            &#34;Dueling type must be &#39;avg&#39;, &#39;max&#39;, or &#39;naive&#39;&#34;
        )

        def layer(x1, x2):
            x1 = keras.layers.Dense(1)(x1)
            x2 = keras.layers.Dense(action_size)(x2)
            x = keras.layers.Concatenate()([x1, x2])
            if dueling_type == &#39;avg&#39;:
                def dueling(a):
                    return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                            K.mean(a[:, 1:], axis=1, keepdims=True))
            elif dueling_type == &#39;max&#39;:
                def dueling(a):
                    return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                            K.max(a[:, 1:], axis=1, keepdims=True))
            else:
                def dueling(a):
                    return K.expand_dims(a[:, 0], -1) + a[:, 1:]
            return keras.layers.Lambda(dueling, output_shape=(action_size,),
                                       name=&#39;q_output&#39;)(x)
        return layer

    def __init__(self, policy, qmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=True, enable_double=False,
                 enable_per=False):
        &#34;&#34;&#34;Initalizes the Deep Q Network Agent.

        Args:
            policy: A policy instance
            qmodel: A keras model, which takes the state as input and outputs
                    Q Values
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used
            enable_double: A boolean, which determiens if the Double Deep Q
                           Network should be used
            enable_per: A boolean, which determines if prioritized experience
                        replay should be used (The implementation for this is
                        not the normal tree implementation, and only weights
                        the probabilily of being choosen and not also the
                        gradient)
        &#34;&#34;&#34;
        MemoryAgent.__init__(self, qmodel.output_shape[1], policy)
        self.qmodel = qmodel
        self.qmodel.compiled_loss.build(
            tf.zeros(self.qmodel.output_shape[1:])
        )
        self.target_qmodel = None
        self.enable_target = enable_target or enable_double
        self.enable_double = enable_double
        if self.enable_target:
            self.target_qmodel = keras.models.clone_model(qmodel)
            self.target_qmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
        else:
            self.target_qmodel = self.qmodel
        self.discounted_rate = discounted_rate
        self.states = create_memory(self.qmodel.input_shape,
                                    keras.backend.floatx())
        self.next_states = create_memory(self.qmodel.input_shape,
                                         keras.backend.floatx())
        self.actions = create_memory(self.qmodel.output_shape,
                                     keras.backend.floatx())
        self.rewards = create_memory((None,),
                                     keras.backend.floatx())
        self.terminals = create_memory((None,),
                                       keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;next_states&#39;: self.next_states,
            &#39;actions&#39;: self.actions, &#39;rewards&#39;: self.rewards,
            &#39;terminals&#39;: self.terminals
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        if enable_per:
            self.per_losses = create_memory((None,),
                                            keras.backend.floatx())
            self.memory[&#39;per_losses&#39;] = self.per_losses
            # assuming the true max loss will be less than 100
            # at least at the begining
            self.max_loss = 100.0
        else:
            self.per_losses = None
        self.action_identity = np.identity(self.action_size)
        self.total_steps = 0
        self.metric = keras.metrics.Mean(name=&#39;loss&#39;)

        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the Q values for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.qmodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        def _select_action():
            qvalues = self.qmodel(np.expand_dims(state, axis=0),
                                  training=False)[0].numpy()
            return qvalues
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target model weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.next_states.add(np.array(new_state))
        self.actions.add(self.action_identity[action])
        self.rewards.add(reward)
        self.terminals.add(0 if terminal else 1)
        if self.per_losses is not None:
            self.per_losses.add(self.max_loss)

    def update_target(self, tau):
        &#34;&#34;&#34;Updates the target Q Model weights.

        Args:
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
        &#34;&#34;&#34;
        if tau == 1.0:
            self.target_qmodel.set_weights(self.qmodel.get_weights())
        else:
            tws = self.target_qmodel.trainable_variables
            ws = self.qmodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)

    def _train_step(self, states, next_states,
                    actions, terminals, rewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment

        Returns:
            A loss for this batch of data
        &#34;&#34;&#34;
        if self.enable_double:
            qvalues = self.qmodel(next_states, training=False)
            actions = tf.argmax(qvalues, axis=-1)
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.squeeze(tf.gather(qvalues, actions[:, tf.newaxis],
                                           axis=-1, batch_dims=1))
            actions = tf.one_hot(actions, self.action_size,
                                 dtype=qvalues.dtype)
        else:
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.reduce_max(qvalues, axis=-1)
        qvalues = (rewards +
                   self.discounted_rate * qvalues * terminals)
        with tf.GradientTape() as tape:
            y_pred = self.qmodel(states, training=True)
            if len(self.qmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.qmodel.losses)
            else:
                reg_loss = 0
            y_true = (y_pred * (1 - actions) +
                      qvalues[:, tf.newaxis] * actions)
            loss = self.qmodel.compiled_loss._losses[0].fn(
                y_true, y_pred
            ) + reg_loss
        grads = tape.gradient(loss, self.qmodel.trainable_variables)
        self.qmodel.optimizer.apply_gradients(
            zip(grads, self.qmodel.trainable_variables)
        )
        self.metric(loss)

        return tf.reduce_sum(tf.abs(y_true - y_pred), axis=-1)

    def _train(self, states, next_states, actions, terminals,
               rewards, epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A list of floats, which are the absolute losses for all
                the data
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        losses = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)

            count = 0
            for batch in batches:
                if epoch == epochs:
                    losses.append(self._tf_train_step(*batch).numpy())
                else:
                    self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            loss_results = self.metric.result()
            self.metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;loss: {loss_results}&#39;)
        return np.hstack(losses)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target model weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.per_losses is None:
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards],
                    length
                )
            else:
                per_losses = self.per_losses.array()
                self.max_loss = per_losses.max()
                per_losses = per_losses / per_losses.sum()
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards],
                    length, weights=per_losses
                )
            losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
            if self.per_losses is not None:
                for ndx, loss in zip(indexes, losses):
                    self.per_losses[ndx] = loss

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architecture and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;r&#39;) as file:
                self.qmodel = model_from_json(file.read())
            self.qmodel.load_weights(os.path.join(path, &#39;qweights.h5&#39;))
            if self.enable_target:
                self.target_qmodel = keras.models.clone_model(self.qmodel)
                self.target_qmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
            else:
                self.target_qmodel = self.qmodel
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;DQNAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architecture and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.qmodel.to_json())
            self.qmodel.save_weights(os.path.join(path, &#39;qweights.h5&#39;))
        return path


class PGAgent(MemoryAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Neural Network like the DQN Agent,
       but instead of learning to predict Q values, it predicts actions. It
       learns to predict these actions through Policy Gradients (PG).
    &#34;&#34;&#34;

    def __init__(self, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        output_shape = amodel.output_shape
        if isinstance(output_shape, list):
            output_shape = output_shape[-1]
        MemoryAgent.__init__(self, output_shape[1], Policy())
        self.amodel = amodel
        self.discounted_rate = discounted_rate
        self.states = create_memory(self.amodel.input_shape,
                                    keras.backend.floatx())
        self.actions = create_memory(output_shape,
                                     keras.backend.floatx())
        self.drewards = create_memory((None,),
                                      keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;actions&#39;: self.actions,
            &#39;drewards&#39;: self.drewards,
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        self.episode_rewards = []
        self.action_identity = np.identity(self.action_size)
        self.metric = keras.metrics.Mean(name=&#39;loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the action for
            training: A boolean, which determines if the
                      agent is training (does nothing)

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        actions = self.amodel(np.expand_dims(state, axis=0),
                              training=False)
        if isinstance(actions, list):
            return actions[-1][0].numpy()
        return np.random.choice(np.arange(self.action_size),
                                p=actions[0].numpy())

    def set_playing_data(self, training=False, memorizing=False,
                         batch_size=None, mini_batch=0, epochs=1,
                         repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;entropy_coef&#39;: entropy_coef,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        False, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(self.action_identity[action])
        self.episode_rewards.append(reward)

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        if len(self.episode_rewards) &gt; 0:
            dreward = 0
            dreward_list = []
            for reward in reversed(self.episode_rewards):
                dreward *= self.discounted_rate
                dreward += reward
                dreward_list.append(dreward)
            self.episode_rewards.clear()
            for dreward in reversed(dreward_list):
                self.drewards.add(dreward)

        MemoryAgent.end_episode(self)

    def _train_step(self, states, drewards, actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            y_pred = self.amodel(states, training=True)
            log_y_pred = tf.math.log(y_pred + keras.backend.epsilon())
            log_probs = tf.reduce_sum(
                actions * log_y_pred, axis=1
            )
            loss = -tf.reduce_mean(drewards * log_probs)
            entropy = tf.reduce_sum(
                y_pred * log_y_pred, axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)

    def _train(self, states, drewards, actions,
               epochs, batch_size, entropy_coef, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            actions: A numpy array that contains the actions performed
                     (onehot encodings for discrete action spaces)
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean loss of batches (not exactly a loss)
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             actions.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            loss_results = self.metric.result()
            self.metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;loss: {loss_results}&#39;)
        return loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions], length
            )
            std = arrays[1].std()
            if std == 0:
                return False
            arrays[1] = (arrays[1] - arrays[1].mean()) / std
            self._train(*arrays, epochs, batch_size,
                        entropy_coef, verbose=verbose)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architecture and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(file.read())
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return note

    def save(self, path, save_model=True, save_data=True, note=&#39;PGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architecture and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return path


class DDPGAgent(MemoryAgent):
    &#34;&#34;&#34;This class (Deep Deterministic Policy Gradient Agent) is an Agent
       that uses two Neural Networks. An Actor network, which is like
       a PGAgent Network and a Critic Network like the DQNAgent
       Network. The critic rates the actions of the actor.
    &#34;&#34;&#34;

    def __init__(self, policy, amodel, cmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=False):
        &#34;&#34;&#34;Initalizes the DDPG Agent.

        Args:
            policy: A NoisePolicy instance
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state and a action as input
                     and outputs Q Values (a judgement)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used for the critic
        &#34;&#34;&#34;
        if not isinstance(policy, NoisePolicy):
            raise ValueError(&#39;The policy parameter must be a &#39;
                             &#39;instance of NoisePolicy.&#39;)
        MemoryAgent.__init__(self, amodel.output_shape[1], policy)
        self.amodel = amodel
        self.cmodel = cmodel
        if isinstance(self.cmodel.output_shape, list):
            self.cmodel.compiled_loss.build(
                tf.zeros(self.cmodel.output_shape[0][1:])
            )
        else: 
            self.cmodel.compiled_loss.build(
                tf.zeros(self.cmodel.output_shape[1:])
            )
        self.target_cmodel = None
        self.enable_target = enable_target
        self.discounted_rate = discounted_rate
        if self.enable_target:
            self.target_amodel = keras.models.clone_model(amodel)
            self.target_amodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
            self.target_cmodel = keras.models.clone_model(cmodel)
            self.target_cmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
        else:
            self.target_amodel = self.amodel
            self.target_cmodel = self.cmodel

        self.states = create_memory(self.amodel.input_shape,
                                    keras.backend.floatx())
        self.next_states = create_memory(self.amodel.input_shape,
                                         keras.backend.floatx())
        self.actions = create_memory(self.amodel.output_shape,
                                     keras.backend.floatx())
        self.rewards = create_memory((None,),
                                     keras.backend.floatx())
        self.terminals = create_memory((None,),
                                       keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;next_states&#39;: self.next_states,
            &#39;actions&#39;: self.actions, &#39;rewards&#39;: self.rewards,
            &#39;terminals&#39;: self.terminals
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        self.total_steps = 0
        self.metric_c = keras.metrics.Mean(name=&#39;critic_loss&#39;)
        self.metric_a = keras.metrics.Mean(name=&#39;actor_loss&#39;)

        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.output_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to predict the actions for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value or list of values, which is the
                selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        def _select_action():
            actions = self.amodel(np.expand_dims(state, axis=0),
                                  training=False)[0].numpy()
            return actions
        actions = self.policy.select_action(_select_action,
                                            training=training)
        return actions

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the
                    action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.next_states.add(np.array(new_state))
        self.actions.add(action)
        self.rewards.add(reward)
        self.terminals.add(0 if terminal else 1)

    def update_target(self, tau):
        &#34;&#34;&#34;Updates the target Actor and Critic Model weights.

        Args:
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
        &#34;&#34;&#34;
        if tau == 1.0:
            self.target_amodel.set_weights(self.amodel.get_weights())
            self.target_cmodel.set_weights(self.cmodel.get_weights())
        else:
            tws = self.target_amodel.trainable_variables
            ws = self.amodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)
            tws = self.target_cmodel.trainable_variables
            ws = self.cmodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)

    def _train_step(self, states, next_states, actions, terminals, rewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains the actions performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
        &#34;&#34;&#34;
        next_actions = self.target_amodel(next_states, training=False)
        next_qvalues = tf.squeeze(
            self.target_cmodel([next_states, next_actions], training=False)
        )
        qvalues_true = (rewards +
                        self.discounted_rate * next_qvalues * terminals)

        # Critic
        with tf.GradientTape() as tape:
            qvalues_pred = tf.squeeze(
                self.cmodel([states, actions], training=True)
            )
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred
            )
            loss = tf.reduce_mean(loss) + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        # Actor
        with tf.GradientTape() as tape:
            action_preds = self.amodel(states, training=True)
            loss = -tf.reduce_mean(tf.squeeze(
                self.cmodel([states, action_preds], training=False)
            ))
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric_a(loss)

    def _train(self, states, next_states, actions, terminals, rewards,
               epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains the actions performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            critic_loss_results = self.metric_c.result()
            actor_loss_results = self.metric_a.result()
            self.metric_c.reset_states()
            self.metric_a.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and predictions are
                    repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length
            )
            self._train(*arrays, epochs, batch_size, verbose=verbose)

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(file.read())
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
                self.cmodel = model_from_json(file.read())
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
            self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return note

    def save(self, path, save_model=True, save_data=True,
             note=&#39;DDPGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, weights of the models, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.cmodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
            self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return path</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="paiutils.reinforcement.Agent"><code class="flex name class">
<span>class <span class="ident">Agent</span></span>
<span>(</span><span>action_size, policy)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is the base class for all agent classes,
and essentially is a random agent.</p>
<p>Initalizes the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integer which is the discrete size
of the action space</dd>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Agent:
    &#34;&#34;&#34;This class is the base class for all agent classes,
       and essentially is a random agent.
    &#34;&#34;&#34;

    def __init__(self, action_size, policy):
        &#34;&#34;&#34;Initalizes the agent.

        Args:
            action_size: An integer which is the discrete size
                         of the action space
            policy: A policy instance
        &#34;&#34;&#34;
        if not isinstance(action_size, int):
            raise TypeError(&#39;action_size must be an integer&#39;)
        if not isinstance(policy, Policy):
            raise TypeError(&#39;policy must be a Policy instance&#39;)
        self.action_size = action_size
        self.policy = policy
        self.playing_data = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to get the action for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            return np.random.random(self.action_size)
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self):
        &#34;&#34;&#34;Sets the episode data.&#34;&#34;&#34;
        self.playing_data = PlayingData(False, False, 0, False, {})

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.
           (For this agent all memory is discarded)

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        pass

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        pass

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode for the agent.&#34;&#34;&#34;
        self.policy.end_episode()

    def learn(self, verbose=True):
        &#34;&#34;&#34;Trains the agent on a batch of its experiences.
           (For this agent no learning is needed)

        Args:
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        with open(os.path.join(path, &#39;note.txt&#39;), &#39;r&#39;) as file:
            note = file.read()
        return note

    def save(self, path, note):
        &#34;&#34;&#34;Saves a note to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        time = datetime.now()
        path = os.path.join(path, time.strftime(r&#39;%Y%m%d_%H%M%S_%f&#39;))
        os.mkdir(path)
        with open(os.path.join(path, &#39;note.txt&#39;), &#39;w&#39;) as file:
            file.write(note)
        return path</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.QAgent" href="#paiutils.reinforcement.QAgent">QAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.Agent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.
(For this agent all memory is discarded)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.
       (For this agent all memory is discarded)

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Ends the episode for the agent.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Ends the episode for the agent.&#34;&#34;&#34;
    self.policy.end_episode()</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.forget"><code class="name flex">
<span>def <span class="ident">forget</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Forgets or clears all memory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forget(self):
    &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a batch of its experiences.
(For this agent no learning is needed)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information
should be printed to the screen</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, verbose=True):
    &#34;&#34;&#34;Trains the agent on a batch of its experiences.
       (For this agent no learning is needed)

    Args:
        verbose: A boolean, which determines if information
                 should be printed to the screen
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    with open(os.path.join(path, &#39;note.txt&#39;), &#39;r&#39;) as file:
        note = file.read()
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, note)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is the note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, note):
    &#34;&#34;&#34;Saves a note to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        note: A string, which is the note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    time = datetime.now()
    path = os.path.join(path, time.strftime(r&#39;%Y%m%d_%H%M%S_%f&#39;))
    os.mkdir(path)
    with open(os.path.join(path, &#39;note.txt&#39;), &#39;w&#39;) as file:
        file.write(note)
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state to get the action for</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value, which is the selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value or list of values, which is the
               state to get the action for
        training: A boolean, which determines if the
                  agent is training

    Returns:
        A value, which is the selected action
    &#34;&#34;&#34;
    def _select_action():
        return np.random.random(self.action_size)
    return self.policy.select_action(_select_action,
                                     training=training)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Agent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the episode data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self):
    &#34;&#34;&#34;Sets the episode data.&#34;&#34;&#34;
    self.playing_data = PlayingData(False, False, 0, False, {})</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.AsceticPolicy"><code class="flex name class">
<span>class <span class="ident">AsceticPolicy</span></span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for calling an Agent's action function and
selecting the most ascetic action.</p>
<p>Initalizes the Policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AsceticPolicy(Policy):
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function and
       selecting the most ascetic action.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initalizes the Policy.&#34;&#34;&#34;
        super().__init__()

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        action = np.argmin(action_func())
        return action</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.AsceticPolicy.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, action_func, training)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent should take.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_func</code></strong></dt>
<dd>A function that returns a list of values</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
Agent is in a training states</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, action_func, training):
    &#34;&#34;&#34;Returns the action the Agent should take.

    Args:
        action_func: A function that returns a list of values
        training: A boolean, which determines if the
                  Agent is in a training states
    &#34;&#34;&#34;
    action = np.argmin(action_func())
    return action</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Policy.end_episode" href="#paiutils.reinforcement.Policy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Policy.reset" href="#paiutils.reinforcement.Policy.reset">reset</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent"><code class="flex name class">
<span>class <span class="ident">DDPGAgent</span></span>
<span>(</span><span>policy, amodel, cmodel, discounted_rate, create_memory=&lt;function DDPGAgent.&lt;lambda&gt;&gt;, enable_target=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This class (Deep Deterministic Policy Gradient Agent) is an Agent
that uses two Neural Networks. An Actor network, which is like
a PGAgent Network and a Critic Network like the DQNAgent
Network. The critic rates the actions of the actor.</p>
<p>Initalizes the DDPG Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>policy</code></strong></dt>
<dd>A NoisePolicy instance</dd>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>cmodel</code></strong></dt>
<dd>A keras model, which takes the state and a action as input
and outputs Q Values (a judgement)</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
<dt><strong><code>enable_target</code></strong></dt>
<dd>A boolean, which determines if a target model
should be used for the critic</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DDPGAgent(MemoryAgent):
    &#34;&#34;&#34;This class (Deep Deterministic Policy Gradient Agent) is an Agent
       that uses two Neural Networks. An Actor network, which is like
       a PGAgent Network and a Critic Network like the DQNAgent
       Network. The critic rates the actions of the actor.
    &#34;&#34;&#34;

    def __init__(self, policy, amodel, cmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=False):
        &#34;&#34;&#34;Initalizes the DDPG Agent.

        Args:
            policy: A NoisePolicy instance
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state and a action as input
                     and outputs Q Values (a judgement)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used for the critic
        &#34;&#34;&#34;
        if not isinstance(policy, NoisePolicy):
            raise ValueError(&#39;The policy parameter must be a &#39;
                             &#39;instance of NoisePolicy.&#39;)
        MemoryAgent.__init__(self, amodel.output_shape[1], policy)
        self.amodel = amodel
        self.cmodel = cmodel
        if isinstance(self.cmodel.output_shape, list):
            self.cmodel.compiled_loss.build(
                tf.zeros(self.cmodel.output_shape[0][1:])
            )
        else: 
            self.cmodel.compiled_loss.build(
                tf.zeros(self.cmodel.output_shape[1:])
            )
        self.target_cmodel = None
        self.enable_target = enable_target
        self.discounted_rate = discounted_rate
        if self.enable_target:
            self.target_amodel = keras.models.clone_model(amodel)
            self.target_amodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
            self.target_cmodel = keras.models.clone_model(cmodel)
            self.target_cmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
        else:
            self.target_amodel = self.amodel
            self.target_cmodel = self.cmodel

        self.states = create_memory(self.amodel.input_shape,
                                    keras.backend.floatx())
        self.next_states = create_memory(self.amodel.input_shape,
                                         keras.backend.floatx())
        self.actions = create_memory(self.amodel.output_shape,
                                     keras.backend.floatx())
        self.rewards = create_memory((None,),
                                     keras.backend.floatx())
        self.terminals = create_memory((None,),
                                       keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;next_states&#39;: self.next_states,
            &#39;actions&#39;: self.actions, &#39;rewards&#39;: self.rewards,
            &#39;terminals&#39;: self.terminals
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        self.total_steps = 0
        self.metric_c = keras.metrics.Mean(name=&#39;critic_loss&#39;)
        self.metric_a = keras.metrics.Mean(name=&#39;actor_loss&#39;)

        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.output_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to predict the actions for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value or list of values, which is the
                selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        def _select_action():
            actions = self.amodel(np.expand_dims(state, axis=0),
                                  training=False)[0].numpy()
            return actions
        actions = self.policy.select_action(_select_action,
                                            training=training)
        return actions

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the
                    action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.next_states.add(np.array(new_state))
        self.actions.add(action)
        self.rewards.add(reward)
        self.terminals.add(0 if terminal else 1)

    def update_target(self, tau):
        &#34;&#34;&#34;Updates the target Actor and Critic Model weights.

        Args:
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
        &#34;&#34;&#34;
        if tau == 1.0:
            self.target_amodel.set_weights(self.amodel.get_weights())
            self.target_cmodel.set_weights(self.cmodel.get_weights())
        else:
            tws = self.target_amodel.trainable_variables
            ws = self.amodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)
            tws = self.target_cmodel.trainable_variables
            ws = self.cmodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)

    def _train_step(self, states, next_states, actions, terminals, rewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains the actions performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
        &#34;&#34;&#34;
        next_actions = self.target_amodel(next_states, training=False)
        next_qvalues = tf.squeeze(
            self.target_cmodel([next_states, next_actions], training=False)
        )
        qvalues_true = (rewards +
                        self.discounted_rate * next_qvalues * terminals)

        # Critic
        with tf.GradientTape() as tape:
            qvalues_pred = tf.squeeze(
                self.cmodel([states, actions], training=True)
            )
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred
            )
            loss = tf.reduce_mean(loss) + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        # Actor
        with tf.GradientTape() as tape:
            action_preds = self.amodel(states, training=True)
            loss = -tf.reduce_mean(tf.squeeze(
                self.cmodel([states, action_preds], training=False)
            ))
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric_a(loss)

    def _train(self, states, next_states, actions, terminals, rewards,
               epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains the actions performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            critic_loss_results = self.metric_c.result()
            actor_loss_results = self.metric_a.result()
            self.metric_c.reset_states()
            self.metric_a.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and predictions are
                    repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length
            )
            self._train(*arrays, epochs, batch_size, verbose=verbose)

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(file.read())
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
                self.cmodel = model_from_json(file.read())
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
            self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return note

    def save(self, path, save_model=True, save_data=True,
             note=&#39;DDPGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, weights of the models, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.cmodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
            self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.TD3Agent" href="reinforcement_agents.html#paiutils.reinforcement_agents.TD3Agent">TD3Agent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.DDPGAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value or list of values, which is the
action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value or list of values, which is the
                action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.next_states.add(np.array(new_state))
    self.actions.add(action)
    self.rewards.add(reward)
    self.terminals.add(0 if terminal else 1)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a sample of its experiences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini_batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and predictions are
repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target critic model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
Actor or Critic model to the target models
(1.0 is a hard copy and less is softer)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, batch_size=None, mini_batch=0,
          epochs=1, repeat=1,
          target_update_interval=1, tau=1.0, verbose=True):
    &#34;&#34;&#34;Trains the agent on a sample of its experiences.

    Args:
        batch_size: An integer, which is the size of each batch
                    within the mini_batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and predictions are
                repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target critic model weights are updated
        tau: A float, which is the strength of the copy from the
             Actor or Critic model to the target models
             (1.0 is a hard copy and less is softer)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    self.total_steps += 1
    if batch_size is None:
        batch_size = len(self.states)
    if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
        length = mini_batch
    else:
        length = len(self.states)

    for count in range(1, repeat+1):
        if verbose:
            print(f&#39;Repeat {count}/{repeat}&#39;)

        arrays, _ = self.states.create_shuffled_subset(
            [self.states, self.next_states, self.actions,
             self.terminals, self.rewards],
            length
        )
        self._train(*arrays, epochs, batch_size, verbose=verbose)

        if (self.enable_target
                and self.total_steps % target_update_interval == 0):
            self.update_target(tau)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_model=True, load_data=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be loaded</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_model=True, load_data=True):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_model: A boolean, which determines if the model
                    architectures and weights
                    should be loaded
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = MemoryAgent.load(self, path, load_data=load_data)
    if load_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
            self.amodel = model_from_json(file.read())
        with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
            self.cmodel = model_from_json(file.read())
        self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
        self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_model=True, save_data=True, note='DDPGAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note, weights of the models, and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be saved</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_model=True, save_data=True,
         note=&#39;DDPGAgent Save&#39;):
    &#34;&#34;&#34;Saves a note, weights of the models, and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_model: A boolean, which determines if the model
                    architectures and weights
                    should be saved
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = MemoryAgent.save(self, path, save_data=save_data, note=note)
    if save_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.amodel.to_json())
        with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.cmodel.to_json())
        self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
        self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state to predict the actions for</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value or list of values, which is the
selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value or list of values, which is the
               state to predict the actions for
        training: A boolean, which determines if the
                  agent is training

    Returns:
        A value or list of values, which is the
            selected action
    &#34;&#34;&#34;
    if (self.time_distributed_states is not None
            and state.shape == self.amodel.input_shape[2:]):
        self.time_distributed_states = np.roll(
            self.time_distributed_states, -1
        )
        self.time_distributed_states[-1] = state
        state = self.time_distributed_states

    def _select_action():
        actions = self.amodel(np.expand_dims(state, axis=0),
                              training=False)[0].numpy()
        return actions
    actions = self.policy.select_action(_select_action,
                                        training=training)
    return actions</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, memorizing=False, learns_in_episode=False, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>memorizing</code></strong></dt>
<dd>A boolean, which determines if the agent
should be adding the information obtained
through playing an episode to memory</dd>
<dt><strong><code>learns_in_episode</code></strong></dt>
<dd>A boolean, which determines if the agent
learns during a episode or at the end</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target critic model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
Actor or Critic model to the target models
(1.0 is a hard copy and less is softer)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, memorizing=False,
                     learns_in_episode=False, batch_size=None,
                     mini_batch=0, epochs=1, repeat=1,
                     target_update_interval=1, tau=1.0, verbose=True):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        memorizing: A boolean, which determines if the agent
                    should be adding the information obtained
                    through playing an episode to memory
        learns_in_episode: A boolean, which determines if the agent
                           learns during a episode or at the end
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target critic model weights are updated
        tau: A float, which is the strength of the copy from the
             Actor or Critic model to the target models
             (1.0 is a hard copy and less is softer)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    learning_params = {&#39;batch_size&#39;: batch_size,
                       &#39;mini_batch&#39;: mini_batch,
                       &#39;epochs&#39;: epochs,
                       &#39;repeat&#39;: repeat,
                       &#39;target_update_interval&#39;: target_update_interval,
                       &#39;tau&#39;: tau,
                       &#39;verbose&#39;: verbose}
    self.playing_data = PlayingData(training, memorizing, epochs,
                                    learns_in_episode, learning_params)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DDPGAgent.update_target"><code class="name flex">
<span>def <span class="ident">update_target</span></span>(<span>self, tau)</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the target Actor and Critic Model weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
Actor or Critic model to the target models
(1.0 is a hard copy and less is softer)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_target(self, tau):
    &#34;&#34;&#34;Updates the target Actor and Critic Model weights.

    Args:
        tau: A float, which is the strength of the copy from the
             Actor or Critic model to the target models
             (1.0 is a hard copy and less is softer)
    &#34;&#34;&#34;
    if tau == 1.0:
        self.target_amodel.set_weights(self.amodel.get_weights())
        self.target_cmodel.set_weights(self.cmodel.get_weights())
    else:
        tws = self.target_amodel.trainable_variables
        ws = self.amodel.trainable_variables
        for ndx in range(len(tws)):
            tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)
        tws = self.target_cmodel.trainable_variables
        ws = self.cmodel.trainable_variables
        for ndx in range(len(tws)):
            tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.MemoryAgent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.MemoryAgent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.DQNAgent"><code class="flex name class">
<span>class <span class="ident">DQNAgent</span></span>
<span>(</span><span>policy, qmodel, discounted_rate, create_memory=&lt;function DQNAgent.&lt;lambda&gt;&gt;, enable_target=True, enable_double=False, enable_per=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is an Agent that uses a Deep Q Network instead of
a table like the QAgent. This allows for generalizations and
large environment states.</p>
<p>Initalizes the Deep Q Network Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
<dt><strong><code>qmodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
Q Values</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
<dt><strong><code>enable_target</code></strong></dt>
<dd>A boolean, which determines if a target model
should be used</dd>
<dt><strong><code>enable_double</code></strong></dt>
<dd>A boolean, which determiens if the Double Deep Q
Network should be used</dd>
<dt><strong><code>enable_per</code></strong></dt>
<dd>A boolean, which determines if prioritized experience
replay should be used (The implementation for this is
not the normal tree implementation, and only weights
the probabilily of being choosen and not also the
gradient)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQNAgent(MemoryAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Deep Q Network instead of
       a table like the QAgent. This allows for generalizations and
       large environment states.
    &#34;&#34;&#34;

    @staticmethod
    def get_dueling_output_layer(action_size, dueling_type=&#39;avg&#39;):
        assert dueling_type in [&#39;avg&#39;, &#39;max&#39;, &#39;naive&#39;], (
            &#34;Dueling type must be &#39;avg&#39;, &#39;max&#39;, or &#39;naive&#39;&#34;
        )

        def layer(x1, x2):
            x1 = keras.layers.Dense(1)(x1)
            x2 = keras.layers.Dense(action_size)(x2)
            x = keras.layers.Concatenate()([x1, x2])
            if dueling_type == &#39;avg&#39;:
                def dueling(a):
                    return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                            K.mean(a[:, 1:], axis=1, keepdims=True))
            elif dueling_type == &#39;max&#39;:
                def dueling(a):
                    return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                            K.max(a[:, 1:], axis=1, keepdims=True))
            else:
                def dueling(a):
                    return K.expand_dims(a[:, 0], -1) + a[:, 1:]
            return keras.layers.Lambda(dueling, output_shape=(action_size,),
                                       name=&#39;q_output&#39;)(x)
        return layer

    def __init__(self, policy, qmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=True, enable_double=False,
                 enable_per=False):
        &#34;&#34;&#34;Initalizes the Deep Q Network Agent.

        Args:
            policy: A policy instance
            qmodel: A keras model, which takes the state as input and outputs
                    Q Values
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used
            enable_double: A boolean, which determiens if the Double Deep Q
                           Network should be used
            enable_per: A boolean, which determines if prioritized experience
                        replay should be used (The implementation for this is
                        not the normal tree implementation, and only weights
                        the probabilily of being choosen and not also the
                        gradient)
        &#34;&#34;&#34;
        MemoryAgent.__init__(self, qmodel.output_shape[1], policy)
        self.qmodel = qmodel
        self.qmodel.compiled_loss.build(
            tf.zeros(self.qmodel.output_shape[1:])
        )
        self.target_qmodel = None
        self.enable_target = enable_target or enable_double
        self.enable_double = enable_double
        if self.enable_target:
            self.target_qmodel = keras.models.clone_model(qmodel)
            self.target_qmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
        else:
            self.target_qmodel = self.qmodel
        self.discounted_rate = discounted_rate
        self.states = create_memory(self.qmodel.input_shape,
                                    keras.backend.floatx())
        self.next_states = create_memory(self.qmodel.input_shape,
                                         keras.backend.floatx())
        self.actions = create_memory(self.qmodel.output_shape,
                                     keras.backend.floatx())
        self.rewards = create_memory((None,),
                                     keras.backend.floatx())
        self.terminals = create_memory((None,),
                                       keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;next_states&#39;: self.next_states,
            &#39;actions&#39;: self.actions, &#39;rewards&#39;: self.rewards,
            &#39;terminals&#39;: self.terminals
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        if enable_per:
            self.per_losses = create_memory((None,),
                                            keras.backend.floatx())
            self.memory[&#39;per_losses&#39;] = self.per_losses
            # assuming the true max loss will be less than 100
            # at least at the begining
            self.max_loss = 100.0
        else:
            self.per_losses = None
        self.action_identity = np.identity(self.action_size)
        self.total_steps = 0
        self.metric = keras.metrics.Mean(name=&#39;loss&#39;)

        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the Q values for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.qmodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        def _select_action():
            qvalues = self.qmodel(np.expand_dims(state, axis=0),
                                  training=False)[0].numpy()
            return qvalues
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target model weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.next_states.add(np.array(new_state))
        self.actions.add(self.action_identity[action])
        self.rewards.add(reward)
        self.terminals.add(0 if terminal else 1)
        if self.per_losses is not None:
            self.per_losses.add(self.max_loss)

    def update_target(self, tau):
        &#34;&#34;&#34;Updates the target Q Model weights.

        Args:
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
        &#34;&#34;&#34;
        if tau == 1.0:
            self.target_qmodel.set_weights(self.qmodel.get_weights())
        else:
            tws = self.target_qmodel.trainable_variables
            ws = self.qmodel.trainable_variables
            for ndx in range(len(tws)):
                tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)

    def _train_step(self, states, next_states,
                    actions, terminals, rewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment

        Returns:
            A loss for this batch of data
        &#34;&#34;&#34;
        if self.enable_double:
            qvalues = self.qmodel(next_states, training=False)
            actions = tf.argmax(qvalues, axis=-1)
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.squeeze(tf.gather(qvalues, actions[:, tf.newaxis],
                                           axis=-1, batch_dims=1))
            actions = tf.one_hot(actions, self.action_size,
                                 dtype=qvalues.dtype)
        else:
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.reduce_max(qvalues, axis=-1)
        qvalues = (rewards +
                   self.discounted_rate * qvalues * terminals)
        with tf.GradientTape() as tape:
            y_pred = self.qmodel(states, training=True)
            if len(self.qmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.qmodel.losses)
            else:
                reg_loss = 0
            y_true = (y_pred * (1 - actions) +
                      qvalues[:, tf.newaxis] * actions)
            loss = self.qmodel.compiled_loss._losses[0].fn(
                y_true, y_pred
            ) + reg_loss
        grads = tape.gradient(loss, self.qmodel.trainable_variables)
        self.qmodel.optimizer.apply_gradients(
            zip(grads, self.qmodel.trainable_variables)
        )
        self.metric(loss)

        return tf.reduce_sum(tf.abs(y_true - y_pred), axis=-1)

    def _train(self, states, next_states, actions, terminals,
               rewards, epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A list of floats, which are the absolute losses for all
                the data
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        losses = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)

            count = 0
            for batch in batches:
                if epoch == epochs:
                    losses.append(self._tf_train_step(*batch).numpy())
                else:
                    self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            loss_results = self.metric.result()
            self.metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;loss: {loss_results}&#39;)
        return np.hstack(losses)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target model weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.per_losses is None:
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards],
                    length
                )
            else:
                per_losses = self.per_losses.array()
                self.max_loss = per_losses.max()
                per_losses = per_losses / per_losses.sum()
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards],
                    length, weights=per_losses
                )
            losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
            if self.per_losses is not None:
                for ndx, loss in zip(indexes, losses):
                    self.per_losses[ndx] = loss

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architecture and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;r&#39;) as file:
                self.qmodel = model_from_json(file.read())
            self.qmodel.load_weights(os.path.join(path, &#39;qweights.h5&#39;))
            if self.enable_target:
                self.target_qmodel = keras.models.clone_model(self.qmodel)
                self.target_qmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
            else:
                self.target_qmodel = self.qmodel
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;DQNAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architecture and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.qmodel.to_json())
            self.qmodel.save_weights(os.path.join(path, &#39;qweights.h5&#39;))
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.DQNPGAgent" href="reinforcement_agents.html#paiutils.reinforcement_agents.DQNPGAgent">DQNPGAgent</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="paiutils.reinforcement.DQNAgent.get_dueling_output_layer"><code class="name flex">
<span>def <span class="ident">get_dueling_output_layer</span></span>(<span>action_size, dueling_type='avg')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_dueling_output_layer(action_size, dueling_type=&#39;avg&#39;):
    assert dueling_type in [&#39;avg&#39;, &#39;max&#39;, &#39;naive&#39;], (
        &#34;Dueling type must be &#39;avg&#39;, &#39;max&#39;, or &#39;naive&#39;&#34;
    )

    def layer(x1, x2):
        x1 = keras.layers.Dense(1)(x1)
        x2 = keras.layers.Dense(action_size)(x2)
        x = keras.layers.Concatenate()([x1, x2])
        if dueling_type == &#39;avg&#39;:
            def dueling(a):
                return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                        K.mean(a[:, 1:], axis=1, keepdims=True))
        elif dueling_type == &#39;max&#39;:
            def dueling(a):
                return (K.expand_dims(a[:, 0], -1) + a[:, 1:] -
                        K.max(a[:, 1:], axis=1, keepdims=True))
        else:
            def dueling(a):
                return K.expand_dims(a[:, 0], -1) + a[:, 1:]
        return keras.layers.Lambda(dueling, output_shape=(action_size,),
                                   name=&#39;q_output&#39;)(x)
    return layer</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.DQNAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.next_states.add(np.array(new_state))
    self.actions.add(self.action_identity[action])
    self.rewards.add(reward)
    self.terminals.add(0 if terminal else 1)
    if self.per_losses is not None:
        self.per_losses.add(self.max_loss)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a sample of its experiences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
qmodel to the target qmodel (1.0 is a hard copy and
less is softer)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, batch_size=None, mini_batch=0,
          epochs=1, repeat=1,
          target_update_interval=1, tau=1.0, verbose=True):
    &#34;&#34;&#34;Trains the agent on a sample of its experiences.

    Args:
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target model weights are updated
        tau: A float, which is the strength of the copy from the
             qmodel to the target qmodel (1.0 is a hard copy and
             less is softer)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    self.total_steps += 1
    if batch_size is None:
        batch_size = len(self.states)
    if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
        length = mini_batch
    else:
        length = len(self.states)

    for count in range(1, repeat+1):
        if verbose:
            print(f&#39;Repeat {count}/{repeat}&#39;)

        if self.per_losses is None:
            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length
            )
        else:
            per_losses = self.per_losses.array()
            self.max_loss = per_losses.max()
            per_losses = per_losses / per_losses.sum()
            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length, weights=per_losses
            )
        losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
        if self.per_losses is not None:
            for ndx, loss in zip(indexes, losses):
                self.per_losses[ndx] = loss

        if (self.enable_target
                and self.total_steps % target_update_interval == 0):
            self.update_target(tau)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_model=True, load_data=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_model</code></strong></dt>
<dd>A boolean, which determines if the model
architecture and weights
should be loaded</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_model=True, load_data=True):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_model: A boolean, which determines if the model
                    architecture and weights
                    should be loaded
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = MemoryAgent.load(self, path, load_data=load_data)
    if load_model:
        with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;r&#39;) as file:
            self.qmodel = model_from_json(file.read())
        self.qmodel.load_weights(os.path.join(path, &#39;qweights.h5&#39;))
        if self.enable_target:
            self.target_qmodel = keras.models.clone_model(self.qmodel)
            self.target_qmodel.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)
        else:
            self.target_qmodel = self.qmodel
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_model=True, save_data=True, note='DQNAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note, model weights, and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_model</code></strong></dt>
<dd>A boolean, which determines if the model
architecture and weights
should be saved</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_model=True,
         save_data=True, note=&#39;DQNAgent Save&#39;):
    &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_model: A boolean, which determines if the model
                    architecture and weights
                    should be saved
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = MemoryAgent.save(self, path, save_data=save_data, note=note)
    if save_model:
        with open(os.path.join(path, &#39;qmodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.qmodel.to_json())
        self.qmodel.save_weights(os.path.join(path, &#39;qweights.h5&#39;))
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value, which is the state to predict
the Q values for</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value, which is the selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value, which is the state to predict
               the Q values for
        training: A boolean, which determines if the
                  agent is training

    Returns:
        A value, which is the selected action
    &#34;&#34;&#34;
    if (self.time_distributed_states is not None
            and state.shape == self.qmodel.input_shape[2:]):
        self.time_distributed_states = np.roll(
            self.time_distributed_states, -1
        )
        self.time_distributed_states[-1] = state
        state = self.time_distributed_states

    def _select_action():
        qvalues = self.qmodel(np.expand_dims(state, axis=0),
                              training=False)[0].numpy()
        return qvalues
    return self.policy.select_action(_select_action,
                                     training=training)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, memorizing=False, learns_in_episode=False, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>memorizing</code></strong></dt>
<dd>A boolean, which determines if the agent
should be adding the information obtained
through playing an episode to memory</dd>
<dt><strong><code>learns_in_episode</code></strong></dt>
<dd>A boolean, which determines if the agent
learns during a episode or at the end</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
qmodel to the target qmodel (1.0 is a hard copy and
less is softer)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, memorizing=False,
                     learns_in_episode=False, batch_size=None,
                     mini_batch=0, epochs=1, repeat=1,
                     target_update_interval=1, tau=1.0, verbose=True):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        memorizing: A boolean, which determines if the agent
                    should be adding the information obtained
                    through playing an episode to memory
        learns_in_episode: A boolean, which determines if the agent
                           learns during a episode or at the end
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target model weights are updated
        tau: A float, which is the strength of the copy from the
             qmodel to the target qmodel (1.0 is a hard copy and
             less is softer)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    learning_params = {&#39;batch_size&#39;: batch_size,
                       &#39;mini_batch&#39;: mini_batch,
                       &#39;epochs&#39;: epochs,
                       &#39;repeat&#39;: repeat,
                       &#39;target_update_interval&#39;: target_update_interval,
                       &#39;tau&#39;: tau,
                       &#39;verbose&#39;: verbose}
    self.playing_data = PlayingData(training, memorizing, epochs,
                                    learns_in_episode, learning_params)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.DQNAgent.update_target"><code class="name flex">
<span>def <span class="ident">update_target</span></span>(<span>self, tau)</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the target Q Model weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
qmodel to the target qmodel (1.0 is a hard copy and
less is softer)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_target(self, tau):
    &#34;&#34;&#34;Updates the target Q Model weights.

    Args:
        tau: A float, which is the strength of the copy from the
             qmodel to the target qmodel (1.0 is a hard copy and
             less is softer)
    &#34;&#34;&#34;
    if tau == 1.0:
        self.target_qmodel.set_weights(self.qmodel.get_weights())
    else:
        tws = self.target_qmodel.trainable_variables
        ws = self.qmodel.trainable_variables
        for ndx in range(len(tws)):
            tws[ndx] = ws[ndx] * tau + tws[ndx] * (1 - tau)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.MemoryAgent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.MemoryAgent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.Decay"><code class="flex name class">
<span>class <span class="ident">Decay</span></span>
<span>(</span><span>initial_value, constant, min_value=0, step_every_call=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This class decays a initial value to a minimum
value through a given number of steps.
(formula: max(initial_value - constant * steps, 0))</p>
<p>Initalizes the state of the decay object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>initial_value</code></strong></dt>
<dd>A float, which is the starting value to decay</dd>
<dt><strong><code>constant</code></strong></dt>
<dd>A float, which is the slope/rate that the decay occurs</dd>
<dt><strong><code>min_value</code></strong></dt>
<dd>A float, which is the minimum value the decay can reach</dd>
<dt><strong><code>step_every_call</code></strong></dt>
<dd>A boolean, which determines if each call should
step the decay</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Decay:
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value through a given number of steps.
       (formula: max(initial_value - constant * steps, 0))
    &#34;&#34;&#34;

    def __init__(self, initial_value, constant,
                 min_value=0, step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            constant: A float, which is the slope/rate that the decay occurs
            min_value: A float, which is the minimum value the decay can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        self.initial_value = initial_value
        self.constant = constant
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.steps = 0

    def reset(self):
        &#34;&#34;&#34;Resets the steps.&#34;&#34;&#34;
        self.steps = 0

    def step(self):
        &#34;&#34;&#34;Steps the decay forward.&#34;&#34;&#34;
        self.steps += 1

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.initial_value - self.constant * self.steps
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.ExponentialDecay" href="#paiutils.reinforcement.ExponentialDecay">ExponentialDecay</a></li>
<li><a title="paiutils.reinforcement.LinearDecay" href="#paiutils.reinforcement.LinearDecay">LinearDecay</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.Decay.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the steps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets the steps.&#34;&#34;&#34;
    self.steps = 0</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Decay.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Steps the decay forward.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self):
    &#34;&#34;&#34;Steps the decay forward.&#34;&#34;&#34;
    self.steps += 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.ETDMemory"><code class="flex name class">
<span>class <span class="ident">ETDMemory</span></span>
<span>(</span><span>num_time_steps, void_state, max_len=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is for the efficient storage of time distributed states.
This type of memory should only be used for states.</p>
<p>Initalizes the memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_time_steps</code></strong></dt>
<dd>An integer, which is the number of
states that make up a complete state</dd>
<dt><strong><code>void_state</code></strong></dt>
<dd>A ndarray, which is used when there is not
enough states to create a complete state</dd>
<dt><strong><code>max_len</code></strong></dt>
<dd>An integer, which is the max length of memory
(if reached, the oldest memory will be removed)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ETDMemory(Memory):
    &#34;&#34;&#34;This class is for the efficient storage of time distributed states.
       This type of memory should only be used for states.
    &#34;&#34;&#34;

    def __init__(self, num_time_steps, void_state, max_len=None):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            num_time_steps: An integer, which is the number of
                            states that make up a complete state
            void_state: A ndarray, which is used when there is not
                        enough states to create a complete state
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        if max_len is not None:
            raise NotImplementedError(&#39;max_len is not yet implemented&#39;)
        self.num_time_steps = num_time_steps
        self.max_len = max_len
        self.buffer = [void_state]
        self.ndxs = []
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def __len__(self):
        &#34;&#34;&#34;Returns the number of entries in the memory.

        Returns:
            An integer
        &#34;&#34;&#34;
        return len(self.ndxs)

    def add(self, x):
        &#34;&#34;&#34;Adds a entry to memory.

        Args:
            x: A entry similar to other entries
        &#34;&#34;&#34;
        self.step_ndxs = np.roll(self.step_ndxs, -1)
        self.step_ndxs[-1] = len(self.buffer)
        self.ndxs.append(self.step_ndxs)
        self.buffer.append(x)

    def __getitem__(self, key):
        &#34;&#34;&#34;Returns an item given a key.

        Args:
            key: A valid key or index for a memory entry
        &#34;&#34;&#34;
        return self.buffer[key + 1 if key &gt;= 0 else key]

    def __setitem__(self, key, value):
        &#34;&#34;&#34;Sets a entry to a given key.

        Args:
            key: A valid key or index for a memory entry
            value: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer[key + 1 if key &gt;= 0 else key] = value

    def array(self):
        &#34;&#34;&#34;Returns a copy of the memory.

        Returns:
            A numpy ndarray
        &#34;&#34;&#34;
        return np.array(self.buffer)[np.array(self.ndxs)]

    def reset(self):
        &#34;&#34;&#34;Resets or clears the memory.
        &#34;&#34;&#34;
        void_state = self.buffer[0]
        self.buffer.clear()
        self.buffer.append(void_state)
        self.ndxs.clear()
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def end_episode(self):
        &#34;&#34;&#34;Tells memory an episode ended.
        &#34;&#34;&#34;
        self.step_ndxs = np.zeros(self.num_time_steps, dtype=np.int)

    def save(self, file, name):
        &#34;&#34;&#34;Creates a h5py dataset with the memory data.

        Args:
            file: A h5py open file for writing
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        file.create_dataset(f&#39;{name}_buffer&#39;, data=np.array(self.buffer))
        file.create_dataset(f&#39;{name}_ndxs&#39;, data=np.array(self.ndxs))

    def load(self, file, name):
        &#34;&#34;&#34;Loads a h5py dataset with the saved memory data.

        Args:
            file: A h5py open file for reading
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        for element in file[f&#39;{name}_ndxs&#39;]:
            self.ndxs.append(element)
            if element.shape != (self.num_time_steps,):
                raise ValueError(&#39;Cannot load dataset: &#39;
                                 &#39;invalid number of time steps&#39;)

        for element in file[f&#39;{name}_buffer&#39;]:
            self.buffer.append(element)

    @staticmethod
    def create_shuffled_subset(memories, subset_size, weights=None):
        &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

        Args:
            memories: A list of Memeory Objects (not asserted but assumed)
            subset_size: A integer, which is the size of the
                         outer dimension of each ndarray
            weights: A list of probabilities that add up to 1

        Returns:
            arrays and shuffled indexes
        &#34;&#34;&#34;
        length = len(memories[0])
        if subset_size &gt; length:
            raise ValueError(f&#39;Subset size {subset_size} is &#39;
                             f&#39;greater than memory length {length}&#39;)
        for memory in memories:
            if len(memory) != length:
                raise ValueError(&#39;Memories are not all the same length.&#39;)
            if not isinstance(memory, Memory):
                raise TypeError(&#39;Memories must also be Memory &#39;
                                &#39;or subclass instances&#39;)
        indexes = np.random.choice(np.arange(len(memories[0])),
                                   size=subset_size, replace=False,
                                   p=weights)
        arrays = []
        for memory in memories:
            if isinstance(memory, ETDMemory):
                arrays.append(np.empty((subset_size,
                                        memory.num_time_steps,
                                        *memory.buffer[0].shape)))
            elif isinstance(memory[0], np.ndarray):
                arrays.append(np.empty((subset_size, *memory[0].shape)))
            else:
                arrays.append(np.empty(subset_size))
        for ndx, rndx in enumerate(indexes):
            for array, memory in zip(arrays, memories):
                if isinstance(memory, ETDMemory):
                    for andx, sndx in enumerate(memory.ndxs[rndx]):
                        array[ndx, andx] = memory.buffer[sndx]
                else:
                    array[ndx] = memory[rndx]
        return arrays, indexes</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Memory" href="#paiutils.reinforcement.Memory">Memory</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="paiutils.reinforcement.ETDMemory.create_shuffled_subset"><code class="name flex">
<span>def <span class="ident">create_shuffled_subset</span></span>(<span>memories, subset_size, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a list of numpy arrays of a shuffled subset of memories.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>memories</code></strong></dt>
<dd>A list of Memeory Objects (not asserted but assumed)</dd>
<dt><strong><code>subset_size</code></strong></dt>
<dd>A integer, which is the size of the
outer dimension of each ndarray</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>A list of probabilities that add up to 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>arrays and shuffled indexes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create_shuffled_subset(memories, subset_size, weights=None):
    &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

    Args:
        memories: A list of Memeory Objects (not asserted but assumed)
        subset_size: A integer, which is the size of the
                     outer dimension of each ndarray
        weights: A list of probabilities that add up to 1

    Returns:
        arrays and shuffled indexes
    &#34;&#34;&#34;
    length = len(memories[0])
    if subset_size &gt; length:
        raise ValueError(f&#39;Subset size {subset_size} is &#39;
                         f&#39;greater than memory length {length}&#39;)
    for memory in memories:
        if len(memory) != length:
            raise ValueError(&#39;Memories are not all the same length.&#39;)
        if not isinstance(memory, Memory):
            raise TypeError(&#39;Memories must also be Memory &#39;
                            &#39;or subclass instances&#39;)
    indexes = np.random.choice(np.arange(len(memories[0])),
                               size=subset_size, replace=False,
                               p=weights)
    arrays = []
    for memory in memories:
        if isinstance(memory, ETDMemory):
            arrays.append(np.empty((subset_size,
                                    memory.num_time_steps,
                                    *memory.buffer[0].shape)))
        elif isinstance(memory[0], np.ndarray):
            arrays.append(np.empty((subset_size, *memory[0].shape)))
        else:
            arrays.append(np.empty(subset_size))
    for ndx, rndx in enumerate(indexes):
        for array, memory in zip(arrays, memories):
            if isinstance(memory, ETDMemory):
                for andx, sndx in enumerate(memory.ndxs[rndx]):
                    array[ndx, andx] = memory.buffer[sndx]
            else:
                array[ndx] = memory[rndx]
    return arrays, indexes</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Memory" href="#paiutils.reinforcement.Memory">Memory</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Memory.add" href="#paiutils.reinforcement.Memory.add">add</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.array" href="#paiutils.reinforcement.Memory.array">array</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.end_episode" href="#paiutils.reinforcement.Memory.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.load" href="#paiutils.reinforcement.Memory.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.reset" href="#paiutils.reinforcement.Memory.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.save" href="#paiutils.reinforcement.Memory.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.Environment"><code class="flex name class">
<span>class <span class="ident">Environment</span></span>
<span>(</span><span>state_shape, action_size)</span>
</code></dt>
<dd>
<div class="desc"><p>This class handles the environment in which the Agent
performs actions in and can get rewards from.</p>
<p>Initalizes state and action shapes and sets the state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state_shape</code></strong></dt>
<dd>A tuple of integers, which is the
expected state shape for the agent,
or an integer of the discrete state
space</dd>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integer which is the discrete size
of the action space</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Environment:
    &#34;&#34;&#34;This class handles the environment in which the Agent
       performs actions in and can get rewards from.
    &#34;&#34;&#34;

    def __init__(self, state_shape, action_size):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            state_shape: A tuple of integers, which is the
                         expected state shape for the agent,
                         or an integer of the discrete state
                         space
            action_size: An integer which is the discrete size
                         of the action space
        &#34;&#34;&#34;
        if isinstance(state_shape, int):
            self.discrete_state_space = state_shape
            state_shape = 1
        else:
            self.discrete_state_space = None
        self.state_shape = state_shape
        self.action_size = action_size

    def reset(self):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = None
        return self.state

    def step(self, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the action.

        Args:
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state = None
        return self.state, 0, False

    def play_episode(self, agent, max_steps,
                     random=False, random_bounds=None,
                     render=False, verbose=True):
        &#34;&#34;&#34;Plays a single complete episode with the agent.

        Args:
            agent: An instance of Agent, which will be used to
                   interact in the environment
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen

        Returns:
            A tuple of an integer (last step) and a float (total reward)
        &#34;&#34;&#34;
        if not isinstance(agent, Agent):
            raise TypeError(&#39;The instance agent is not a child of Agent.&#39;)
        if not isinstance(agent.playing_data, PlayingData):
            raise ValueError(&#39;Invalid playing_data value. &#39;
                             &#39;(Forgot to set playing_data?)&#39;)
        total_reward = 0
        state = self.reset()
        if render:
            self.render()
        for step in range(1, max_steps + 1):
            if random:
                if random_bounds is None:
                    action = np.random.randint(0, self.action_size)
                else:
                    action = np.random.uniform(*random_bounds,
                                               size=self.action_size)
            else:
                action = agent.select_action(
                    state, training=agent.playing_data.training
                )
            new_state, reward, terminal = self.step(action)

            total_reward += reward

            if agent.playing_data.memorizing:
                agent.add_memory(state, action, new_state, reward, terminal)

            state = new_state

            if verbose:
                print(f&#39;Step: {step} - Reward: {reward} &#39;
                      f&#39;- Action: {action}&#39;)
            if render:
                self.render()
            if (agent.playing_data.training
                    and agent.playing_data.learns_in_episode
                    and agent.playing_data.epochs &gt; 0):
                agent.learn(**agent.playing_data.learning_params)
            if terminal:
                break
        agent.end_episode()
        if (agent.playing_data.training
                and not agent.playing_data.learns_in_episode
                and agent.playing_data.epochs &gt; 0):
            agent.learn(**agent.playing_data.learning_params)
        return step, total_reward

    def play_episodes(self, agent, num_episodes, max_steps,
                      random=False, random_bounds=None,
                      render=False, verbose=True,
                      episode_verbose=None,
                      end_episode_callback=None):
        &#34;&#34;&#34;Plays atleast 1 complete episode with the agent.

        Args:
            agent: An instance of Agent, which will be used to
                   interact in the environment
            num_episodes: An integer, which is the number of episodes to play
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen
            episode_verbose: A boolean, which determines if single episode
                             information should be printed to the screen
            end_episode_callback: A function called at the end of each episode
                                  with episode count, steps, and total reward
                                  from the most recent episode. If True
                                  is returned, play_episodes will stop
                                  early.

        Returns:
            A float, which is the average total reward of all episodes
        &#34;&#34;&#34;
        if episode_verbose is None:
            episode_verbose = verbose
        total_rewards = 0
        best_reward = &#39;Unknown&#39;
        for episode in range(1, num_episodes + 1):
            step, total_reward = self.play_episode(
                agent, max_steps, random=random, random_bounds=random_bounds,
                render=render, verbose=episode_verbose,
            )
            total_rewards += total_reward
            if best_reward == &#39;Unknown&#39; or total_reward &gt; best_reward:
                best_reward = total_reward
            if verbose:
                str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
                if isinstance(agent, MemoryAgent):
                    mem_len = len(next(iter(agent.memory.values())))
                    mem_str = f&#39; - Memory Size: {mem_len}&#39;
                else:
                    mem_str = &#39;&#39;
                print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                      f&#39;Steps: {step} - &#39;
                      f&#39;Total Reward: {total_reward} - &#39;
                      f&#39;Best Total Reward: {best_reward} - &#39;
                      f&#39;Average Total Reward: {total_rewards / episode}&#39;
                      f&#39;{mem_str}&#39;)
            if end_episode_callback is not None:
                end = end_episode_callback(
                    episode, step, total_reward
                )
                if end:
                    break
        return total_rewards / episode

    def close(self):
        &#34;&#34;&#34;Closes any threads or loose ends of the environment.
        &#34;&#34;&#34;
        pass

    def render(self):
        &#34;&#34;&#34;Renders the environment.
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.GymWrapper" href="#paiutils.reinforcement.GymWrapper">GymWrapper</a></li>
<li><a title="paiutils.reinforcement.MultiSeqAgentEnvironment" href="#paiutils.reinforcement.MultiSeqAgentEnvironment">MultiSeqAgentEnvironment</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.Environment.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Closes any threads or loose ends of the environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    &#34;&#34;&#34;Closes any threads or loose ends of the environment.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Environment.play_episode"><code class="name flex">
<span>def <span class="ident">play_episode</span></span>(<span>self, agent, max_steps, random=False, random_bounds=None, render=False, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plays a single complete episode with the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agent</code></strong></dt>
<dd>An instance of Agent, which will be used to
interact in the environment</dd>
<dt><strong><code>max_steps</code></strong></dt>
<dd>An integer, which is the max steps an episode
can take before terminating the episode</dd>
<dt><strong><code>random</code></strong></dt>
<dd>A booelan, which determines if the agent should not
be used, but instead pick random actions</dd>
<dt><strong><code>random_bounds</code></strong></dt>
<dd>A tuple of two bounds (lower and upper), which
are used for random actions that are not onehots</dd>
<dt><strong><code>render</code></strong></dt>
<dd>A boolean, which determines if the environment should
be rendered each step</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information should be
printed to the screen</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple of an integer (last step) and a float (total reward)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play_episode(self, agent, max_steps,
                 random=False, random_bounds=None,
                 render=False, verbose=True):
    &#34;&#34;&#34;Plays a single complete episode with the agent.

    Args:
        agent: An instance of Agent, which will be used to
               interact in the environment
        max_steps: An integer, which is the max steps an episode
                   can take before terminating the episode
        random: A booelan, which determines if the agent should not
                be used, but instead pick random actions
        random_bounds: A tuple of two bounds (lower and upper), which
                       are used for random actions that are not onehots
        render: A boolean, which determines if the environment should
                be rendered each step
        verbose: A boolean, which determines if information should be
                 printed to the screen

    Returns:
        A tuple of an integer (last step) and a float (total reward)
    &#34;&#34;&#34;
    if not isinstance(agent, Agent):
        raise TypeError(&#39;The instance agent is not a child of Agent.&#39;)
    if not isinstance(agent.playing_data, PlayingData):
        raise ValueError(&#39;Invalid playing_data value. &#39;
                         &#39;(Forgot to set playing_data?)&#39;)
    total_reward = 0
    state = self.reset()
    if render:
        self.render()
    for step in range(1, max_steps + 1):
        if random:
            if random_bounds is None:
                action = np.random.randint(0, self.action_size)
            else:
                action = np.random.uniform(*random_bounds,
                                           size=self.action_size)
        else:
            action = agent.select_action(
                state, training=agent.playing_data.training
            )
        new_state, reward, terminal = self.step(action)

        total_reward += reward

        if agent.playing_data.memorizing:
            agent.add_memory(state, action, new_state, reward, terminal)

        state = new_state

        if verbose:
            print(f&#39;Step: {step} - Reward: {reward} &#39;
                  f&#39;- Action: {action}&#39;)
        if render:
            self.render()
        if (agent.playing_data.training
                and agent.playing_data.learns_in_episode
                and agent.playing_data.epochs &gt; 0):
            agent.learn(**agent.playing_data.learning_params)
        if terminal:
            break
    agent.end_episode()
    if (agent.playing_data.training
            and not agent.playing_data.learns_in_episode
            and agent.playing_data.epochs &gt; 0):
        agent.learn(**agent.playing_data.learning_params)
    return step, total_reward</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Environment.play_episodes"><code class="name flex">
<span>def <span class="ident">play_episodes</span></span>(<span>self, agent, num_episodes, max_steps, random=False, random_bounds=None, render=False, verbose=True, episode_verbose=None, end_episode_callback=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plays atleast 1 complete episode with the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agent</code></strong></dt>
<dd>An instance of Agent, which will be used to
interact in the environment</dd>
<dt><strong><code>num_episodes</code></strong></dt>
<dd>An integer, which is the number of episodes to play</dd>
<dt><strong><code>max_steps</code></strong></dt>
<dd>An integer, which is the max steps an episode
can take before terminating the episode</dd>
<dt><strong><code>random</code></strong></dt>
<dd>A booelan, which determines if the agent should not
be used, but instead pick random actions</dd>
<dt><strong><code>random_bounds</code></strong></dt>
<dd>A tuple of two bounds (lower and upper), which
are used for random actions that are not onehots</dd>
<dt><strong><code>render</code></strong></dt>
<dd>A boolean, which determines if the environment should
be rendered each step</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information should be
printed to the screen</dd>
<dt><strong><code>episode_verbose</code></strong></dt>
<dd>A boolean, which determines if single episode
information should be printed to the screen</dd>
<dt><strong><code>end_episode_callback</code></strong></dt>
<dd>A function called at the end of each episode
with episode count, steps, and total reward
from the most recent episode. If True
is returned, play_episodes will stop
early.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A float, which is the average total reward of all episodes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play_episodes(self, agent, num_episodes, max_steps,
                  random=False, random_bounds=None,
                  render=False, verbose=True,
                  episode_verbose=None,
                  end_episode_callback=None):
    &#34;&#34;&#34;Plays atleast 1 complete episode with the agent.

    Args:
        agent: An instance of Agent, which will be used to
               interact in the environment
        num_episodes: An integer, which is the number of episodes to play
        max_steps: An integer, which is the max steps an episode
                   can take before terminating the episode
        random: A booelan, which determines if the agent should not
                be used, but instead pick random actions
        random_bounds: A tuple of two bounds (lower and upper), which
                       are used for random actions that are not onehots
        render: A boolean, which determines if the environment should
                be rendered each step
        verbose: A boolean, which determines if information should be
                 printed to the screen
        episode_verbose: A boolean, which determines if single episode
                         information should be printed to the screen
        end_episode_callback: A function called at the end of each episode
                              with episode count, steps, and total reward
                              from the most recent episode. If True
                              is returned, play_episodes will stop
                              early.

    Returns:
        A float, which is the average total reward of all episodes
    &#34;&#34;&#34;
    if episode_verbose is None:
        episode_verbose = verbose
    total_rewards = 0
    best_reward = &#39;Unknown&#39;
    for episode in range(1, num_episodes + 1):
        step, total_reward = self.play_episode(
            agent, max_steps, random=random, random_bounds=random_bounds,
            render=render, verbose=episode_verbose,
        )
        total_rewards += total_reward
        if best_reward == &#39;Unknown&#39; or total_reward &gt; best_reward:
            best_reward = total_reward
        if verbose:
            str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
            if isinstance(agent, MemoryAgent):
                mem_len = len(next(iter(agent.memory.values())))
                mem_str = f&#39; - Memory Size: {mem_len}&#39;
            else:
                mem_str = &#39;&#39;
            print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                  f&#39;Steps: {step} - &#39;
                  f&#39;Total Reward: {total_reward} - &#39;
                  f&#39;Best Total Reward: {best_reward} - &#39;
                  f&#39;Average Total Reward: {total_rewards / episode}&#39;
                  f&#39;{mem_str}&#39;)
        if end_episode_callback is not None:
            end = end_episode_callback(
                episode, step, total_reward
            )
            if end:
                break
    return total_rewards / episode</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Environment.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Renders the environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self):
    &#34;&#34;&#34;Renders the environment.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Environment.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to its initialized state.</p>
<h2 id="returns">Returns</h2>
<p>A numpy ndarray, which is the state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets the environment to its initialized state.

    Returns:
        A numpy ndarray, which is the state
    &#34;&#34;&#34;
    self.state = None
    return self.state</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Environment.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Moves the current state one step forward
with regard to the action.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong></dt>
<dd>An integer or value that determines an action</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple of a ndarray (state), a float/integer (reward),
and a boolean (terminal state)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, action):
    &#34;&#34;&#34;Moves the current state one step forward
       with regard to the action.

    Args:
        action: An integer or value that determines an action

    Returns:
        A tuple of a ndarray (state), a float/integer (reward),
            and a boolean (terminal state)
    &#34;&#34;&#34;
    self.state = None
    return self.state, 0, False</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.ExponentialDecay"><code class="flex name class">
<span>class <span class="ident">ExponentialDecay</span></span>
<span>(</span><span>initial_value, rate, min_value=0, step_every_call=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This class decays a initial value to a minimum
value exponentially through a given number of steps.
(formula: inital_value * (1 - rate)^steps + min_value)</p>
<p>Initalizes the state of the decay object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>initial_value</code></strong></dt>
<dd>A float, which is the starting value to decay</dd>
<dt><strong><code>rate</code></strong></dt>
<dd>A float, which is the slope/rate that the decay occurs</dd>
<dt><strong><code>min_value</code></strong></dt>
<dd>A float, which is the minimum value the decay can reach</dd>
<dt><strong><code>step_every_call</code></strong></dt>
<dd>A boolean, which determines if each call should
step the decay</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExponentialDecay(Decay):
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value exponentially through a given number of steps.
       (formula: inital_value * (1 - rate)^steps + min_value)
    &#34;&#34;&#34;

    def __init__(self, initial_value, rate, min_value=0,
                 step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            rate: A float, which is the slope/rate that the decay occurs
            min_value: A float, which is the minimum value the decay can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        self.initial_value = initial_value
        self.rate = rate
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.steps = 0

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.initial_value * (1 - self.rate)**self.steps
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Decay" href="#paiutils.reinforcement.Decay">Decay</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Decay" href="#paiutils.reinforcement.Decay">Decay</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Decay.reset" href="#paiutils.reinforcement.Decay.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Decay.step" href="#paiutils.reinforcement.Decay.step">step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.GreedyPolicy"><code class="flex name class">
<span>class <span class="ident">GreedyPolicy</span></span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for calling an Agent's action function and
selecting the greediest action.</p>
<p>Initalizes the Policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GreedyPolicy(Policy):
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function and
       selecting the greediest action.
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        action = np.argmax(action_func())
        return action</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.GreedyPolicy.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, action_func, training)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent should take.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_func</code></strong></dt>
<dd>A function that returns a list of values</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
Agent is in a training states</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, action_func, training):
    &#34;&#34;&#34;Returns the action the Agent should take.

    Args:
        action_func: A function that returns a list of values
        training: A boolean, which determines if the
                  Agent is in a training states
    &#34;&#34;&#34;
    action = np.argmax(action_func())
    return action</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Policy.end_episode" href="#paiutils.reinforcement.Policy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Policy.reset" href="#paiutils.reinforcement.Policy.reset">reset</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.GymWrapper"><code class="flex name class">
<span>class <span class="ident">GymWrapper</span></span>
<span>(</span><span>genv)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is a environment wrapper for OpenAI Gyms.</p>
<p>Initalizes state and action shapes and sets the state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>genv</code></strong></dt>
<dd>An OpenAI Gym</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GymWrapper(Environment):
    &#34;&#34;&#34;This class is a environment wrapper for OpenAI Gyms.&#34;&#34;&#34;

    def __init__(self, genv):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            genv: An OpenAI Gym
        &#34;&#34;&#34;
        self.genv = genv
        if isinstance(self.genv.observation_space, gym.spaces.Discrete):
            self.discrete_state_space = self.genv.observation_space.n
            self.state_shape = 1
        elif isinstance(self.genv.observation_space, gym.spaces.Box):
            self.discrete_state_space = None
            self.state_shape = self.genv.observation_space.shape
        else:
            raise NotImplementedError(&#39;Only Discrete and Box &#39;
                                      &#39;observation spaces &#39;
                                      &#39;are supported&#39;)

        if isinstance(self.genv.action_space, gym.spaces.Discrete):
            self.action_size = self.genv.action_space.n
        elif isinstance(self.genv.observation_space, gym.spaces.Box):
            if len(self.genv.action_space.shape) &gt; 1:
                raise NotImplementedError(&#39;Box action spaces with more &#39;
                                          &#39;than one dimension are not &#39;
                                          &#39;supported&#39;)
            self.action_size = self.genv.action_space.shape[0]
        else:
            raise NotImplementedError(&#39;Only Discrete action &#39;
                                      &#39;spaces are supported&#39;)

    def reset(self):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = self.genv.reset()
        if self.discrete_state_space is None:
            return self.state
        else:
            return [self.state]

    def step(self, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the action.

        Args:
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state, reward, terminal, _ = self.genv.step(action)
        if self.discrete_state_space is None:
            return self.state, reward, terminal
        else:
            return [self.state], reward, terminal

    def close(self):
        &#34;&#34;&#34;Closes any threads or loose ends of the environment.
        &#34;&#34;&#34;
        self.genv.close()

    def render(self):
        &#34;&#34;&#34;Renders the environment.
        &#34;&#34;&#34;
        self.genv.render()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Environment" href="#paiutils.reinforcement.Environment">Environment</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Environment" href="#paiutils.reinforcement.Environment">Environment</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Environment.close" href="#paiutils.reinforcement.Environment.close">close</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.play_episode" href="#paiutils.reinforcement.Environment.play_episode">play_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.play_episodes" href="#paiutils.reinforcement.Environment.play_episodes">play_episodes</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.render" href="#paiutils.reinforcement.Environment.render">render</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.reset" href="#paiutils.reinforcement.Environment.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.step" href="#paiutils.reinforcement.Environment.step">step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.LinearDecay"><code class="flex name class">
<span>class <span class="ident">LinearDecay</span></span>
<span>(</span><span>initial_value, total_steps, min_value=0, step_every_call=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This class decays a initial value to a minimum
value linearly through a given number of steps.
(formula: max(initial_value - (inital_value - min_value)
/ total_steps * steps, min_value))</p>
<p>Initalizes the state of the decay object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>initial_value</code></strong></dt>
<dd>A float, which is the starting value to decay</dd>
<dt><strong><code>total_steps</code></strong></dt>
<dd>An integer, which is the number of steps until
min_value would be reach</dd>
<dt><strong><code>min_value</code></strong></dt>
<dd>A float, which is the minimum value the decay
can reach</dd>
<dt><strong><code>step_every_call</code></strong></dt>
<dd>A boolean, which determines if each call should
step the decay</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearDecay(Decay):
    &#34;&#34;&#34;This class decays a initial value to a minimum
       value linearly through a given number of steps.
       (formula: max(initial_value - (inital_value - min_value)
                     / total_steps * steps, min_value))
    &#34;&#34;&#34;

    def __init__(self, initial_value, total_steps,
                 min_value=0, step_every_call=True):
        &#34;&#34;&#34;Initalizes the state of the decay object.

        Args:
            initial_value: A float, which is the starting value to decay
            total_steps: An integer, which is the number of steps until
                         min_value would be reach
            min_value: A float, which is the minimum value the decay
                       can reach
            step_every_call: A boolean, which determines if each call should
                             step the decay
        &#34;&#34;&#34;
        if initial_value &lt; min_value:
            raise ValueError(f&#39;initial_value {initial_value} must &#39;
                             f&#39;be greater or equal to min_value {min_value}&#39;)
        if not isinstance(total_steps, int):
            raise TypeError(&#39;total_steps should be an integer&#39;)
        self.initial_value = initial_value
        self.total_steps = total_steps
        self.min_value = min_value
        self.step_ever_call = step_every_call
        self.a = (-1 * (self.initial_value - self.min_value) /
                  self.total_steps)
        self.steps = 0

    def __call__(self):
        &#34;&#34;&#34;Returns the current value with regard to the state of decay.

        Returns:
            A float
        &#34;&#34;&#34;
        value = self.a * self.steps + self.initial_value
        if self.step_ever_call:
            self.step()
        return np.maximum(value, self.min_value)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Decay" href="#paiutils.reinforcement.Decay">Decay</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Decay" href="#paiutils.reinforcement.Decay">Decay</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Decay.reset" href="#paiutils.reinforcement.Decay.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Decay.step" href="#paiutils.reinforcement.Decay.step">step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.Memory"><code class="flex name class">
<span>class <span class="ident">Memory</span></span>
<span>(</span><span>max_len=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used by agents to store episode information.
(uses a normal python list)</p>
<p>Initalizes the memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_len</code></strong></dt>
<dd>An integer, which is the max length of memory
(if reached, the oldest memory will be removed)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Memory:
    &#34;&#34;&#34;This class is used by agents to store episode information.
       (uses a normal python list)
    &#34;&#34;&#34;

    def __init__(self, max_len=None):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        self.max_len = max_len
        self.buffer = []

    def __len__(self):
        &#34;&#34;&#34;Returns the number of entries in the memory.

        Returns:
            An integer
        &#34;&#34;&#34;
        return len(self.buffer)

    def add(self, x):
        &#34;&#34;&#34;Adds a entry to memory.

        Args:
            x: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer.append(x)
        if (self.max_len is not None
                and len(self.buffer) &gt; self.max_len):
            del self.buffer[0]

    def __getitem__(self, key):
        &#34;&#34;&#34;Returns an item given a key.

        Args:
            key: A valid key or index for a memory entry
        &#34;&#34;&#34;
        return self.buffer[key]

    def __setitem__(self, key, value):
        &#34;&#34;&#34;Sets a entry to a given key.

        Args:
            key: A valid key or index for a memory entry
            value: A entry similar to other entries
        &#34;&#34;&#34;
        self.buffer[key] = value

    def array(self):
        &#34;&#34;&#34;Returns a copy of the memory.

        Returns:
            A numpy ndarray
        &#34;&#34;&#34;
        return np.array(self.buffer)

    def reset(self):
        &#34;&#34;&#34;Resets or clears the memory.
        &#34;&#34;&#34;
        self.buffer.clear()

    def end_episode(self):
        &#34;&#34;&#34;Tells memory an episode ended.
        &#34;&#34;&#34;
        pass

    def save(self, file, name):
        &#34;&#34;&#34;Creates a h5py dataset with the memory data.

        Args:
            file: A h5py open file for writing
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        file.create_dataset(name, data=self.array())

    def load(self, file, name):
        &#34;&#34;&#34;Loads a h5py dataset with the saved memory data.

        Args:
            file: A h5py open file for reading
            name: A string, which is the dataset name
        &#34;&#34;&#34;
        for element in file[name]:
            self.add(element)

    @staticmethod
    def create_shuffled_subset(memories, subset_size, weights=None):
        &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

        Args:
            memories: A list of Memeory Objects
            subset_size: A integer, which is the size of the
                         outer dimension of each ndarray
            weights: A list of probabilities that add up to 1

        Returns:
            arrays and shuffled indexes
        &#34;&#34;&#34;
        length = len(memories[0])
        if subset_size &gt; length:
            raise ValueError(f&#39;Subset size {subset_size} is &#39;
                             f&#39;greater than memory length {length}&#39;)
        for memory in memories:
            if len(memory) != length:
                raise ValueError(&#39;Memories are not all the same length.&#39;)
            if not isinstance(memory, Memory):
                raise TypeError(&#39;Memories must also be Memory &#39;
                                &#39;or subclass instances&#39;)
        indexes = np.random.choice(np.arange(length),
                                   size=subset_size, replace=False,
                                   p=weights)
        arrays = [np.empty((subset_size, *memory[0].shape))
                  if isinstance(memory[0], np.ndarray)
                  else np.empty(subset_size)
                  for memory in memories]
        for ndx, rndx in enumerate(indexes):
            for array, memory in zip(arrays, memories):
                array[ndx] = memory[rndx]
        return arrays, indexes</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.ETDMemory" href="#paiutils.reinforcement.ETDMemory">ETDMemory</a></li>
<li><a title="paiutils.reinforcement.RingMemory" href="#paiutils.reinforcement.RingMemory">RingMemory</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="paiutils.reinforcement.Memory.create_shuffled_subset"><code class="name flex">
<span>def <span class="ident">create_shuffled_subset</span></span>(<span>memories, subset_size, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a list of numpy arrays of a shuffled subset of memories.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>memories</code></strong></dt>
<dd>A list of Memeory Objects</dd>
<dt><strong><code>subset_size</code></strong></dt>
<dd>A integer, which is the size of the
outer dimension of each ndarray</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>A list of probabilities that add up to 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>arrays and shuffled indexes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create_shuffled_subset(memories, subset_size, weights=None):
    &#34;&#34;&#34;Creates a list of numpy arrays of a shuffled subset of memories.

    Args:
        memories: A list of Memeory Objects
        subset_size: A integer, which is the size of the
                     outer dimension of each ndarray
        weights: A list of probabilities that add up to 1

    Returns:
        arrays and shuffled indexes
    &#34;&#34;&#34;
    length = len(memories[0])
    if subset_size &gt; length:
        raise ValueError(f&#39;Subset size {subset_size} is &#39;
                         f&#39;greater than memory length {length}&#39;)
    for memory in memories:
        if len(memory) != length:
            raise ValueError(&#39;Memories are not all the same length.&#39;)
        if not isinstance(memory, Memory):
            raise TypeError(&#39;Memories must also be Memory &#39;
                            &#39;or subclass instances&#39;)
    indexes = np.random.choice(np.arange(length),
                               size=subset_size, replace=False,
                               p=weights)
    arrays = [np.empty((subset_size, *memory[0].shape))
              if isinstance(memory[0], np.ndarray)
              else np.empty(subset_size)
              for memory in memories]
    for ndx, rndx in enumerate(indexes):
        for array, memory in zip(arrays, memories):
            array[ndx] = memory[rndx]
    return arrays, indexes</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.Memory.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a entry to memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>A entry similar to other entries</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, x):
    &#34;&#34;&#34;Adds a entry to memory.

    Args:
        x: A entry similar to other entries
    &#34;&#34;&#34;
    self.buffer.append(x)
    if (self.max_len is not None
            and len(self.buffer) &gt; self.max_len):
        del self.buffer[0]</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Memory.array"><code class="name flex">
<span>def <span class="ident">array</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of the memory.</p>
<h2 id="returns">Returns</h2>
<p>A numpy ndarray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def array(self):
    &#34;&#34;&#34;Returns a copy of the memory.

    Returns:
        A numpy ndarray
    &#34;&#34;&#34;
    return np.array(self.buffer)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Memory.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Tells memory an episode ended.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Tells memory an episode ended.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Memory.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, file, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a h5py dataset with the saved memory data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>A h5py open file for reading</dd>
<dt><strong><code>name</code></strong></dt>
<dd>A string, which is the dataset name</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, file, name):
    &#34;&#34;&#34;Loads a h5py dataset with the saved memory data.

    Args:
        file: A h5py open file for reading
        name: A string, which is the dataset name
    &#34;&#34;&#34;
    for element in file[name]:
        self.add(element)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Memory.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets or clears the memory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets or clears the memory.
    &#34;&#34;&#34;
    self.buffer.clear()</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Memory.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, file, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a h5py dataset with the memory data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>A h5py open file for writing</dd>
<dt><strong><code>name</code></strong></dt>
<dd>A string, which is the dataset name</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, file, name):
    &#34;&#34;&#34;Creates a h5py dataset with the memory data.

    Args:
        file: A h5py open file for writing
        name: A string, which is the dataset name
    &#34;&#34;&#34;
    file.create_dataset(name, data=self.array())</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.MemoryAgent"><code class="flex name class">
<span>class <span class="ident">MemoryAgent</span></span>
<span>(</span><span>action_size, policy)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is the base class for all agent that use memory.</p>
<p>Initalizes the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integer which is the discrete size
of the action space</dd>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MemoryAgent(Agent):
    &#34;&#34;&#34;This class is the base class for all agent that use memory.
    &#34;&#34;&#34;

    def __init__(self, action_size, policy):
        &#34;&#34;&#34;Initalizes the agent.

        Args:
            action_size: An integer which is the discrete size
                         of the action space
            policy: A policy instance
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.memory = {}
        self.time_distributed_states = None

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        Agent.end_episode(self)
        for memory in self.memory.values():
            memory.reset()

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode for the agent.&#34;&#34;&#34;
        Agent.end_episode(self)
        for memory in self.memory.values():
            memory.end_episode()
        if (&#39;states&#39; in self.memory
                and isinstance(self.memory[&#39;states&#39;], ETDMemory)):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])

    def load(self, path, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

            Returns:
                A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        if load_data:
            with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;r&#39;) as file:
                for name, memory in self.memory.items():
                    memory.load(file, name)
        return note

    def save(self, path, save_data=True, note=&#39;MemoryAgent&#39;):
        &#34;&#34;&#34;Saves a note and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        if save_data:
            with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;w&#39;) as file:
                for name, memory in self.memory.items():
                    memory.save(file, name)
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.DDPGAgent" href="#paiutils.reinforcement.DDPGAgent">DDPGAgent</a></li>
<li><a title="paiutils.reinforcement.DQNAgent" href="#paiutils.reinforcement.DQNAgent">DQNAgent</a></li>
<li><a title="paiutils.reinforcement.PGAgent" href="#paiutils.reinforcement.PGAgent">PGAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.MemoryAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_data=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
</dl>
<p>Returns:
A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_data=True):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded

        Returns:
            A string of note.txt
    &#34;&#34;&#34;
    note = Agent.load(self, path)
    if load_data:
        with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;r&#39;) as file:
            for name, memory in self.memory.items():
                memory.load(file, name)
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.MemoryAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_data=True, note='MemoryAgent')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_data=True, note=&#39;MemoryAgent&#39;):
    &#34;&#34;&#34;Saves a note and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = Agent.save(self, path, note)
    if save_data:
        with h5py.File(os.path.join(path, &#39;data.h5&#39;), &#39;w&#39;) as file:
            for name, memory in self.memory.items():
                memory.save(file, name)
    return path</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Agent.add_memory" href="#paiutils.reinforcement.Agent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.learn" href="#paiutils.reinforcement.Agent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.select_action" href="#paiutils.reinforcement.Agent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.set_playing_data" href="#paiutils.reinforcement.Agent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.MultiSeqAgentEnvironment"><code class="flex name class">
<span>class <span class="ident">MultiSeqAgentEnvironment</span></span>
<span>(</span><span>state_shape, action_size)</span>
</code></dt>
<dd>
<div class="desc"><p>This class handles the environment in which multiple agents
can perform actions against eachother in a sequential manner.</p>
<p>Initalizes state and action shapes and sets the state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state_shape</code></strong></dt>
<dd>A tuple of integers, which is the
expected state shape for the agent</dd>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integer which is the discrete size
of the action space</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiSeqAgentEnvironment(Environment):
    &#34;&#34;&#34;This class handles the environment in which multiple agents
       can perform actions against eachother in a sequential manner.
    &#34;&#34;&#34;

    def __init__(self, state_shape, action_size):
        &#34;&#34;&#34;Initalizes state and action shapes and sets the state.

        Args:
            state_shape: A tuple of integers, which is the
                         expected state shape for the agent
            action_size: An integer which is the discrete size
                         of the action space
        &#34;&#34;&#34;
        if isinstance(state_shape, int):
            self.discrete_state_space = state_shape
            state_shape = 1
        else:
            self.discrete_state_space = None
        self.state_shape = state_shape
        self.action_size = action_size

    def reset(self, num_agents):
        &#34;&#34;&#34;Resets the environment to its initialized state.

        Args:
            num_agents: An integer, which is the number of states needed

        Returns:
            A numpy ndarray, which is the state
        &#34;&#34;&#34;
        self.state = None
        return [self.state] * num_agents

    def step(self, agent_ndx, action):
        &#34;&#34;&#34;Moves the current state one step forward
           with regard to the agent&#39;s action.

        Args:
            agent_ndx: An integer, which is the index of the
                       agent taking a step
            action: An integer or value that determines an action

        Returns:
            A tuple of a ndarray (state), a float/integer (reward),
                and a boolean (terminal state)
        &#34;&#34;&#34;
        self.state = None
        return self.state, 0, False

    def play_episode(self, agents, max_steps, shuffle=True,
                     random=False, random_bounds=None,
                     render=False, verbose=True):
        &#34;&#34;&#34;Plays a single complete episode with the agents.

        Args:
            agents: A list of Agent instances, which will be used to
                    interact in the environment
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            shuffle: A boolean, which determines if the agents&#39; positions
                     should be shuffled
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen

        Returns:
            A tuple of a list of integers (last steps)
                and a list of floats (total rewards)
        &#34;&#34;&#34;
        num_agents = len(agents)
        ndxs = np.arange(num_agents)
        if shuffle:
            np.random.shuffle(ndxs)
        for ndx in ndxs:
            if not isinstance(agents[ndx], Agent):
                raise TypeError(f&#39;The instance agent ({ndx}) is &#39;
                                f&#39;not a child of Agent.&#39;)
            if not isinstance(agents[ndx].playing_data, PlayingData):
                raise ValueError(f&#39;Invalid playing_data value for agent &#39;
                                 f&#39;{ndx}. (Forgot to set playing_data?)&#39;)
        total_rewards = [0] * num_agents
        states = self.reset(num_agents)
        if render:
            self.render()
        break_loop = [False] * num_agents
        for step in range(1, max_steps + 1):
            for ndx in ndxs:
                if random:
                    if random_bounds is None:
                        action = np.random.randint(0, self.action_size)
                    else:
                        action = np.random.uniform(*random_bounds,
                                                   size=self.action_size)
                else:
                    action = agents[ndx].select_action(
                        states[ndx], training=agents[ndx].playing_data.training
                    )
                new_state, reward, terminal = self.step(ndx, action)
                total_rewards[ndx] += reward

                if agents[ndx].playing_data.memorizing:
                    agents[ndx].add_memory(states[ndx], action, new_state,
                                           reward, terminal)
                states[ndx] = new_state

                if verbose:
                    print(f&#39;Step: {step} - Agent: {ndx} - &#39;
                          f&#39;Reward: {reward} - Action: {action}&#39;)
                if render:
                    self.render()
                if (agents[ndx].playing_data.training
                        and agents[ndx].playing_data.learns_in_episode
                        and agents[ndx].playing_data.epochs &gt; 0):
                    agents[ndx].learn(
                        **agents[ndx].playing_data.learning_params
                    )
                if terminal:
                    break_loop[ndx] = True
                    if False not in break_loop:
                        break
            if False not in break_loop:
                break
        for ndx in ndxs:
            agents[ndx].end_episode()
            if (agents[ndx].playing_data.training
                    and not agents[ndx].playing_data.learns_in_episode
                    and agents[ndx].playing_data.epochs &gt; 0):
                agents[ndx].learn(**agents[ndx].playing_data.learning_params)
        return step, total_rewards

    def play_episodes(self, agents, num_episodes, max_steps, shuffle=True,
                      random=False, random_bounds=None, render=False,
                      verbose=True, episode_verbose=None,
                      end_episode_callback=None):
        &#34;&#34;&#34;Plays at least 1 complete episode with the agents.

        Args:
            agents: A list of Agent instances, which will be used to
                    interact in the environment
            num_episodes: An integer, which is the number of episodes to play
            max_steps: An integer, which is the max steps an episode
                       can take before terminating the episode
            shuffle: A boolean, which determines if the agents&#39; positions
                     should be shuffled
            random: A booelan, which determines if the agent should not
                    be used, but instead pick random actions
            random_bounds: A tuple of two bounds (lower and upper), which
                           are used for random actions that are not onehots
            render: A boolean, which determines if the environment should
                    be rendered each step
            verbose: A boolean, which determines if information should be
                     printed to the screen
            episode_verbose: A boolean, which determines if single episode
                             information should be printed to the screen
            end_episode_callback: A function called at the end of each episode
                                  with episode count, steps, and total reward
                                  from the most recent episode. If True
                                  is returned, play_episodes will stop
                                  early.

        Returns:
            A list of floats, which are the average total reward of all
                episodes for each agent
        &#34;&#34;&#34;
        if episode_verbose is None:
            episode_verbose = verbose
        num_agents = len(agents)
        total_rewards = [0] * num_agents
        best_rewards = [&#39;Unknown&#39;] * num_agents
        for episode in range(1, num_episodes + 1):
            step, total_reward = self.play_episode(
                agents, max_steps, shuffle=shuffle, random=random,
                random_bounds=random_bounds, render=render,
                verbose=episode_verbose,
            )
            for ndx in range(num_agents):
                total_rewards[ndx] += total_reward[ndx]
                if (best_rewards[ndx] == &#39;Unknown&#39;
                        or total_reward[ndx] &gt; best_rewards[ndx]):
                    best_rewards[ndx] = total_reward[ndx]
            if verbose:
                str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
                print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                      f&#39;Steps: {step}&#39;)
                for ndx in range(num_agents):
                    avg_total_reward = total_rewards[ndx] / episode
                    print(f&#39;Agent {ndx}. &#39;
                          f&#39;Total Reward: {total_reward[ndx]} - &#39;
                          f&#39;Best Total Reward: {best_rewards[ndx]} - &#39;
                          f&#39;Average Total Reward: {avg_total_reward}&#39;)
            if end_episode_callback is not None:
                end = end_episode_callback(
                    episode, step, total_reward
                )
                if end:
                    break
        return [tr / episode for tr in total_rewards]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Environment" href="#paiutils.reinforcement.Environment">Environment</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.MultiSeqAgentEnvironment.play_episode"><code class="name flex">
<span>def <span class="ident">play_episode</span></span>(<span>self, agents, max_steps, shuffle=True, random=False, random_bounds=None, render=False, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plays a single complete episode with the agents.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agents</code></strong></dt>
<dd>A list of Agent instances, which will be used to
interact in the environment</dd>
<dt><strong><code>max_steps</code></strong></dt>
<dd>An integer, which is the max steps an episode
can take before terminating the episode</dd>
<dt><strong><code>shuffle</code></strong></dt>
<dd>A boolean, which determines if the agents' positions
should be shuffled</dd>
<dt><strong><code>random</code></strong></dt>
<dd>A booelan, which determines if the agent should not
be used, but instead pick random actions</dd>
<dt><strong><code>random_bounds</code></strong></dt>
<dd>A tuple of two bounds (lower and upper), which
are used for random actions that are not onehots</dd>
<dt><strong><code>render</code></strong></dt>
<dd>A boolean, which determines if the environment should
be rendered each step</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information should be
printed to the screen</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple of a list of integers (last steps)
and a list of floats (total rewards)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play_episode(self, agents, max_steps, shuffle=True,
                 random=False, random_bounds=None,
                 render=False, verbose=True):
    &#34;&#34;&#34;Plays a single complete episode with the agents.

    Args:
        agents: A list of Agent instances, which will be used to
                interact in the environment
        max_steps: An integer, which is the max steps an episode
                   can take before terminating the episode
        shuffle: A boolean, which determines if the agents&#39; positions
                 should be shuffled
        random: A booelan, which determines if the agent should not
                be used, but instead pick random actions
        random_bounds: A tuple of two bounds (lower and upper), which
                       are used for random actions that are not onehots
        render: A boolean, which determines if the environment should
                be rendered each step
        verbose: A boolean, which determines if information should be
                 printed to the screen

    Returns:
        A tuple of a list of integers (last steps)
            and a list of floats (total rewards)
    &#34;&#34;&#34;
    num_agents = len(agents)
    ndxs = np.arange(num_agents)
    if shuffle:
        np.random.shuffle(ndxs)
    for ndx in ndxs:
        if not isinstance(agents[ndx], Agent):
            raise TypeError(f&#39;The instance agent ({ndx}) is &#39;
                            f&#39;not a child of Agent.&#39;)
        if not isinstance(agents[ndx].playing_data, PlayingData):
            raise ValueError(f&#39;Invalid playing_data value for agent &#39;
                             f&#39;{ndx}. (Forgot to set playing_data?)&#39;)
    total_rewards = [0] * num_agents
    states = self.reset(num_agents)
    if render:
        self.render()
    break_loop = [False] * num_agents
    for step in range(1, max_steps + 1):
        for ndx in ndxs:
            if random:
                if random_bounds is None:
                    action = np.random.randint(0, self.action_size)
                else:
                    action = np.random.uniform(*random_bounds,
                                               size=self.action_size)
            else:
                action = agents[ndx].select_action(
                    states[ndx], training=agents[ndx].playing_data.training
                )
            new_state, reward, terminal = self.step(ndx, action)
            total_rewards[ndx] += reward

            if agents[ndx].playing_data.memorizing:
                agents[ndx].add_memory(states[ndx], action, new_state,
                                       reward, terminal)
            states[ndx] = new_state

            if verbose:
                print(f&#39;Step: {step} - Agent: {ndx} - &#39;
                      f&#39;Reward: {reward} - Action: {action}&#39;)
            if render:
                self.render()
            if (agents[ndx].playing_data.training
                    and agents[ndx].playing_data.learns_in_episode
                    and agents[ndx].playing_data.epochs &gt; 0):
                agents[ndx].learn(
                    **agents[ndx].playing_data.learning_params
                )
            if terminal:
                break_loop[ndx] = True
                if False not in break_loop:
                    break
        if False not in break_loop:
            break
    for ndx in ndxs:
        agents[ndx].end_episode()
        if (agents[ndx].playing_data.training
                and not agents[ndx].playing_data.learns_in_episode
                and agents[ndx].playing_data.epochs &gt; 0):
            agents[ndx].learn(**agents[ndx].playing_data.learning_params)
    return step, total_rewards</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.MultiSeqAgentEnvironment.play_episodes"><code class="name flex">
<span>def <span class="ident">play_episodes</span></span>(<span>self, agents, num_episodes, max_steps, shuffle=True, random=False, random_bounds=None, render=False, verbose=True, episode_verbose=None, end_episode_callback=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plays at least 1 complete episode with the agents.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agents</code></strong></dt>
<dd>A list of Agent instances, which will be used to
interact in the environment</dd>
<dt><strong><code>num_episodes</code></strong></dt>
<dd>An integer, which is the number of episodes to play</dd>
<dt><strong><code>max_steps</code></strong></dt>
<dd>An integer, which is the max steps an episode
can take before terminating the episode</dd>
<dt><strong><code>shuffle</code></strong></dt>
<dd>A boolean, which determines if the agents' positions
should be shuffled</dd>
<dt><strong><code>random</code></strong></dt>
<dd>A booelan, which determines if the agent should not
be used, but instead pick random actions</dd>
<dt><strong><code>random_bounds</code></strong></dt>
<dd>A tuple of two bounds (lower and upper), which
are used for random actions that are not onehots</dd>
<dt><strong><code>render</code></strong></dt>
<dd>A boolean, which determines if the environment should
be rendered each step</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information should be
printed to the screen</dd>
<dt><strong><code>episode_verbose</code></strong></dt>
<dd>A boolean, which determines if single episode
information should be printed to the screen</dd>
<dt><strong><code>end_episode_callback</code></strong></dt>
<dd>A function called at the end of each episode
with episode count, steps, and total reward
from the most recent episode. If True
is returned, play_episodes will stop
early.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of floats, which are the average total reward of all
episodes for each agent</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play_episodes(self, agents, num_episodes, max_steps, shuffle=True,
                  random=False, random_bounds=None, render=False,
                  verbose=True, episode_verbose=None,
                  end_episode_callback=None):
    &#34;&#34;&#34;Plays at least 1 complete episode with the agents.

    Args:
        agents: A list of Agent instances, which will be used to
                interact in the environment
        num_episodes: An integer, which is the number of episodes to play
        max_steps: An integer, which is the max steps an episode
                   can take before terminating the episode
        shuffle: A boolean, which determines if the agents&#39; positions
                 should be shuffled
        random: A booelan, which determines if the agent should not
                be used, but instead pick random actions
        random_bounds: A tuple of two bounds (lower and upper), which
                       are used for random actions that are not onehots
        render: A boolean, which determines if the environment should
                be rendered each step
        verbose: A boolean, which determines if information should be
                 printed to the screen
        episode_verbose: A boolean, which determines if single episode
                         information should be printed to the screen
        end_episode_callback: A function called at the end of each episode
                              with episode count, steps, and total reward
                              from the most recent episode. If True
                              is returned, play_episodes will stop
                              early.

    Returns:
        A list of floats, which are the average total reward of all
            episodes for each agent
    &#34;&#34;&#34;
    if episode_verbose is None:
        episode_verbose = verbose
    num_agents = len(agents)
    total_rewards = [0] * num_agents
    best_rewards = [&#39;Unknown&#39;] * num_agents
    for episode in range(1, num_episodes + 1):
        step, total_reward = self.play_episode(
            agents, max_steps, shuffle=shuffle, random=random,
            random_bounds=random_bounds, render=render,
            verbose=episode_verbose,
        )
        for ndx in range(num_agents):
            total_rewards[ndx] += total_reward[ndx]
            if (best_rewards[ndx] == &#39;Unknown&#39;
                    or total_reward[ndx] &gt; best_rewards[ndx]):
                best_rewards[ndx] = total_reward[ndx]
        if verbose:
            str_time = datetime.now().strftime(r&#39;%H:%M:%S&#39;)
            print(f&#39;Time: {str_time} - Episode: {episode} - &#39;
                  f&#39;Steps: {step}&#39;)
            for ndx in range(num_agents):
                avg_total_reward = total_rewards[ndx] / episode
                print(f&#39;Agent {ndx}. &#39;
                      f&#39;Total Reward: {total_reward[ndx]} - &#39;
                      f&#39;Best Total Reward: {best_rewards[ndx]} - &#39;
                      f&#39;Average Total Reward: {avg_total_reward}&#39;)
        if end_episode_callback is not None:
            end = end_episode_callback(
                episode, step, total_reward
            )
            if end:
                break
    return [tr / episode for tr in total_rewards]</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.MultiSeqAgentEnvironment.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self, num_agents)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to its initialized state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_agents</code></strong></dt>
<dd>An integer, which is the number of states needed</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A numpy ndarray, which is the state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self, num_agents):
    &#34;&#34;&#34;Resets the environment to its initialized state.

    Args:
        num_agents: An integer, which is the number of states needed

    Returns:
        A numpy ndarray, which is the state
    &#34;&#34;&#34;
    self.state = None
    return [self.state] * num_agents</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.MultiSeqAgentEnvironment.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, agent_ndx, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Moves the current state one step forward
with regard to the agent's action.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agent_ndx</code></strong></dt>
<dd>An integer, which is the index of the
agent taking a step</dd>
<dt><strong><code>action</code></strong></dt>
<dd>An integer or value that determines an action</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple of a ndarray (state), a float/integer (reward),
and a boolean (terminal state)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, agent_ndx, action):
    &#34;&#34;&#34;Moves the current state one step forward
       with regard to the agent&#39;s action.

    Args:
        agent_ndx: An integer, which is the index of the
                   agent taking a step
        action: An integer or value that determines an action

    Returns:
        A tuple of a ndarray (state), a float/integer (reward),
            and a boolean (terminal state)
    &#34;&#34;&#34;
    self.state = None
    return self.state, 0, False</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Environment" href="#paiutils.reinforcement.Environment">Environment</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Environment.close" href="#paiutils.reinforcement.Environment.close">close</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.render" href="#paiutils.reinforcement.Environment.render">render</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.NoisePolicy"><code class="flex name class">
<span>class <span class="ident">NoisePolicy</span></span>
<span>(</span><span>noise_scale_decay_training, noise_scale_testing, action_bounds)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for adding normal noise to an Agent's action.</p>
<p>Initalizes the Noise Policy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>noise_scale_decay_training</code></strong></dt>
<dd>A decay instance, which decays
the noise scale (a fraction of
action range) for the policy</dd>
<dt><strong><code>noise_scale_testing</code></strong></dt>
<dd>A float, which is the noise scale
of the policy when the agent is not
training</dd>
<dt><strong><code>action_bounds</code></strong></dt>
<dd>A tuple of two floats/integers, which are
the lower and upper bounds of the action
range</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NoisePolicy(Policy):
    &#34;&#34;&#34;This class is used for adding normal noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds):
        &#34;&#34;&#34;Initalizes the Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
        &#34;&#34;&#34;
        self.noise_scale_decay_training = noise_scale_decay_training
        self.noise_scale_testing = noise_scale_testing
        self.action_bounds = action_bounds

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        noise = np.random.normal(scale=noise_scale, size=actions.shape)
        return np.clip(actions + noise, *self.action_bounds)

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
        self.noise_scale_decay_training.step()

    def reset(self):
        &#34;&#34;&#34;Resets decay state.&#34;&#34;&#34;
        self.noise_scale_decay_training.reset()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.TemporalNoisePolicy" href="#paiutils.reinforcement.TemporalNoisePolicy">TemporalNoisePolicy</a></li>
<li><a title="paiutils.reinforcement.UniformNoisePolicy" href="#paiutils.reinforcement.UniformNoisePolicy">UniformNoisePolicy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.NoisePolicy.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Tells the policy the episode ended and steps the decay.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
    self.noise_scale_decay_training.step()</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.NoisePolicy.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets decay state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets decay state.&#34;&#34;&#34;
    self.noise_scale_decay_training.reset()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Policy.select_action" href="#paiutils.reinforcement.Policy.select_action">select_action</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.PGAgent"><code class="flex name class">
<span>class <span class="ident">PGAgent</span></span>
<span>(</span><span>amodel, discounted_rate, create_memory=&lt;function PGAgent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is an Agent that uses a Neural Network like the DQN Agent,
but instead of learning to predict Q values, it predicts actions. It
learns to predict these actions through Policy Gradients (PG).</p>
<p>Initalizes the Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PGAgent(MemoryAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Neural Network like the DQN Agent,
       but instead of learning to predict Q values, it predicts actions. It
       learns to predict these actions through Policy Gradients (PG).
    &#34;&#34;&#34;

    def __init__(self, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        output_shape = amodel.output_shape
        if isinstance(output_shape, list):
            output_shape = output_shape[-1]
        MemoryAgent.__init__(self, output_shape[1], Policy())
        self.amodel = amodel
        self.discounted_rate = discounted_rate
        self.states = create_memory(self.amodel.input_shape,
                                    keras.backend.floatx())
        self.actions = create_memory(output_shape,
                                     keras.backend.floatx())
        self.drewards = create_memory((None,),
                                      keras.backend.floatx())
        self.memory = {
            &#39;states&#39;: self.states, &#39;actions&#39;: self.actions,
            &#39;drewards&#39;: self.drewards,
        }
        if isinstance(self.memory[&#39;states&#39;], ETDMemory):
            self.time_distributed_states = np.array([
                self.memory[&#39;states&#39;].buffer[0]
                for _ in range(self.memory[&#39;states&#39;].num_time_steps)
            ])
        self.episode_rewards = []
        self.action_identity = np.identity(self.action_size)
        self.metric = keras.metrics.Mean(name=&#39;loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the action for
            training: A boolean, which determines if the
                      agent is training (does nothing)

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        actions = self.amodel(np.expand_dims(state, axis=0),
                              training=False)
        if isinstance(actions, list):
            return actions[-1][0].numpy()
        return np.random.choice(np.arange(self.action_size),
                                p=actions[0].numpy())

    def set_playing_data(self, training=False, memorizing=False,
                         batch_size=None, mini_batch=0, epochs=1,
                         repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;entropy_coef&#39;: entropy_coef,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        False, learning_params)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(self.action_identity[action])
        self.episode_rewards.append(reward)

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        if len(self.episode_rewards) &gt; 0:
            dreward = 0
            dreward_list = []
            for reward in reversed(self.episode_rewards):
                dreward *= self.discounted_rate
                dreward += reward
                dreward_list.append(dreward)
            self.episode_rewards.clear()
            for dreward in reversed(dreward_list):
                self.drewards.add(dreward)

        MemoryAgent.end_episode(self)

    def _train_step(self, states, drewards, actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            y_pred = self.amodel(states, training=True)
            log_y_pred = tf.math.log(y_pred + keras.backend.epsilon())
            log_probs = tf.reduce_sum(
                actions * log_y_pred, axis=1
            )
            loss = -tf.reduce_mean(drewards * log_probs)
            entropy = tf.reduce_sum(
                y_pred * log_y_pred, axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)

    def _train(self, states, drewards, actions,
               epochs, batch_size, entropy_coef, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            actions: A numpy array that contains the actions performed
                     (onehot encodings for discrete action spaces)
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean loss of batches (not exactly a loss)
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             actions.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            loss_results = self.metric.result()
            self.metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;loss: {loss_results}&#39;)
        return loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions], length
            )
            std = arrays[1].std()
            if std == 0:
                return False
            arrays[1] = (arrays[1] - arrays[1].mean()) / std
            self._train(*arrays, epochs, batch_size,
                        entropy_coef, verbose=verbose)

    def load(self, path, load_model=True, load_data=True):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architecture and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = MemoryAgent.load(self, path, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(file.read())
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return note

    def save(self, path, save_model=True, save_data=True, note=&#39;PGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architecture and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = MemoryAgent.save(self, path, save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.A2CAgent" href="reinforcement_agents.html#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></li>
<li><a title="paiutils.reinforcement_agents.DQNPGAgent" href="reinforcement_agents.html#paiutils.reinforcement_agents.DQNPGAgent">DQNPGAgent</a></li>
<li><a title="paiutils.reinforcement_agents.PGCAgent" href="reinforcement_agents.html#paiutils.reinforcement_agents.PGCAgent">PGCAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.PGAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action (discarded)</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode
(discarded)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action (discarded)
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
                  (discarded)
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.actions.add(self.action_identity[action])
    self.episode_rewards.append(reward)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Ends the episode and creates drewards based
on the episodes rewards.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Ends the episode and creates drewards based
       on the episodes rewards.
    &#34;&#34;&#34;
    if len(self.episode_rewards) &gt; 0:
        dreward = 0
        dreward_list = []
        for reward in reversed(self.episode_rewards):
            dreward *= self.discounted_rate
            dreward += reward
            dreward_list.append(dreward)
        self.episode_rewards.clear()
        for dreward in reversed(dreward_list):
            self.drewards.add(dreward)

    MemoryAgent.end_episode(self)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, batch_size=None, mini_batch=0, epochs=1, repeat=1, entropy_coef=0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a sample of its experiences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini_batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled)</dd>
<dt><strong><code>entropy_coef</code></strong></dt>
<dd>A float, which is the coefficent of entropy to add
to the actor loss</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, batch_size=None, mini_batch=0,
          epochs=1, repeat=1, entropy_coef=0, verbose=True):
    &#34;&#34;&#34;Trains the agent on a sample of its experiences.

    Args:
        batch_size: An integer, which is the size of each batch
                    within the mini_batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled)
        entropy_coef: A float, which is the coefficent of entropy to add
                      to the actor loss
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    if batch_size is None:
        batch_size = len(self.states)
    if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
        length = mini_batch
    else:
        length = len(self.states)

    for count in range(1, repeat+1):
        if verbose:
            print(f&#39;Repeat {count}/{repeat}&#39;)

        arrays, _ = self.states.create_shuffled_subset(
            [self.states, self.drewards, self.actions], length
        )
        std = arrays[1].std()
        if std == 0:
            return False
        arrays[1] = (arrays[1] - arrays[1].mean()) / std
        self._train(*arrays, epochs, batch_size,
                    entropy_coef, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_model=True, load_data=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_model</code></strong></dt>
<dd>A boolean, which determines if the model
architecture and weights
should be loaded</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_model=True, load_data=True):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_model: A boolean, which determines if the model
                    architecture and weights
                    should be loaded
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = MemoryAgent.load(self, path, load_data=load_data)
    if load_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
            self.amodel = model_from_json(file.read())
        self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_model=True, save_data=True, note='PGAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note, model weights, and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_model</code></strong></dt>
<dd>A boolean, which determines if the model
architecture and weights
should be saved</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_model=True, save_data=True, note=&#39;PGAgent Save&#39;):
    &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_model: A boolean, which determines if the model
                    architecture and weights
                    should be saved
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = MemoryAgent.save(self, path, save_data=save_data, note=note)
    if save_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.amodel.to_json())
        self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value, which is the state to predict
the action for</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training (does nothing)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value, which is the selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value, which is the state to predict
               the action for
        training: A boolean, which determines if the
                  agent is training (does nothing)

    Returns:
        A value, which is the selected action
    &#34;&#34;&#34;
    if (self.time_distributed_states is not None
            and state.shape == self.amodel.input_shape[2:]):
        self.time_distributed_states = np.roll(
            self.time_distributed_states, -1
        )
        self.time_distributed_states[-1] = state
        state = self.time_distributed_states

    actions = self.amodel(np.expand_dims(state, axis=0),
                          training=False)
    if isinstance(actions, list):
        return actions[-1][0].numpy()
    return np.random.choice(np.arange(self.action_size),
                            p=actions[0].numpy())</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PGAgent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, memorizing=False, batch_size=None, mini_batch=0, epochs=1, repeat=1, entropy_coef=0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>memorizing</code></strong></dt>
<dd>A boolean, which determines if the agent
should be adding the information obtained
through playing an episode to memory</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>entropy_coef</code></strong></dt>
<dd>A float, which is the coefficent of entropy to add
to the actor loss</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, memorizing=False,
                     batch_size=None, mini_batch=0, epochs=1,
                     repeat=1, entropy_coef=0, verbose=True):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        memorizing: A boolean, which determines if the agent
                    should be adding the information obtained
                    through playing an episode to memory
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        entropy_coef: A float, which is the coefficent of entropy to add
                      to the actor loss
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    learning_params = {&#39;batch_size&#39;: batch_size,
                       &#39;mini_batch&#39;: mini_batch,
                       &#39;epochs&#39;: epochs,
                       &#39;repeat&#39;: repeat,
                       &#39;entropy_coef&#39;: entropy_coef,
                       &#39;verbose&#39;: verbose}
    self.playing_data = PlayingData(training, memorizing, epochs,
                                    False, learning_params)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.MemoryAgent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.PQAgent"><code class="flex name class">
<span>class <span class="ident">PQAgent</span></span>
<span>(</span><span>discrete_state_space, action_size, policy, discounted_rates, learning_rates)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is like QAgent, but it uses multiple variables at once,
hince Parallel Q Agent.</p>
<p>Initalizes the Q-learning agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>discrete_state_space</code></strong></dt>
<dd>An integer, which</dd>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integers, which is the
action size of the environment</dd>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
<dt><strong><code>discounted_rates</code></strong></dt>
<dd>A list of floats within 0.0-1.0, which
are the rates that future rewards should
be counted for the current reward</dd>
<dt><strong><code>learning_rate</code></strong></dt>
<dd>A list of floats, which are the rates
that the table is updated with the currect
Q reward</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PQAgent(QAgent):
    &#34;&#34;&#34;This class is like QAgent, but it uses multiple variables at once,
       hince Parallel Q Agent.
    &#34;&#34;&#34;

    def __init__(self, discrete_state_space, action_size,
                 policy, discounted_rates, learning_rates):
        &#34;&#34;&#34;Initalizes the Q-learning agent.

        Args:
            discrete_state_space: An integer, which
            action_size: An integers, which is the
                         action size of the environment
            policy: A policy instance
            discounted_rates: A list of floats within 0.0-1.0, which
                              are the rates that future rewards should
                              be counted for the current reward
            learning_rate: A list of floats, which are the rates
                           that the table is updated with the currect
                           Q reward
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.discrete_state_space = discrete_state_space
        self.discounted_rates = np.array(discounted_rates)
        self.learning_rates = np.array(learning_rates)
        self.inv_learning_rates = 1 - self.learning_rates
        self.qtables = np.zeros((len(self.learning_rates),
                                 len(self.discounted_rates),
                                 self.discrete_state_space,
                                 self.action_size))
        self.selected_qtable = lambda: self.qtables[0, 0]
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to look up the action for in the table
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            nonlocal state
            if isinstance(state, list) and len(state) == 1:
                state = state[0]
            return self.selected_qtable()[state]
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, learning_rate_ndx=None,
                         discounted_rate_ndx=None, verbose=False):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            learning_rate_ndx: An integer, which is a ndx for
                               the learning rates
            discounted_rate_ndx: An integer, which is a ndx for
                                 the discounted rate
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        lrn = 0
        drn = 0
        if learning_rate_ndx is not None:
            lrn = learning_rate_ndx
        if discounted_rate_ndx is not None:
            drn = discounted_rate_ndx
        self.selected_qtable = lambda: self.qtables[lrn, drn]
        self.playing_data = PlayingData(
            training, training, 1, True,
            {&#39;learning_rate_ndx&#39;: learning_rate_ndx,
             &#39;discounted_rate_ndx&#39;: discounted_rate_ndx,
             &#39;verbose&#39;: verbose}
        )

    def learn(self, learning_rate_ndx=None,
              discounted_rate_ndx=None, verbose=True):
        &#34;&#34;&#34;Trains the agent on its last experience.

        Args:
            learning_rate_ndx: An integer, which is a ndx for
                               the learning rates
            discounted_rate_ndx: An integer, which is a ndx for
                                 the discounted rate
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        if self.state is None:
            raise ValueError(&#39;Memory is empty&#39;)
        lrn = learning_rate_ndx
        drn = discounted_rate_ndx
        if lrn is None and drn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates *
                    np.amax(self.qtables[:, :, self.new_state], axis=-1)
                )
            self.qtables[:, :, self.state, self.action] = (
                self.inv_learning_rates[:, np.newaxis] *
                self.qtables[:, :, self.state, self.action] +
                self.learning_rates[:, np.newaxis] *
                (self.reward + discounted_reward)
            )
        elif drn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates *
                    np.amax(self.qtables[lrn, :, self.new_state], axis=-1)
                )
            self.qtables[lrn, :, self.state, self.action] = (
                self.inv_learning_rates[lrn, np.newaxis] *
                self.qtables[lrn, :, self.state, self.action] +
                self.learning_rates[lrn, np.newaxis] *
                (self.reward + discounted_reward)
            )
        elif lrn is None:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates[drn] *
                    np.amax(self.qtables[:, drn, self.new_state], axis=-1)
                )
            self.qtables[:, drn, self.state, self.action] = (
                self.inv_learning_rates[:, np.newaxis] *
                self.qtables[:, drn, self.state, self.action] +
                self.learning_rates[:, np.newaxis] *
                (self.reward + discounted_reward)
            )
        else:
            discounted_reward = 0
            if not self.terminal:
                discounted_reward = (
                    self.discounted_rates[drn] *
                    np.amax(self.qtables[lrn, drn, self.new_state], axis=-1)
                )
            self.qtables[lrn, drn, self.state, self.action] = (
                self.inv_learning_rates[lrn] *
                self.qtables[lrn, drn, self.state, self.action] +
                self.learning_rates[lrn] *
                (self.reward + discounted_reward)
            )

        if verbose:
            pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
                  from

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        self.qtables = np.load(os.path.join(path, &#39;qtables.npy&#39;))
        return note

    def save(self, path, note=&#39;PQAgent Save&#39;):
        &#34;&#34;&#34;Saves a note and qtables to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        np.save(os.path.join(path, &#39;qtables.npy&#39;), self.qtables)
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.QAgent" href="#paiutils.reinforcement.QAgent">QAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.PQAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, learning_rate_ndx=None, discounted_rate_ndx=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on its last experience.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>learning_rate_ndx</code></strong></dt>
<dd>An integer, which is a ndx for
the learning rates</dd>
<dt><strong><code>discounted_rate_ndx</code></strong></dt>
<dd>An integer, which is a ndx for
the discounted rate</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information
should be printed to the screen</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, learning_rate_ndx=None,
          discounted_rate_ndx=None, verbose=True):
    &#34;&#34;&#34;Trains the agent on its last experience.

    Args:
        learning_rate_ndx: An integer, which is a ndx for
                           the learning rates
        discounted_rate_ndx: An integer, which is a ndx for
                             the discounted rate
        verbose: A boolean, which determines if information
                 should be printed to the screen
    &#34;&#34;&#34;
    if self.state is None:
        raise ValueError(&#39;Memory is empty&#39;)
    lrn = learning_rate_ndx
    drn = discounted_rate_ndx
    if lrn is None and drn is None:
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (
                self.discounted_rates *
                np.amax(self.qtables[:, :, self.new_state], axis=-1)
            )
        self.qtables[:, :, self.state, self.action] = (
            self.inv_learning_rates[:, np.newaxis] *
            self.qtables[:, :, self.state, self.action] +
            self.learning_rates[:, np.newaxis] *
            (self.reward + discounted_reward)
        )
    elif drn is None:
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (
                self.discounted_rates *
                np.amax(self.qtables[lrn, :, self.new_state], axis=-1)
            )
        self.qtables[lrn, :, self.state, self.action] = (
            self.inv_learning_rates[lrn, np.newaxis] *
            self.qtables[lrn, :, self.state, self.action] +
            self.learning_rates[lrn, np.newaxis] *
            (self.reward + discounted_reward)
        )
    elif lrn is None:
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (
                self.discounted_rates[drn] *
                np.amax(self.qtables[:, drn, self.new_state], axis=-1)
            )
        self.qtables[:, drn, self.state, self.action] = (
            self.inv_learning_rates[:, np.newaxis] *
            self.qtables[:, drn, self.state, self.action] +
            self.learning_rates[:, np.newaxis] *
            (self.reward + discounted_reward)
        )
    else:
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (
                self.discounted_rates[drn] *
                np.amax(self.qtables[lrn, drn, self.new_state], axis=-1)
            )
        self.qtables[lrn, drn, self.state, self.action] = (
            self.inv_learning_rates[lrn] *
            self.qtables[lrn, drn, self.state, self.action] +
            self.learning_rates[lrn] *
            (self.reward + discounted_reward)
        )

    if verbose:
        pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PQAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load
from</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
              from

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = Agent.load(self, path)
    self.qtables = np.load(os.path.join(path, &#39;qtables.npy&#39;))
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PQAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, note='PQAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note and qtables to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is the note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, note=&#39;PQAgent Save&#39;):
    &#34;&#34;&#34;Saves a note and qtables to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        note: A string, which is the note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = Agent.save(self, path, note)
    np.save(os.path.join(path, &#39;qtables.npy&#39;), self.qtables)
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.PQAgent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, learning_rate_ndx=None, discounted_rate_ndx=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>learning_rate_ndx</code></strong></dt>
<dd>An integer, which is a ndx for
the learning rates</dd>
<dt><strong><code>discounted_rate_ndx</code></strong></dt>
<dd>An integer, which is a ndx for
the discounted rate</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, learning_rate_ndx=None,
                     discounted_rate_ndx=None, verbose=False):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        learning_rate_ndx: An integer, which is a ndx for
                           the learning rates
        discounted_rate_ndx: An integer, which is a ndx for
                             the discounted rate
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    lrn = 0
    drn = 0
    if learning_rate_ndx is not None:
        lrn = learning_rate_ndx
    if discounted_rate_ndx is not None:
        drn = discounted_rate_ndx
    self.selected_qtable = lambda: self.qtables[lrn, drn]
    self.playing_data = PlayingData(
        training, training, 1, True,
        {&#39;learning_rate_ndx&#39;: learning_rate_ndx,
         &#39;discounted_rate_ndx&#39;: discounted_rate_ndx,
         &#39;verbose&#39;: verbose}
    )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.QAgent" href="#paiutils.reinforcement.QAgent">QAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.QAgent.add_memory" href="#paiutils.reinforcement.QAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.select_action" href="#paiutils.reinforcement.QAgent.select_action">select_action</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.PlayingData"><code class="flex name class">
<span>class <span class="ident">PlayingData</span></span>
<span>(</span><span>training, memorizing, epochs, learns_in_episode, learning_params)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for containing data
that the environment needs to know, but the agent has.</p>
<p>Initalizes the data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>memorizing</code></strong></dt>
<dd>A boolean, which determines if the agent
should be adding the information obtained
through playing an episode to memory</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>learns_in_episode</code></strong></dt>
<dd>A boolean, which determines if the agent
learns during a episode or at the end</dd>
<dt><strong><code>learning_Args</code></strong></dt>
<dd>A dictionary of parameters for the agent's
learn method</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PlayingData:
    &#34;&#34;&#34;This class is used for containing data
       that the environment needs to know, but the agent has.
    &#34;&#34;&#34;

    def __init__(self, training, memorizing, epochs,
                 learns_in_episode, learning_params):
        &#34;&#34;&#34;Initalizes the data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            learning_Args: A dictionary of parameters for the agent&#39;s
                             learn method
        &#34;&#34;&#34;
        if not (training is True or training is False):
            raise ValueError(&#39;Invalid training value. Must be True or False.&#39;)
        if not (memorizing is True or memorizing is False):
            raise ValueError(
                &#39;Invalid memorizing value. Must be True or False.&#39;)
        if epochs &lt; 0:
            raise ValueError(&#39;Invalid epoch value. Must &#39;
                             &#39;be greater or equal to zero.&#39;)
        if not (learns_in_episode is True or learns_in_episode is False):
            raise ValueError(&#39;Invalid learns_in_episode value. &#39;
                             &#39;Must be True or False.&#39;)
        if not isinstance(learning_params, dict):
            raise TypeError(&#39;Invalid learning_params value. &#39;
                            &#39;Must be a dictionary.&#39;)
        self.training = training
        self.memorizing = memorizing
        self.epochs = epochs
        self.learns_in_episode = learns_in_episode
        self.learning_params = learning_params</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Policy"><code class="flex name class">
<span>class <span class="ident">Policy</span></span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for calling an Agent's action function.</p>
<p>Initalizes the Policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Policy:
    &#34;&#34;&#34;This class is used for calling an Agent&#39;s action function.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initalizes the Policy.&#34;&#34;&#34;
        pass

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        return action_func()

    def reset(self):
        &#34;&#34;&#34;Resets any states.&#34;&#34;&#34;
        pass

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended.&#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.AsceticPolicy" href="#paiutils.reinforcement.AsceticPolicy">AsceticPolicy</a></li>
<li><a title="paiutils.reinforcement.GreedyPolicy" href="#paiutils.reinforcement.GreedyPolicy">GreedyPolicy</a></li>
<li><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></li>
<li><a title="paiutils.reinforcement.StochasticPolicy" href="#paiutils.reinforcement.StochasticPolicy">StochasticPolicy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.Policy.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Tells the policy the episode ended.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Tells the policy the episode ended.&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Policy.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets any states.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets any states.&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.Policy.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, action_func, training)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent should take.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_func</code></strong></dt>
<dd>A function that returns a value</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
Agent is in a training states</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, action_func, training):
    &#34;&#34;&#34;Returns the action the Agent should take.

    Args:
        action_func: A function that returns a value
        training: A boolean, which determines if the
                  Agent is in a training states
    &#34;&#34;&#34;
    return action_func()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.QAgent"><code class="flex name class">
<span>class <span class="ident">QAgent</span></span>
<span>(</span><span>discrete_state_space, action_size, policy, discounted_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is a Q-learning Agent. It does not uses a neural network,
but instead uses a table.</p>
<p>Initalizes the Q-learning agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>discrete_state_space</code></strong></dt>
<dd>An integer, which is the size of
the state space</dd>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integers, which is the
action size of the environment</dd>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QAgent(Agent):
    &#34;&#34;&#34;This class is a Q-learning Agent. It does not uses a neural network,
       but instead uses a table.
    &#34;&#34;&#34;

    def __init__(self, discrete_state_space, action_size,
                 policy, discounted_rate):
        &#34;&#34;&#34;Initalizes the Q-learning agent.

        Args:
            discrete_state_space: An integer, which is the size of
                                  the state space
            action_size: An integers, which is the
                         action size of the environment
            policy: A policy instance
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
        &#34;&#34;&#34;
        Agent.__init__(self, action_size, policy)
        self.discrete_state_space = discrete_state_space
        self.discounted_rate = discounted_rate
        self.qtable = np.zeros((self.discrete_state_space,
                                self.action_size))
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value or list of values, which is the
                   state to look up the action for in the table
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        def _select_action():
            nonlocal state
            if isinstance(state, list) and len(state) == 1:
                state = state[0]
            return self.qtable[state]
        return self.policy.select_action(_select_action,
                                         training=training)

    def set_playing_data(self, training=False, learning_rate=None,
                         verbose=False):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            learning_rate: A float, which is the rate that the table
                           is updated with the currect Q reward
                           (Must be provided if training is True)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.playing_data = PlayingData(training, training, 1, True,
                                        {&#39;learning_rate&#39;: learning_rate,
                                         &#39;verbose&#39;: verbose})

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        if isinstance(state, list) and len(state) == 1:
            state = state[0]
        if isinstance(new_state, list) and len(new_state) == 1:
            new_state = new_state[0]
        self.state = state
        self.action = action
        self.new_state = new_state
        self.reward = reward
        self.terminal = terminal

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        self.state = None
        self.action = None
        self.new_state = None
        self.reward = None
        self.terminal = None

    def learn(self, learning_rate, verbose=True):
        &#34;&#34;&#34;Trains the agent on its last experience.

        Args:
            learning_rate: A float, which is the rate that the table
                           is updated with the currect Q reward
            verbose: A boolean, which determines if information
                     should be printed to the screen
        &#34;&#34;&#34;
        if self.state is None:
            raise ValueError(&#39;Memory is empty&#39;)
        discounted_reward = 0
        if not self.terminal:
            discounted_reward = (self.discounted_rate *
                                 np.amax(self.qtable[self.new_state]))
        self.qtable[self.state, self.action] = (
            (1 - learning_rate) * self.qtable[self.state, self.action] +
            learning_rate * (self.reward + discounted_reward)
        )
        if verbose:
            pass

    def load(self, path):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = Agent.load(self, path)
        self.qtable = np.load(os.path.join(path, &#39;qtable.npy&#39;))
        return note

    def save(self, path, note=&#39;QAgent Save&#39;):
        &#34;&#34;&#34;Saves a note and qtable to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            note: A string, which is the note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = Agent.save(self, path, note)
        np.save(os.path.join(path, &#39;qtable.npy&#39;), self.qtable)
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.PQAgent" href="#paiutils.reinforcement.PQAgent">PQAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.QAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    if isinstance(state, list) and len(state) == 1:
        state = state[0]
    if isinstance(new_state, list) and len(new_state) == 1:
        new_state = new_state[0]
    self.state = state
    self.action = action
    self.new_state = new_state
    self.reward = reward
    self.terminal = terminal</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.QAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, learning_rate, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on its last experience.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>learning_rate</code></strong></dt>
<dd>A float, which is the rate that the table
is updated with the currect Q reward</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if information
should be printed to the screen</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, learning_rate, verbose=True):
    &#34;&#34;&#34;Trains the agent on its last experience.

    Args:
        learning_rate: A float, which is the rate that the table
                       is updated with the currect Q reward
        verbose: A boolean, which determines if information
                 should be printed to the screen
    &#34;&#34;&#34;
    if self.state is None:
        raise ValueError(&#39;Memory is empty&#39;)
    discounted_reward = 0
    if not self.terminal:
        discounted_reward = (self.discounted_rate *
                             np.amax(self.qtable[self.new_state]))
    self.qtable[self.state, self.action] = (
        (1 - learning_rate) * self.qtable[self.state, self.action] +
        learning_rate * (self.reward + discounted_reward)
    )
    if verbose:
        pass</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.QAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, note='QAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note and qtable to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is the note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, note=&#39;QAgent Save&#39;):
    &#34;&#34;&#34;Saves a note and qtable to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        note: A string, which is the note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = Agent.save(self, path, note)
    np.save(os.path.join(path, &#39;qtable.npy&#39;), self.qtable)
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.QAgent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state to look up the action for in the table</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value, which is the selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value or list of values, which is the
               state to look up the action for in the table
        training: A boolean, which determines if the
                  agent is training

    Returns:
        A value, which is the selected action
    &#34;&#34;&#34;
    def _select_action():
        nonlocal state
        if isinstance(state, list) and len(state) == 1:
            state = state[0]
        return self.qtable[state]
    return self.policy.select_action(_select_action,
                                     training=training)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.QAgent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, learning_rate=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>learning_rate</code></strong></dt>
<dd>A float, which is the rate that the table
is updated with the currect Q reward
(Must be provided if training is True)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, learning_rate=None,
                     verbose=False):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        learning_rate: A float, which is the rate that the table
                       is updated with the currect Q reward
                       (Must be provided if training is True)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    self.playing_data = PlayingData(training, training, 1, True,
                                    {&#39;learning_rate&#39;: learning_rate,
                                     &#39;verbose&#39;: verbose})</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Agent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.load" href="#paiutils.reinforcement.Agent.load">load</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.RingMemory"><code class="flex name class">
<span>class <span class="ident">RingMemory</span></span>
<span>(</span><span>max_len)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used by agents to store episode information.
(uses a normal python list)</p>
<p>Initalizes the memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_len</code></strong></dt>
<dd>An integer, which is the max length of memory
(if reached, the oldest memory will be removed)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RingMemory(Memory):
    def __init__(self, max_len):
        &#34;&#34;&#34;Initalizes the memory.

        Args:
            max_len: An integer, which is the max length of memory
                     (if reached, the oldest memory will be removed)
        &#34;&#34;&#34;
        Memory.__init__(self, max_len=max_len)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Memory" href="#paiutils.reinforcement.Memory">Memory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.Memory" href="#paiutils.reinforcement.Memory">Memory</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.Memory.add" href="#paiutils.reinforcement.Memory.add">add</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.array" href="#paiutils.reinforcement.Memory.array">array</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.create_shuffled_subset" href="#paiutils.reinforcement.Memory.create_shuffled_subset">create_shuffled_subset</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.end_episode" href="#paiutils.reinforcement.Memory.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.load" href="#paiutils.reinforcement.Memory.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.reset" href="#paiutils.reinforcement.Memory.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.save" href="#paiutils.reinforcement.Memory.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.StochasticPolicy"><code class="flex name class">
<span>class <span class="ident">StochasticPolicy</span></span>
<span>(</span><span>policy, stochasticity_decay_training, stochasticity_testing, action_size)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for calling an Agent's action function.</p>
<p>Initalizes the Policy's states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance</dd>
<dt><strong><code>stochasticity_decay_training</code></strong></dt>
<dd>A decay instance which decays
the stochasticity of the policy</dd>
<dt><strong><code>stochasticity_testing</code></strong></dt>
<dd>A float, which is the stochasticity
of the policy when the agent is not
training</dd>
<dt><strong><code>action_size</code></strong></dt>
<dd>An integer, which is the size of the action ndarray</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StochasticPolicy(Policy):
    def __init__(self, policy, stochasticity_decay_training,
                 stochasticity_testing, action_size):
        &#34;&#34;&#34;Initalizes the Policy&#39;s states.

        Args:
            policy: A policy instance
            stochasticity_decay_training: A decay instance which decays
                                          the stochasticity of the policy
            stochasticity_testing: A float, which is the stochasticity
                                   of the policy when the agent is not
                                   training
            action_size: An integer, which is the size of the action ndarray
        &#34;&#34;&#34;
        super().__init__()
        self.policy = policy
        self.stochasticity_decay_training = stochasticity_decay_training
        self.stochasticity_testing = stochasticity_testing
        self.action_size = action_size

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a list of values
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        if training:
            stochasticity = self.stochasticity_decay_training()
        else:
            stochasticity = self.stochasticity_testing
        if np.random.uniform() &lt; stochasticity:
            return np.random.randint(0, self.action_size)
        else:
            return self.policy.select_action(action_func, training)

    def end_episode(self):
        &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
        self.stochasticity_decay_training.step()

    def reset(self):
        &#34;&#34;&#34;Resets state of the stochasticity decay instance.&#34;&#34;&#34;
        self.stochasticity_decay_training.reset()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.StochasticPolicy.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Tells the policy the episode ended and steps the decay.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Tells the policy the episode ended and steps the decay.&#34;&#34;&#34;
    self.stochasticity_decay_training.step()</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.StochasticPolicy.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets state of the stochasticity decay instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets state of the stochasticity decay instance.&#34;&#34;&#34;
    self.stochasticity_decay_training.reset()</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement.StochasticPolicy.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, action_func, training)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent should take.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action_func</code></strong></dt>
<dd>A function that returns a list of values</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
Agent is in a training states</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, action_func, training):
    &#34;&#34;&#34;Returns the action the Agent should take.

    Args:
        action_func: A function that returns a list of values
        training: A boolean, which determines if the
                  Agent is in a training states
    &#34;&#34;&#34;
    if training:
        stochasticity = self.stochasticity_decay_training()
    else:
        stochasticity = self.stochasticity_testing
    if np.random.uniform() &lt; stochasticity:
        return np.random.randint(0, self.action_size)
    else:
        return self.policy.select_action(action_func, training)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement.TemporalNoisePolicy"><code class="flex name class">
<span>class <span class="ident">TemporalNoisePolicy</span></span>
<span>(</span><span>noise_scale_decay_training, noise_scale_testing, action_bounds, sigma=0.3, theta=0.15, dt=0.01, init_noise=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for adding temporal noise to an Agent's action.</p>
<p>Initalizes the Temporal Noise Policy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>noise_scale_decay_training</code></strong></dt>
<dd>A decay instance, which decays
the noise scale (a fraction of
action range) for the policy</dd>
<dt><strong><code>noise_scale_testing</code></strong></dt>
<dd>A float, which is the noise scale
of the policy when the agent is not
training</dd>
<dt><strong><code>action_bounds</code></strong></dt>
<dd>A tuple of two floats/integers, which are
the lower and upper bounds of the action
range</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TemporalNoisePolicy(NoisePolicy):
    &#34;&#34;&#34;This class is used for adding temporal noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds,
                 sigma=.3, theta=.15, dt=.01, init_noise=None):
        &#34;&#34;&#34;Initalizes the Temporal Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
        &#34;&#34;&#34;
        super().__init__(noise_scale_decay_training,
                         noise_scale_testing, action_bounds)
        self.sigma = sigma
        self.theta = theta
        self.dt = dt
        self.sqrt_dt = np.sqrt(self.dt)
        if init_noise is None:
            self.init_noise = None
        else:
            self.init_noise = init_noise
        self.last_noise = None

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        if self.init_noise is None:
            self.init_noise = np.full(actions.shape,
                                      np.mean(self.action_bounds))
            self.last_noise = self.init_noise
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        if noise_scale == 0:
            return actions
        noise = np.random.normal(scale=noise_scale, size=actions.shape)
        noise = (self.last_noise +
                 self.theta * -self.last_noise * self.dt +
                 self.sigma * self.sqrt_dt * noise)
        self.last_noise = noise
        return np.clip(actions + noise, *self.action_bounds)

    def reset(self):
        &#34;&#34;&#34;Resets decay state and initial actions.&#34;&#34;&#34;
        super().reset()
        self.last_noise = self.init_noise</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></li>
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement.TemporalNoisePolicy.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets decay state and initial actions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets decay state and initial actions.&#34;&#34;&#34;
    super().reset()
    self.last_noise = self.init_noise</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.NoisePolicy.end_episode" href="#paiutils.reinforcement.NoisePolicy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.NoisePolicy.select_action" href="#paiutils.reinforcement.Policy.select_action">select_action</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement.UniformNoisePolicy"><code class="flex name class">
<span>class <span class="ident">UniformNoisePolicy</span></span>
<span>(</span><span>noise_scale_decay_training, noise_scale_testing, action_bounds, additive=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for adding noise to an Agent's action.</p>
<p>Initalizes the Uniform Noise Policy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>noise_scale_decay_training</code></strong></dt>
<dd>A decay instance, which decays
the noise scale (a fraction of
action range) for the policy</dd>
<dt><strong><code>noise_scale_testing</code></strong></dt>
<dd>A float, which is the noise scale
of the policy when the agent is not
training</dd>
<dt><strong><code>action_bounds</code></strong></dt>
<dd>A tuple of two floats/integers, which are
the lower and upper bounds of the action
range</dd>
<dt><strong><code>additive</code></strong></dt>
<dd>A boolean, which determines if the noise should be
added or replace the action value completely</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UniformNoisePolicy(NoisePolicy):
    &#34;&#34;&#34;This class is used for adding noise to an Agent&#39;s action.&#34;&#34;&#34;

    def __init__(self, noise_scale_decay_training,
                 noise_scale_testing, action_bounds, additive=False):
        &#34;&#34;&#34;Initalizes the Uniform Noise Policy.

        Args:
            noise_scale_decay_training: A decay instance, which decays
                                        the noise scale (a fraction of
                                        action range) for the policy
            noise_scale_testing: A float, which is the noise scale
                                 of the policy when the agent is not
                                 training
            action_bounds: A tuple of two floats/integers, which are
                           the lower and upper bounds of the action
                           range
            additive: A boolean, which determines if the noise should be
                      added or replace the action value completely
        &#34;&#34;&#34;
        super().__init__(noise_scale_decay_training,
                         noise_scale_testing, action_bounds)
        self.additive = additive

    def select_action(self, action_func, training):
        &#34;&#34;&#34;Returns the action the Agent should take.

        Args:
            action_func: A function that returns a value
            training: A boolean, which determines if the
                      Agent is in a training states
        &#34;&#34;&#34;
        actions = np.asarray(action_func())
        noise = np.random.uniform(*self.action_bounds,
                                  size=actions.shape)
        if training:
            noise_scale = self.noise_scale_decay_training()
        else:
            noise_scale = self.noise_scale_testing
        if self.additive:
            return np.clip(actions + noise * noise_scale, *self.action_bounds)
        else:
            return np.where(
                np.random.uniform(size=actions.shape) &lt; noise_scale,
                noise, actions
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></li>
<li><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.NoisePolicy.end_episode" href="#paiutils.reinforcement.NoisePolicy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.NoisePolicy.reset" href="#paiutils.reinforcement.NoisePolicy.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.NoisePolicy.select_action" href="#paiutils.reinforcement.Policy.select_action">select_action</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="paiutils" href="index.html">paiutils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="paiutils.reinforcement.Agent" href="#paiutils.reinforcement.Agent">Agent</a></code></h4>
<ul class="two-column">
<li><code><a title="paiutils.reinforcement.Agent.add_memory" href="#paiutils.reinforcement.Agent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.end_episode" href="#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.forget" href="#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.learn" href="#paiutils.reinforcement.Agent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.load" href="#paiutils.reinforcement.Agent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.save" href="#paiutils.reinforcement.Agent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.select_action" href="#paiutils.reinforcement.Agent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.Agent.set_playing_data" href="#paiutils.reinforcement.Agent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.AsceticPolicy" href="#paiutils.reinforcement.AsceticPolicy">AsceticPolicy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.AsceticPolicy.select_action" href="#paiutils.reinforcement.AsceticPolicy.select_action">select_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.DDPGAgent" href="#paiutils.reinforcement.DDPGAgent">DDPGAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="paiutils.reinforcement.DDPGAgent.add_memory" href="#paiutils.reinforcement.DDPGAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.learn" href="#paiutils.reinforcement.DDPGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.load" href="#paiutils.reinforcement.DDPGAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.save" href="#paiutils.reinforcement.DDPGAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.select_action" href="#paiutils.reinforcement.DDPGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.set_playing_data" href="#paiutils.reinforcement.DDPGAgent.set_playing_data">set_playing_data</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.update_target" href="#paiutils.reinforcement.DDPGAgent.update_target">update_target</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.DQNAgent" href="#paiutils.reinforcement.DQNAgent">DQNAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.DQNAgent.add_memory" href="#paiutils.reinforcement.DQNAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.get_dueling_output_layer" href="#paiutils.reinforcement.DQNAgent.get_dueling_output_layer">get_dueling_output_layer</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.learn" href="#paiutils.reinforcement.DQNAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.load" href="#paiutils.reinforcement.DQNAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.save" href="#paiutils.reinforcement.DQNAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.select_action" href="#paiutils.reinforcement.DQNAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.set_playing_data" href="#paiutils.reinforcement.DQNAgent.set_playing_data">set_playing_data</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.update_target" href="#paiutils.reinforcement.DQNAgent.update_target">update_target</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.Decay" href="#paiutils.reinforcement.Decay">Decay</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.Decay.reset" href="#paiutils.reinforcement.Decay.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Decay.step" href="#paiutils.reinforcement.Decay.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.ETDMemory" href="#paiutils.reinforcement.ETDMemory">ETDMemory</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.ETDMemory.create_shuffled_subset" href="#paiutils.reinforcement.ETDMemory.create_shuffled_subset">create_shuffled_subset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.Environment" href="#paiutils.reinforcement.Environment">Environment</a></code></h4>
<ul class="two-column">
<li><code><a title="paiutils.reinforcement.Environment.close" href="#paiutils.reinforcement.Environment.close">close</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.play_episode" href="#paiutils.reinforcement.Environment.play_episode">play_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.play_episodes" href="#paiutils.reinforcement.Environment.play_episodes">play_episodes</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.render" href="#paiutils.reinforcement.Environment.render">render</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.reset" href="#paiutils.reinforcement.Environment.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Environment.step" href="#paiutils.reinforcement.Environment.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.ExponentialDecay" href="#paiutils.reinforcement.ExponentialDecay">ExponentialDecay</a></code></h4>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.GreedyPolicy" href="#paiutils.reinforcement.GreedyPolicy">GreedyPolicy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.GreedyPolicy.select_action" href="#paiutils.reinforcement.GreedyPolicy.select_action">select_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.GymWrapper" href="#paiutils.reinforcement.GymWrapper">GymWrapper</a></code></h4>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.LinearDecay" href="#paiutils.reinforcement.LinearDecay">LinearDecay</a></code></h4>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.Memory" href="#paiutils.reinforcement.Memory">Memory</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.Memory.add" href="#paiutils.reinforcement.Memory.add">add</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.array" href="#paiutils.reinforcement.Memory.array">array</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.create_shuffled_subset" href="#paiutils.reinforcement.Memory.create_shuffled_subset">create_shuffled_subset</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.end_episode" href="#paiutils.reinforcement.Memory.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.load" href="#paiutils.reinforcement.Memory.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.reset" href="#paiutils.reinforcement.Memory.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Memory.save" href="#paiutils.reinforcement.Memory.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.MemoryAgent" href="#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.MemoryAgent.load" href="#paiutils.reinforcement.MemoryAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.MemoryAgent.save" href="#paiutils.reinforcement.MemoryAgent.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.MultiSeqAgentEnvironment" href="#paiutils.reinforcement.MultiSeqAgentEnvironment">MultiSeqAgentEnvironment</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.MultiSeqAgentEnvironment.play_episode" href="#paiutils.reinforcement.MultiSeqAgentEnvironment.play_episode">play_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.MultiSeqAgentEnvironment.play_episodes" href="#paiutils.reinforcement.MultiSeqAgentEnvironment.play_episodes">play_episodes</a></code></li>
<li><code><a title="paiutils.reinforcement.MultiSeqAgentEnvironment.reset" href="#paiutils.reinforcement.MultiSeqAgentEnvironment.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.MultiSeqAgentEnvironment.step" href="#paiutils.reinforcement.MultiSeqAgentEnvironment.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.NoisePolicy" href="#paiutils.reinforcement.NoisePolicy">NoisePolicy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.NoisePolicy.end_episode" href="#paiutils.reinforcement.NoisePolicy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.NoisePolicy.reset" href="#paiutils.reinforcement.NoisePolicy.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.PGAgent" href="#paiutils.reinforcement.PGAgent">PGAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="paiutils.reinforcement.PGAgent.add_memory" href="#paiutils.reinforcement.PGAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.end_episode" href="#paiutils.reinforcement.PGAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.learn" href="#paiutils.reinforcement.PGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.load" href="#paiutils.reinforcement.PGAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.save" href="#paiutils.reinforcement.PGAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.select_action" href="#paiutils.reinforcement.PGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.set_playing_data" href="#paiutils.reinforcement.PGAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.PQAgent" href="#paiutils.reinforcement.PQAgent">PQAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.PQAgent.learn" href="#paiutils.reinforcement.PQAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.PQAgent.load" href="#paiutils.reinforcement.PQAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.PQAgent.save" href="#paiutils.reinforcement.PQAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.PQAgent.set_playing_data" href="#paiutils.reinforcement.PQAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.PlayingData" href="#paiutils.reinforcement.PlayingData">PlayingData</a></code></h4>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.Policy" href="#paiutils.reinforcement.Policy">Policy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.Policy.end_episode" href="#paiutils.reinforcement.Policy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.Policy.reset" href="#paiutils.reinforcement.Policy.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.Policy.select_action" href="#paiutils.reinforcement.Policy.select_action">select_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.QAgent" href="#paiutils.reinforcement.QAgent">QAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.QAgent.add_memory" href="#paiutils.reinforcement.QAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.learn" href="#paiutils.reinforcement.QAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.save" href="#paiutils.reinforcement.QAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.select_action" href="#paiutils.reinforcement.QAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.QAgent.set_playing_data" href="#paiutils.reinforcement.QAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.RingMemory" href="#paiutils.reinforcement.RingMemory">RingMemory</a></code></h4>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.StochasticPolicy" href="#paiutils.reinforcement.StochasticPolicy">StochasticPolicy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.StochasticPolicy.end_episode" href="#paiutils.reinforcement.StochasticPolicy.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.StochasticPolicy.reset" href="#paiutils.reinforcement.StochasticPolicy.reset">reset</a></code></li>
<li><code><a title="paiutils.reinforcement.StochasticPolicy.select_action" href="#paiutils.reinforcement.StochasticPolicy.select_action">select_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.TemporalNoisePolicy" href="#paiutils.reinforcement.TemporalNoisePolicy">TemporalNoisePolicy</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement.TemporalNoisePolicy.reset" href="#paiutils.reinforcement.TemporalNoisePolicy.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement.UniformNoisePolicy" href="#paiutils.reinforcement.UniformNoisePolicy">UniformNoisePolicy</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>