<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>paiutils.reinforcement_agents API documentation</title>
<meta name="description" content="Author: Travis Hammond
Version: 12_21_2020" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>paiutils.reinforcement_agents</code></h1>
</header>
<section id="section-intro">
<p>Author: Travis Hammond
Version: 12_21_2020</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Author: Travis Hammond
Version: 12_21_2020
&#34;&#34;&#34;


import os
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow import keras
from tensorflow.keras.models import model_from_json

from paiutils.reinforcement import (
    Memory, ETDMemory, PlayingData, Policy,
    DQNAgent, MemoryAgent, PGAgent, DDPGAgent
)


class DQNPGAgent(DQNAgent, PGAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Deep Q Network by default to
       select actions, but can be easily be changed to a policy gradient
       based network to predict actions.
    &#34;&#34;&#34;

    def __init__(self, policy, qmodel, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=True, enable_double=False,
                 enable_per=False):
        &#34;&#34;&#34;Initalizes the Deep Q Network and Policy Gradient Agent.

        Args:
            policy: A policy instance (for DQN Agent)
            qmodel: A keras model, which takes the state as input and outputs
                    Q Values
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not be applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used
            enable_double: A boolean, which determiens if the Double Deep Q
                           Network should be used
            enable_per: A boolean, which determines if prioritized experience
                        replay should be used
                        (The implementation for this is not the normal tree
                         implementation, and only weights the probabilily of
                         being choosen)
        &#34;&#34;&#34;
        DQNAgent.__init__(self, policy, qmodel, discounted_rate,
                          create_memory=create_memory,
                          enable_target=enable_target,
                          enable_double=enable_double,
                          enable_per=enable_per)
        self.amodel = amodel
        self.uses_dqn_method = True
        self.drewards = create_memory((None,),
                                      keras.backend.floatx())
        self.memory[&#39;drewards&#39;] = self.drewards
        self.episode_rewards = []
        self.pg_metric = keras.metrics.Mean(name=&#39;pg_loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def use_dqn(self):
        self.uses_dqn_method = True

    def use_pg(self):
        self.uses_dqn_method = False

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the Q values for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if self.uses_dqn_method:
            return DQNAgent.select_action(self, state, training=training)
        else:
            return PGAgent.select_action(self, state, training=training)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        DQNAgent.add_memory(self, state, action, new_state, reward, terminal)
        self.episode_rewards.append(reward)

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        DQNAgent.forget(self)
        self.episode_rewards.clear()

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode, and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        PGAgent.end_episode(self)

    def _train_step(self, states, next_states, actions,
                    terminals, rewards, drewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
        &#34;&#34;&#34;
        if self.enable_double:
            qvalues = self.qmodel(next_states, training=False)
            actions = tf.argmax(qvalues, axis=-1)
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.squeeze(tf.gather(qvalues, actions[:, tf.newaxis],
                                           axis=-1, batch_dims=1))
            actions = tf.one_hot(actions, self.action_size,
                                 dtype=qvalues.dtype)
        else:
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.reduce_max(qvalues, axis=-1)
        qvalues = (rewards +
                   self.discounted_rate * qvalues * terminals)
        with tf.GradientTape() as tape:
            y_pred = self.qmodel(states, training=True)
            if len(self.qmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.qmodel.losses)
            else:
                reg_loss = 0
            y_true = (y_pred * (1 - actions) +
                      qvalues[:, tf.newaxis] * actions)
            loss = self.qmodel.compiled_loss._losses[0].fn(
                y_true, y_pred
            ) + reg_loss
        grads = tape.gradient(loss, self.qmodel.trainable_variables)
        self.qmodel.optimizer.apply_gradients(
            zip(grads, self.qmodel.trainable_variables)
        )
        self.metric(loss)

        abs_loss = tf.reduce_sum(tf.abs(y_true - y_pred), axis=-1)

        with tf.GradientTape() as tape:
            y_pred = self.amodel(states, training=True)
            # log_softmax may be more stable, but in practice
            # seems to give worse results
            log_probs = tf.reduce_sum(
                actions *
                tf.math.log(y_pred + keras.backend.epsilon()), axis=1
            )
            loss = -tf.reduce_mean(drewards * log_probs)
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.pg_metric(loss)

        return abs_loss

    def _train(self, states, next_states, actions, terminals,
               rewards, drewards, epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A list of floats, which are the absolute losses for all
                the data
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type),
             drewards.astype(float_type))
        ).batch(batch_size)
        losses = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                if epoch == epochs:
                    losses.append(self._tf_train_step(*batch).numpy())
                else:
                    self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            dqn_loss_results = self.metric.result()
            self.metric.reset_states()
            pg_loss_results = self.pg_metric.result()
            self.pg_metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;dqn_loss: {dqn_loss_results} - &#39;
                      f&#39;pg_loss: {pg_loss_results}&#39;)
        return np.hstack(losses)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target qmodel weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.per_losses is None:
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards, self.drewards],
                    length
                )
            else:
                per_losses = self.per_losses.array()
                self.max_loss = per_losses.max()
                per_losses = per_losses / per_losses.sum()
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards, self.drewards],
                    length, weights=per_losses
                )

            std = arrays[-1].std()
            if std == 0:
                return False
            arrays[-1] = (arrays[-1] - arrays[-1].mean()) / std
            losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
            if self.per_losses is not None:
                for ndx, loss in zip(indexes, losses):
                    self.per_losses[ndx] = loss

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True, custom_objects=None):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded
            custom_objects: A dictionary mapping to custom classes
                            or functions for loading the model

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = DQNAgent.load(
            self, path, load_model=load_model, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(
                    file.read(), custom_objects=custom_objects
                )
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;DQNPGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = DQNAgent.save(self, path, save_model=save_model,
                             save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return path


class A2CAgent(PGAgent):
    &#34;&#34;&#34;This class (Advantage Actor-Critic) is like the PGAgent, but it also
       has a critic network which is used to estimate the value function
       in order to train the Actor network on the advantages instead of
       the discounted rewards.
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        PGAgent.__init__(self, amodel, discounted_rate,
                         create_memory=create_memory)
        self.cmodel = cmodel
        self.cmodel.compiled_loss.build(
            tf.zeros(self.cmodel.output_shape[1:])
        )
        self.lambda_rate = lambda_rate
        if lambda_rate != 0:
            self.terminals = create_memory((None,),
                                           keras.backend.floatx())
            self.rewards = create_memory((None,),
                                         keras.backend.floatx())
            self.memory[&#39;terminals&#39;] = self.terminals
            self.memory[&#39;rewards&#39;] = self.rewards
        self.metric_c = keras.metrics.Mean(name=&#39;critic_loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(self.action_identity[action])
        self.episode_rewards.append(reward)
        if self.lambda_rate &gt; 0:
            self.terminals.add(terminal)
            self.rewards.add(reward)

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode, and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        if len(self.episode_rewards) &gt; 0:
            dreward = 0
            dreward_list = []
            for reward in reversed(self.episode_rewards):
                dreward *= self.discounted_rate
                dreward += reward
                dreward_list.append(dreward)
            self.episode_rewards.clear()
            for dreward in reversed(dreward_list):
                self.drewards.add(dreward)
            if self.lambda_rate &gt; 0:
                self.terminals[-1] = True

        MemoryAgent.end_episode(self)

    def _train_step(self, states, drewards, advantages,
                    actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            action_pred = self.amodel(states, training=True)
            log_action_pred = tf.math.log(
                action_pred + keras.backend.epsilon()
            )
            log_probs = tf.reduce_sum(
                actions * log_action_pred, axis=1
            )
            if self.lambda_rate == 0:
                advantages = (drewards - value_pred)
            loss = -tf.reduce_mean(advantages * log_probs)
            entropy = tf.reduce_sum(
                action_pred * log_action_pred, axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)

    def _train(self, states, drewards, advantages, actions,
               epochs, batch_size, entropy_coef, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted rewards
                      for the actions performed in the environment
            advantages: A numpy array, which if valid (lambda_rate &gt; 0)
                        contains advantages for the actions performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             advantages.astype(float_type),
             actions.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            actor_loss_results = self.metric.result()
            critic_loss_results = self.metric_c.result()
            self.metric.reset_states()
            self.metric_c.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.lambda_rate == 0:
                advantages_arr = np.empty(length)
            else:
                # cmodel predict on batches if large?
                values = tf.squeeze(
                    self.cmodel(self.states.array())
                ).numpy()
                advantages = np.empty(len(self.rewards))
                for ndx in reversed(range(len(self.rewards))):
                    delta = self.rewards[ndx] - values[ndx]
                    if not self.terminals[ndx]:
                        delta += self.discounted_rate * values[ndx + 1]
                    if self.terminals[ndx]:
                        advantage = 0
                    advantage = (delta + self.discounted_rate *
                                 self.lambda_rate * advantage)
                    advantages[ndx] = advantage

            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions], length
            )

            if self.lambda_rate == 0:
                arrays = [arrays[0], arrays[1], advantages_arr, arrays[2]]
                std = arrays[1].std()
                if std == 0:
                    return False
                arrays[1] = (arrays[1] - arrays[1].mean()) / std
            else:
                arrays = [arrays[0], arrays[1], advantages[indexes], arrays[2]]
                std = arrays[2].std()
                if std == 0:
                    return False
                arrays[2] = (arrays[2] - arrays[2].mean()) / std

            self._train(*arrays, epochs, batch_size,
                        entropy_coef, verbose=verbose)

    def load(self, path, load_model=True, load_data=True, custom_objects=None):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded
            custom_objects: A dictionary mapping to custom classes
                            or functions for loading the model

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = PGAgent.load(
            self, path, load_model=load_model, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
                self.cmodel = model_from_json(
                    file.read(), custom_objects=custom_objects
                )
            self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;A2CAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, models, weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = PGAgent.save(self, path, save_model=save_model,
                            save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.cmodel.to_json())
            self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return path


class PPOAgent(A2CAgent):
    &#34;&#34;&#34;This class (Proximal Policy Optimization) is like the A2CAgent
       but attempts to avoid taking large gradient steps that would
       collapse the performacne of the agent. (this is the clip variant)
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, clip_ratio=.2,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            clip_ratio: A float, which is the ratio to clip the differences
                        between new and old action probabilities
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        A2CAgent.__init__(self, amodel, cmodel, discounted_rate,
                          lambda_rate=lambda_rate,
                          create_memory=create_memory)
        self.clip_ratio = clip_ratio
        self.old_probs = create_memory((None,),
                                       keras.backend.floatx())
        self.memory[&#39;old_probs&#39;] = self.old_probs
        self.prob = None
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the action for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        actions = self.amodel(np.expand_dims(state, axis=0),
                              training=False)[0].numpy()
        action = np.random.choice(np.arange(self.action_size),
                                  p=actions)
        self.prob = actions[action]
        return action

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        A2CAgent.add_memory(self, state, action, new_state, reward, terminal)
        if self.prob is None:
            # actions = self.amodel(np.expand_dims(state, axis=0),
            #                       training=False)[0].numpy()
            # prob = actions[action]
            # self.old_probs.add(prob)

            # Assuming a uniform distribution
            self.old_probs.add(1 / self.action_size)
        else:
            self.old_probs.add(self.prob)
            self.prob = None

    def _train_step(self, states, drewards, advantages, actions,
                    old_probs, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            old_probs: A tensor of the old probs
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss

        Returns:
            A tensor of the new probs
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            action_pred = self.amodel(states, training=True)
            probs = tf.reduce_sum(actions * action_pred, axis=1)
            ratio = probs / (old_probs + keras.backend.epsilon())
            clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_ratio,
                                             1.0 + self.clip_ratio)
            if self.lambda_rate == 0:
                advantages = drewards - value_pred
            loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            entropy = tf.reduce_sum(
                action_pred *
                tf.math.log(action_pred + keras.backend.epsilon()),
                axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )

        self.metric(loss)
        return probs

    def _train(self, states, drewards, advantages, actions,
               old_probs, epochs, batch_size, entropy_coef,
               verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            advantages: A numpy array, which if valid (lambda_rate &gt; 0)
                        contains advantages for the actions performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            old_probs: A numpy array of the old probs
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A tuple of a float (mean critic loss of the batches) and
                a numpy ndarray of probs
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             advantages.astype(float_type),
             actions.astype(float_type),
             old_probs.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        new_probs = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                if epoch == epochs:
                    new_probs.append(self._tf_train_step(*batch,
                                                         entropy_coef))
                else:
                    self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            actor_loss_results = self.metric.result()
            critic_loss_results = self.metric_c.result()
            self.metric.reset_states()
            self.metric_c.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results, np.hstack(new_probs)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.lambda_rate == 0:
                advantages_arr = np.empty(length)
            else:
                # cmodel predict on batches if large?
                values = tf.squeeze(
                    self.cmodel(self.states.array())
                ).numpy()
                advantages = np.empty(len(self.rewards))
                for ndx in reversed(range(len(self.rewards))):
                    delta = self.rewards[ndx] - values[ndx]
                    if not self.terminals[ndx]:
                        delta += self.discounted_rate * values[ndx + 1]
                    if self.terminals[ndx]:
                        advantage = 0
                    advantage = (delta + self.discounted_rate *
                                 self.lambda_rate * advantage)
                    advantages[ndx] = advantage

            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions, self.old_probs],
                length
            )

            if self.lambda_rate == 0:
                arrays = [arrays[0], arrays[1], advantages_arr,
                          arrays[2], arrays[3]]
                std = arrays[1].std()
                if std == 0:
                    return False
                arrays[1] = (arrays[1] - arrays[1].mean()) / std
            else:
                arrays = [arrays[0], arrays[1], advantages[indexes],
                          arrays[2], arrays[3]]
                std = arrays[2].std()
                if std == 0:
                    return False
                arrays[2] = (arrays[2] - arrays[2].mean()) / std

            loss, new_probs = self._train(*arrays, epochs, batch_size,
                                          entropy_coef, verbose=verbose)
            for ndx in range(length):
                self.old_probs[indexes[ndx]] = new_probs[ndx]

    def save(self, path, save_model=True,
             save_data=True, note=&#39;PPOAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, models, weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = A2CAgent.save(self, path, save_model=save_model,
                             save_data=save_data, note=note)
        return path


class TD3Agent(DDPGAgent):
    &#34;&#34;&#34;This class (Twin Delayed DDPG Agent) attempts to mitigate
       the problems that a DDPGAgent faces through clipping Q targets
       between two Q models, delaying policy updates, and adding noise
       to target actions.
    &#34;&#34;&#34;

    def __init__(self, policy, amodel, cmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the DDPG Agent.

        Args:
            policy: A noise policy instance, which used for exploring
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state and a action as input
                     and outputs two seperate Q Values (a judgement)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        DDPGAgent.__init__(self, policy, amodel, cmodel, discounted_rate,
                           create_memory=create_memory,
                           enable_target=True)
        coutput_shape = self.cmodel.output_shape
        if not isinstance(coutput_shape, list) or len(coutput_shape) != 2:
            raise ValueError(&#39;cmodel should have two outputs&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.output_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(), dtype=tf.int32))
        )
        self.gradient_step_count = 0

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0,
                         policy_noise_std=.2, policy_noise_clip=.5,
                         actor_update_infreq=2,
                         verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                               the actor is updated compared to the critic
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;policy_noise_std&#39;: policy_noise_std,
                           &#39;policy_noise_clip&#39;: policy_noise_clip,
                           &#39;actor_update_infreq&#39;: actor_update_infreq,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def _train_step(self, states, next_states, actions, terminals,
                    rewards, policy_noise_std, policy_noise_clip,
                    actor_update):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains the actions performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
            policy_noise_std: A tensor constant float, which is the
                              standard deviation of the noise to add
                              to the target actions for gradient steps
            policy_noise_clip: A tensor constant float, which is the
                               min and max value of the normal noise
                               added to target actions for gradient steps
            actor_update: A tensor constant integer, which is determines
                          if the actor should update
        &#34;&#34;&#34;
        next_actions = self.target_amodel(next_states, training=False)
        noise = tf.random.normal(tf.shape(next_actions),
                                 stddev=policy_noise_std)
        noise = tf.clip_by_value(noise, -policy_noise_clip, policy_noise_clip)
        next_actions = tf.clip_by_value(next_actions + noise,
                                        *self.policy.action_bounds)
        next_qvalues1, next_qvalues2 = self.target_cmodel(
            [next_states, next_actions], training=False
        )
        next_qvalues = tf.squeeze(tf.minimum(next_qvalues1, next_qvalues2))
        qvalues_true = (rewards +
                        self.discounted_rate * next_qvalues * terminals)
        # Critic
        with tf.GradientTape() as tape:
            qvalues_pred1, qvalues_pred2 = self.cmodel(
                [states, actions], training=True
            )
            qvalues_pred1 = tf.squeeze(qvalues_pred1)
            qvalues_pred2 = tf.squeeze(qvalues_pred2)
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss1 = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred1
            )
            loss2 = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred2
            )
            loss = tf.reduce_mean(loss1) + tf.reduce_mean(loss2) + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        # Actor
        if actor_update == 0:
            with tf.GradientTape() as tape:
                action_preds = self.amodel(states, training=True)
                loss = -tf.reduce_mean(
                    self.cmodel([states, action_preds], training=False)
                )
            grads = tape.gradient(loss, self.amodel.trainable_variables)
            self.amodel.optimizer.apply_gradients(
                zip(grads, self.amodel.trainable_variables)
            )
            self.metric_a(loss)

    def _train(self, states, next_states, actions, terminals, rewards,
               epochs, batch_size, policy_noise_std, policy_noise_clip,
               actor_update_infreq, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains the actions performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                               the actor is updated compared to the critic
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        policy_noise_std = tf.constant(policy_noise_std,
                                       dtype=float_type)
        policy_noise_clip = tf.constant(policy_noise_clip,
                                        dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            self.gradient_step_count += 1
            count = 0
            actor_update = self.gradient_step_count % actor_update_infreq
            actor_update = tf.constant(actor_update, dtype=tf.int32)
            for batch in batches:
                self._tf_train_step(*batch, policy_noise_std,
                                    policy_noise_clip,
                                    actor_update)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            critic_loss_results = self.metric_c.result()
            actor_loss_results = self.metric_a.result()
            self.metric_c.reset_states()
            if actor_update == 1:
                self.metric_a.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;critic_loss: {critic_loss_results} - &#39;
                      f&#39;actor_loss: {actor_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, target_update_interval=1,
              tau=1.0, policy_noise_std=.2, policy_noise_clip=.5,
              actor_update_infreq=2, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and predictions are
                    repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                                 the actor is updated compared to the critic
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length
            )

            self._train(*arrays, epochs, batch_size,
                        policy_noise_std, policy_noise_clip,
                        actor_update_infreq, verbose=verbose)

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def save(self, path, save_model=True, save_data=True,
             note=&#39;T3DAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, weights of the models, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        return DDPGAgent.save(self, path, save_model=save_model,
                              save_data=save_data, note=note)


class Continuous:
    &#34;&#34;&#34;This interface is used for the continuous action space
       variants of algorithms.
    &#34;&#34;&#34;

    @staticmethod
    def scale(lower_bound, upper_bound, name=None):
        def _scale(x):
            x = tf.multiply(x, upper_bound - lower_bound)
            x = x + upper_bound + lower_bound
            return tf.multiply(.5, x)
        return _scale

    @staticmethod
    def sample(name=None):
        def _sample(x):
            mean, stddev = x
            eps = tf.random.normal(tf.shape(mean))
            action = eps * stddev + mean
            return action
        return _sample

    @staticmethod
    def clip(lower_bound, upper_bound, name=None):
        def _clip(x):
            return tf.clip_by_value(x, lower_bound, upper_bound)
        return _clip


class PGCAgent(PGAgent, Continuous):
    &#34;&#34;&#34;This class is a continuous action space variant of the PGAgent.
    &#34;&#34;&#34;

    def __init__(self, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        if len(amodel.output_shape) != 3:
            raise ValueError(&#39;The model must have three outputs: &#39;
                             &#39;mean, stddev, and actions&#39;)
        PGAgent.__init__(self, amodel, discounted_rate,
                         create_memory=create_memory)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the action
                    the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(action)
        self.episode_rewards.append(reward)

    def _train_step(self, states, drewards, actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            locs, scales, _ = self.amodel(states, training=True)
            normal = tfp.distributions.Normal(locs, scales)
            probs = normal.prob(actions)
            log_probs = tf.math.log(probs + keras.backend.epsilon())
            loss = -tf.reduce_mean(drewards * log_probs)
            entropy = tf.reduce_mean(probs * log_probs)
            loss += entropy * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)


class A2CCAgent(A2CAgent, Continuous):
    &#34;&#34;&#34;This class is a continuous action space variant of the A2CAgent.
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        if len(amodel.output_shape) != 3:
            raise ValueError(&#39;The model must have three outputs: &#39;
                             &#39;mean, stddev, and actions&#39;)
        A2CAgent.__init__(self, amodel, cmodel, discounted_rate,
                          lambda_rate=lambda_rate,
                          create_memory=create_memory)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the action
                    the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(action)
        self.episode_rewards.append(reward)
        if self.lambda_rate &gt; 0:
            self.terminals.add(terminal)
            self.rewards.add(reward)

    def _train_step(self, states, drewards, advantages,
                    actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            locs, scales, _ = self.amodel(states, training=True)
            normal = tfp.distributions.Normal(locs, scales)
            probs = normal.prob(actions)
            log_probs = tf.math.log(probs + keras.backend.epsilon())
            if self.lambda_rate == 0:
                advantages = (drewards - value_pred)
            loss = -tf.reduce_mean(advantages * log_probs)
            entropy = tf.reduce_mean(probs * log_probs)
            loss += entropy * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="paiutils.reinforcement_agents.A2CAgent"><code class="flex name class">
<span>class <span class="ident">A2CAgent</span></span>
<span>(</span><span>amodel, cmodel, discounted_rate, lambda_rate=0, create_memory=&lt;function A2CAgent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class (Advantage Actor-Critic) is like the PGAgent, but it also
has a critic network which is used to estimate the value function
in order to train the Actor network on the advantages instead of
the discounted rewards.</p>
<p>Initalizes the Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>cmodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
the value of that state</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>lambda_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which if nonzero will enable
generalized advantage estimation</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class A2CAgent(PGAgent):
    &#34;&#34;&#34;This class (Advantage Actor-Critic) is like the PGAgent, but it also
       has a critic network which is used to estimate the value function
       in order to train the Actor network on the advantages instead of
       the discounted rewards.
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        PGAgent.__init__(self, amodel, discounted_rate,
                         create_memory=create_memory)
        self.cmodel = cmodel
        self.cmodel.compiled_loss.build(
            tf.zeros(self.cmodel.output_shape[1:])
        )
        self.lambda_rate = lambda_rate
        if lambda_rate != 0:
            self.terminals = create_memory((None,),
                                           keras.backend.floatx())
            self.rewards = create_memory((None,),
                                         keras.backend.floatx())
            self.memory[&#39;terminals&#39;] = self.terminals
            self.memory[&#39;rewards&#39;] = self.rewards
        self.metric_c = keras.metrics.Mean(name=&#39;critic_loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(self.action_identity[action])
        self.episode_rewards.append(reward)
        if self.lambda_rate &gt; 0:
            self.terminals.add(terminal)
            self.rewards.add(reward)

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode, and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        if len(self.episode_rewards) &gt; 0:
            dreward = 0
            dreward_list = []
            for reward in reversed(self.episode_rewards):
                dreward *= self.discounted_rate
                dreward += reward
                dreward_list.append(dreward)
            self.episode_rewards.clear()
            for dreward in reversed(dreward_list):
                self.drewards.add(dreward)
            if self.lambda_rate &gt; 0:
                self.terminals[-1] = True

        MemoryAgent.end_episode(self)

    def _train_step(self, states, drewards, advantages,
                    actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            action_pred = self.amodel(states, training=True)
            log_action_pred = tf.math.log(
                action_pred + keras.backend.epsilon()
            )
            log_probs = tf.reduce_sum(
                actions * log_action_pred, axis=1
            )
            if self.lambda_rate == 0:
                advantages = (drewards - value_pred)
            loss = -tf.reduce_mean(advantages * log_probs)
            entropy = tf.reduce_sum(
                action_pred * log_action_pred, axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)

    def _train(self, states, drewards, advantages, actions,
               epochs, batch_size, entropy_coef, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted rewards
                      for the actions performed in the environment
            advantages: A numpy array, which if valid (lambda_rate &gt; 0)
                        contains advantages for the actions performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             advantages.astype(float_type),
             actions.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            actor_loss_results = self.metric.result()
            critic_loss_results = self.metric_c.result()
            self.metric.reset_states()
            self.metric_c.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.lambda_rate == 0:
                advantages_arr = np.empty(length)
            else:
                # cmodel predict on batches if large?
                values = tf.squeeze(
                    self.cmodel(self.states.array())
                ).numpy()
                advantages = np.empty(len(self.rewards))
                for ndx in reversed(range(len(self.rewards))):
                    delta = self.rewards[ndx] - values[ndx]
                    if not self.terminals[ndx]:
                        delta += self.discounted_rate * values[ndx + 1]
                    if self.terminals[ndx]:
                        advantage = 0
                    advantage = (delta + self.discounted_rate *
                                 self.lambda_rate * advantage)
                    advantages[ndx] = advantage

            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions], length
            )

            if self.lambda_rate == 0:
                arrays = [arrays[0], arrays[1], advantages_arr, arrays[2]]
                std = arrays[1].std()
                if std == 0:
                    return False
                arrays[1] = (arrays[1] - arrays[1].mean()) / std
            else:
                arrays = [arrays[0], arrays[1], advantages[indexes], arrays[2]]
                std = arrays[2].std()
                if std == 0:
                    return False
                arrays[2] = (arrays[2] - arrays[2].mean()) / std

            self._train(*arrays, epochs, batch_size,
                        entropy_coef, verbose=verbose)

    def load(self, path, load_model=True, load_data=True, custom_objects=None):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded
            custom_objects: A dictionary mapping to custom classes
                            or functions for loading the model

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = PGAgent.load(
            self, path, load_model=load_model, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
                self.cmodel = model_from_json(
                    file.read(), custom_objects=custom_objects
                )
            self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;A2CAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, models, weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = PGAgent.save(self, path, save_model=save_model,
                            save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.cmodel.to_json())
            self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.A2CCAgent" href="#paiutils.reinforcement_agents.A2CCAgent">A2CCAgent</a></li>
<li><a title="paiutils.reinforcement_agents.PPOAgent" href="#paiutils.reinforcement_agents.PPOAgent">PPOAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.A2CAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float/integer, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float/integer, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.actions.add(self.action_identity[action])
    self.episode_rewards.append(reward)
    if self.lambda_rate &gt; 0:
        self.terminals.add(terminal)
        self.rewards.add(reward)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.A2CAgent.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Ends the episode, and creates drewards based
on the episodes rewards.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Ends the episode, and creates drewards based
       on the episodes rewards.
    &#34;&#34;&#34;
    if len(self.episode_rewards) &gt; 0:
        dreward = 0
        dreward_list = []
        for reward in reversed(self.episode_rewards):
            dreward *= self.discounted_rate
            dreward += reward
            dreward_list.append(dreward)
        self.episode_rewards.clear()
        for dreward in reversed(dreward_list):
            self.drewards.add(dreward)
        if self.lambda_rate &gt; 0:
            self.terminals[-1] = True

    MemoryAgent.end_episode(self)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.A2CAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_model=True, load_data=True, custom_objects=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be loaded</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
<dt><strong><code>custom_objects</code></strong></dt>
<dd>A dictionary mapping to custom classes
or functions for loading the model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_model=True, load_data=True, custom_objects=None):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_model: A boolean, which determines if the model
                    architectures and weights
                    should be loaded
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded
        custom_objects: A dictionary mapping to custom classes
                        or functions for loading the model

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = PGAgent.load(
        self, path, load_model=load_model, load_data=load_data)
    if load_model:
        with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;r&#39;) as file:
            self.cmodel = model_from_json(
                file.read(), custom_objects=custom_objects
            )
        self.cmodel.load_weights(os.path.join(path, &#39;cweights.h5&#39;))
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.A2CAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_model=True, save_data=True, note='A2CAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note, models, weights, and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be saved</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_model=True,
         save_data=True, note=&#39;A2CAgent Save&#39;):
    &#34;&#34;&#34;Saves a note, models, weights, and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_model: A boolean, which determines if the model
                    architectures and weights
                    should be saved
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = PGAgent.save(self, path, save_model=save_model,
                        save_data=save_data, note=note)
    if save_model:
        with open(os.path.join(path, &#39;cmodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.cmodel.to_json())
        self.cmodel.save_weights(os.path.join(path, &#39;cweights.h5&#39;))
    return path</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.PGAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.learn" href="reinforcement.html#paiutils.reinforcement.PGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.select_action" href="reinforcement.html#paiutils.reinforcement.PGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.set_playing_data" href="reinforcement.html#paiutils.reinforcement.PGAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement_agents.A2CCAgent"><code class="flex name class">
<span>class <span class="ident">A2CCAgent</span></span>
<span>(</span><span>amodel, cmodel, discounted_rate, lambda_rate=0, create_memory=&lt;function A2CCAgent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is a continuous action space variant of the A2CAgent.</p>
<p>Initalizes the Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>cmodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
the value of that state</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>lambda_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which if nonzero will enable
generalized advantage estimation</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class A2CCAgent(A2CAgent, Continuous):
    &#34;&#34;&#34;This class is a continuous action space variant of the A2CAgent.
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        if len(amodel.output_shape) != 3:
            raise ValueError(&#39;The model must have three outputs: &#39;
                             &#39;mean, stddev, and actions&#39;)
        A2CAgent.__init__(self, amodel, cmodel, discounted_rate,
                          lambda_rate=lambda_rate,
                          create_memory=create_memory)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the action
                    the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(action)
        self.episode_rewards.append(reward)
        if self.lambda_rate &gt; 0:
            self.terminals.add(terminal)
            self.rewards.add(reward)

    def _train_step(self, states, drewards, advantages,
                    actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            locs, scales, _ = self.amodel(states, training=True)
            normal = tfp.distributions.Normal(locs, scales)
            probs = normal.prob(actions)
            log_probs = tf.math.log(probs + keras.backend.epsilon())
            if self.lambda_rate == 0:
                advantages = (drewards - value_pred)
            loss = -tf.reduce_mean(advantages * log_probs)
            entropy = tf.reduce_mean(probs * log_probs)
            loss += entropy * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.A2CAgent" href="#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></li>
<li><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
<li><a title="paiutils.reinforcement_agents.Continuous" href="#paiutils.reinforcement_agents.Continuous">Continuous</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.A2CCAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value or list of values, which is the action
the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float/integer, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value or list of values, which is the action
                the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float/integer, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.actions.add(action)
    self.episode_rewards.append(reward)
    if self.lambda_rate &gt; 0:
        self.terminals.add(terminal)
        self.rewards.add(reward)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement_agents.A2CAgent" href="#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.end_episode" href="#paiutils.reinforcement_agents.A2CAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.learn" href="reinforcement.html#paiutils.reinforcement.PGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.load" href="#paiutils.reinforcement_agents.A2CAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.save" href="#paiutils.reinforcement_agents.A2CAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.select_action" href="reinforcement.html#paiutils.reinforcement.PGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.set_playing_data" href="reinforcement.html#paiutils.reinforcement.PGAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement_agents.Continuous"><code class="flex name class">
<span>class <span class="ident">Continuous</span></span>
</code></dt>
<dd>
<div class="desc"><p>This interface is used for the continuous action space
variants of algorithms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Continuous:
    &#34;&#34;&#34;This interface is used for the continuous action space
       variants of algorithms.
    &#34;&#34;&#34;

    @staticmethod
    def scale(lower_bound, upper_bound, name=None):
        def _scale(x):
            x = tf.multiply(x, upper_bound - lower_bound)
            x = x + upper_bound + lower_bound
            return tf.multiply(.5, x)
        return _scale

    @staticmethod
    def sample(name=None):
        def _sample(x):
            mean, stddev = x
            eps = tf.random.normal(tf.shape(mean))
            action = eps * stddev + mean
            return action
        return _sample

    @staticmethod
    def clip(lower_bound, upper_bound, name=None):
        def _clip(x):
            return tf.clip_by_value(x, lower_bound, upper_bound)
        return _clip</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.A2CCAgent" href="#paiutils.reinforcement_agents.A2CCAgent">A2CCAgent</a></li>
<li><a title="paiutils.reinforcement_agents.PGCAgent" href="#paiutils.reinforcement_agents.PGCAgent">PGCAgent</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.Continuous.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>lower_bound, upper_bound, name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clip(lower_bound, upper_bound, name=None):
    def _clip(x):
        return tf.clip_by_value(x, lower_bound, upper_bound)
    return _clip</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.Continuous.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def sample(name=None):
    def _sample(x):
        mean, stddev = x
        eps = tf.random.normal(tf.shape(mean))
        action = eps * stddev + mean
        return action
    return _sample</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.Continuous.scale"><code class="name flex">
<span>def <span class="ident">scale</span></span>(<span>lower_bound, upper_bound, name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def scale(lower_bound, upper_bound, name=None):
    def _scale(x):
        x = tf.multiply(x, upper_bound - lower_bound)
        x = x + upper_bound + lower_bound
        return tf.multiply(.5, x)
    return _scale</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent"><code class="flex name class">
<span>class <span class="ident">DQNPGAgent</span></span>
<span>(</span><span>policy, qmodel, amodel, discounted_rate, create_memory=&lt;function DQNPGAgent.&lt;lambda&gt;&gt;, enable_target=True, enable_double=False, enable_per=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is an Agent that uses a Deep Q Network by default to
select actions, but can be easily be changed to a policy gradient
based network to predict actions.</p>
<p>Initalizes the Deep Q Network and Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>policy</code></strong></dt>
<dd>A policy instance (for DQN Agent)</dd>
<dt><strong><code>qmodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
Q Values</dd>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not be applied,
and compiled loss are not used)</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
<dt><strong><code>enable_target</code></strong></dt>
<dd>A boolean, which determines if a target model
should be used</dd>
<dt><strong><code>enable_double</code></strong></dt>
<dd>A boolean, which determiens if the Double Deep Q
Network should be used</dd>
<dt><strong><code>enable_per</code></strong></dt>
<dd>A boolean, which determines if prioritized experience
replay should be used
(The implementation for this is not the normal tree
implementation, and only weights the probabilily of
being choosen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQNPGAgent(DQNAgent, PGAgent):
    &#34;&#34;&#34;This class is an Agent that uses a Deep Q Network by default to
       select actions, but can be easily be changed to a policy gradient
       based network to predict actions.
    &#34;&#34;&#34;

    def __init__(self, policy, qmodel, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory(),
                 enable_target=True, enable_double=False,
                 enable_per=False):
        &#34;&#34;&#34;Initalizes the Deep Q Network and Policy Gradient Agent.

        Args:
            policy: A policy instance (for DQN Agent)
            qmodel: A keras model, which takes the state as input and outputs
                    Q Values
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not be applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
            enable_target: A boolean, which determines if a target model
                           should be used
            enable_double: A boolean, which determiens if the Double Deep Q
                           Network should be used
            enable_per: A boolean, which determines if prioritized experience
                        replay should be used
                        (The implementation for this is not the normal tree
                         implementation, and only weights the probabilily of
                         being choosen)
        &#34;&#34;&#34;
        DQNAgent.__init__(self, policy, qmodel, discounted_rate,
                          create_memory=create_memory,
                          enable_target=enable_target,
                          enable_double=enable_double,
                          enable_per=enable_per)
        self.amodel = amodel
        self.uses_dqn_method = True
        self.drewards = create_memory((None,),
                                      keras.backend.floatx())
        self.memory[&#39;drewards&#39;] = self.drewards
        self.episode_rewards = []
        self.pg_metric = keras.metrics.Mean(name=&#39;pg_loss&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.qmodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()))
        )

    def use_dqn(self):
        self.uses_dqn_method = True

    def use_pg(self):
        self.uses_dqn_method = False

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the Q values for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if self.uses_dqn_method:
            return DQNAgent.select_action(self, state, training=training)
        else:
            return PGAgent.select_action(self, state, training=training)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
        &#34;&#34;&#34;
        DQNAgent.add_memory(self, state, action, new_state, reward, terminal)
        self.episode_rewards.append(reward)

    def forget(self):
        &#34;&#34;&#34;Forgets or clears all memory.&#34;&#34;&#34;
        DQNAgent.forget(self)
        self.episode_rewards.clear()

    def end_episode(self):
        &#34;&#34;&#34;Ends the episode, and creates drewards based
           on the episodes rewards.
        &#34;&#34;&#34;
        PGAgent.end_episode(self)

    def _train_step(self, states, next_states, actions,
                    terminals, rewards, drewards):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
        &#34;&#34;&#34;
        if self.enable_double:
            qvalues = self.qmodel(next_states, training=False)
            actions = tf.argmax(qvalues, axis=-1)
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.squeeze(tf.gather(qvalues, actions[:, tf.newaxis],
                                           axis=-1, batch_dims=1))
            actions = tf.one_hot(actions, self.action_size,
                                 dtype=qvalues.dtype)
        else:
            qvalues = self.target_qmodel(next_states, training=False)
            qvalues = tf.reduce_max(qvalues, axis=-1)
        qvalues = (rewards +
                   self.discounted_rate * qvalues * terminals)
        with tf.GradientTape() as tape:
            y_pred = self.qmodel(states, training=True)
            if len(self.qmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.qmodel.losses)
            else:
                reg_loss = 0
            y_true = (y_pred * (1 - actions) +
                      qvalues[:, tf.newaxis] * actions)
            loss = self.qmodel.compiled_loss._losses[0].fn(
                y_true, y_pred
            ) + reg_loss
        grads = tape.gradient(loss, self.qmodel.trainable_variables)
        self.qmodel.optimizer.apply_gradients(
            zip(grads, self.qmodel.trainable_variables)
        )
        self.metric(loss)

        abs_loss = tf.reduce_sum(tf.abs(y_true - y_pred), axis=-1)

        with tf.GradientTape() as tape:
            y_pred = self.amodel(states, training=True)
            # log_softmax may be more stable, but in practice
            # seems to give worse results
            log_probs = tf.reduce_sum(
                actions *
                tf.math.log(y_pred + keras.backend.epsilon()), axis=1
            )
            loss = -tf.reduce_mean(drewards * log_probs)
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.pg_metric(loss)

        return abs_loss

    def _train(self, states, next_states, actions, terminals,
               rewards, drewards, epochs, batch_size, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A list of floats, which are the absolute losses for all
                the data
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type),
             drewards.astype(float_type))
        ).batch(batch_size)
        losses = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                if epoch == epochs:
                    losses.append(self._tf_train_step(*batch).numpy())
                else:
                    self._tf_train_step(*batch)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            dqn_loss_results = self.metric.result()
            self.metric.reset_states()
            pg_loss_results = self.pg_metric.result()
            self.pg_metric.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;dqn_loss: {dqn_loss_results} - &#39;
                      f&#39;pg_loss: {pg_loss_results}&#39;)
        return np.hstack(losses)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1,
              target_update_interval=1, tau=1.0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target qmodel weights are updated
            tau: A float, which is the strength of the copy from the
                 qmodel to the target qmodel (1.0 is a hard copy and
                 less is softer)
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.per_losses is None:
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards, self.drewards],
                    length
                )
            else:
                per_losses = self.per_losses.array()
                self.max_loss = per_losses.max()
                per_losses = per_losses / per_losses.sum()
                arrays, indexes = self.states.create_shuffled_subset(
                    [self.states, self.next_states, self.actions,
                     self.terminals, self.rewards, self.drewards],
                    length, weights=per_losses
                )

            std = arrays[-1].std()
            if std == 0:
                return False
            arrays[-1] = (arrays[-1] - arrays[-1].mean()) / std
            losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
            if self.per_losses is not None:
                for ndx, loss in zip(indexes, losses):
                    self.per_losses[ndx] = loss

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def load(self, path, load_model=True, load_data=True, custom_objects=None):
        &#34;&#34;&#34;Loads a save from a folder.

        Args:
            path: A string, which is the path to a folder to load
            load_model: A boolean, which determines if the model
                        architectures and weights
                        should be loaded
            load_data: A boolean, which determines if the memory
                       from a folder should be loaded
            custom_objects: A dictionary mapping to custom classes
                            or functions for loading the model

        Returns:
            A string of note.txt
        &#34;&#34;&#34;
        note = DQNAgent.load(
            self, path, load_model=load_model, load_data=load_data)
        if load_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
                self.amodel = model_from_json(
                    file.read(), custom_objects=custom_objects
                )
            self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return note

    def save(self, path, save_model=True,
             save_data=True, note=&#39;DQNPGAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = DQNAgent.save(self, path, save_model=save_model,
                             save_data=save_data, note=note)
        if save_model:
            with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
                file.write(self.amodel.to_json())
            self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.DQNAgent" href="reinforcement.html#paiutils.reinforcement.DQNAgent">DQNAgent</a></li>
<li><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float/integer, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action
        reward: A float/integer, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
    &#34;&#34;&#34;
    DQNAgent.add_memory(self, state, action, new_state, reward, terminal)
    self.episode_rewards.append(reward)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.end_episode"><code class="name flex">
<span>def <span class="ident">end_episode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Ends the episode, and creates drewards based
on the episodes rewards.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_episode(self):
    &#34;&#34;&#34;Ends the episode, and creates drewards based
       on the episodes rewards.
    &#34;&#34;&#34;
    PGAgent.end_episode(self)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a sample of its experiences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target qmodel weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
qmodel to the target qmodel (1.0 is a hard copy and
less is softer)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, batch_size=None, mini_batch=0,
          epochs=1, repeat=1,
          target_update_interval=1, tau=1.0, verbose=True):
    &#34;&#34;&#34;Trains the agent on a sample of its experiences.

    Args:
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target qmodel weights are updated
        tau: A float, which is the strength of the copy from the
             qmodel to the target qmodel (1.0 is a hard copy and
             less is softer)
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    self.total_steps += 1
    if batch_size is None:
        batch_size = len(self.states)
    if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
        length = mini_batch
    else:
        length = len(self.states)

    for count in range(1, repeat+1):
        if verbose:
            print(f&#39;Repeat {count}/{repeat}&#39;)

        if self.per_losses is None:
            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards, self.drewards],
                length
            )
        else:
            per_losses = self.per_losses.array()
            self.max_loss = per_losses.max()
            per_losses = per_losses / per_losses.sum()
            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards, self.drewards],
                length, weights=per_losses
            )

        std = arrays[-1].std()
        if std == 0:
            return False
        arrays[-1] = (arrays[-1] - arrays[-1].mean()) / std
        losses = self._train(*arrays, epochs, batch_size, verbose=verbose)
        if self.per_losses is not None:
            for ndx, loss in zip(indexes, losses):
                self.per_losses[ndx] = loss

        if (self.enable_target
                and self.total_steps % target_update_interval == 0):
            self.update_target(tau)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path, load_model=True, load_data=True, custom_objects=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a save from a folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to load</dd>
<dt><strong><code>load_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be loaded</dd>
<dt><strong><code>load_data</code></strong></dt>
<dd>A boolean, which determines if the memory
from a folder should be loaded</dd>
<dt><strong><code>custom_objects</code></strong></dt>
<dd>A dictionary mapping to custom classes
or functions for loading the model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string of note.txt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path, load_model=True, load_data=True, custom_objects=None):
    &#34;&#34;&#34;Loads a save from a folder.

    Args:
        path: A string, which is the path to a folder to load
        load_model: A boolean, which determines if the model
                    architectures and weights
                    should be loaded
        load_data: A boolean, which determines if the memory
                   from a folder should be loaded
        custom_objects: A dictionary mapping to custom classes
                        or functions for loading the model

    Returns:
        A string of note.txt
    &#34;&#34;&#34;
    note = DQNAgent.load(
        self, path, load_model=load_model, load_data=load_data)
    if load_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;r&#39;) as file:
            self.amodel = model_from_json(
                file.read(), custom_objects=custom_objects
            )
        self.amodel.load_weights(os.path.join(path, &#39;aweights.h5&#39;))
    return note</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, save_model=True, save_data=True, note='DQNPGAgent Save')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a note, model weights, and memory to a new folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>A string, which is the path to a folder to save within</dd>
<dt><strong><code>save_model</code></strong></dt>
<dd>A boolean, which determines if the model
architectures and weights
should be saved</dd>
<dt><strong><code>save_data</code></strong></dt>
<dd>A boolean, which determines if the memory
should be saved</dd>
<dt><strong><code>note</code></strong></dt>
<dd>A string, which is a note to save in the folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A string, which is the complete path of the save</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, save_model=True,
         save_data=True, note=&#39;DQNPGAgent Save&#39;):
    &#34;&#34;&#34;Saves a note, model weights, and memory to a new folder.

    Args:
        path: A string, which is the path to a folder to save within
        save_model: A boolean, which determines if the model
                    architectures and weights
                    should be saved
        save_data: A boolean, which determines if the memory
                   should be saved
        note: A string, which is a note to save in the folder

    Returns:
        A string, which is the complete path of the save
    &#34;&#34;&#34;
    path = DQNAgent.save(self, path, save_model=save_model,
                         save_data=save_data, note=note)
    if save_model:
        with open(os.path.join(path, &#39;amodel.json&#39;), &#39;w&#39;) as file:
            file.write(self.amodel.to_json())
        self.amodel.save_weights(os.path.join(path, &#39;aweights.h5&#39;))
    return path</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.use_dqn"><code class="name flex">
<span>def <span class="ident">use_dqn</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_dqn(self):
    self.uses_dqn_method = True</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.DQNPGAgent.use_pg"><code class="name flex">
<span>def <span class="ident">use_pg</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_pg(self):
    self.uses_dqn_method = False</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.DQNAgent" href="reinforcement.html#paiutils.reinforcement.DQNAgent">DQNAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.DQNAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.select_action" href="reinforcement.html#paiutils.reinforcement.DQNAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.set_playing_data" href="reinforcement.html#paiutils.reinforcement.DQNAgent.set_playing_data">set_playing_data</a></code></li>
<li><code><a title="paiutils.reinforcement.DQNAgent.update_target" href="reinforcement.html#paiutils.reinforcement.DQNAgent.update_target">update_target</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement_agents.PGCAgent"><code class="flex name class">
<span>class <span class="ident">PGCAgent</span></span>
<span>(</span><span>amodel, discounted_rate, create_memory=&lt;function PGCAgent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is a continuous action space variant of the PGAgent.</p>
<p>Initalizes the Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PGCAgent(PGAgent, Continuous):
    &#34;&#34;&#34;This class is a continuous action space variant of the PGAgent.
    &#34;&#34;&#34;

    def __init__(self, amodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        if len(amodel.output_shape) != 3:
            raise ValueError(&#39;The model must have three outputs: &#39;
                             &#39;mean, stddev, and actions&#39;)
        PGAgent.__init__(self, amodel, discounted_rate,
                         create_memory=create_memory)

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value or list of values, which is the action
                    the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        self.states.add(np.array(state))
        self.actions.add(action)
        self.episode_rewards.append(reward)

    def _train_step(self, states, drewards, actions, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            actions: A tensor that contains onehot encodings of
                     the action performed
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            locs, scales, _ = self.amodel(states, training=True)
            normal = tfp.distributions.Normal(locs, scales)
            probs = normal.prob(actions)
            log_probs = tf.math.log(probs + keras.backend.epsilon())
            loss = -tf.reduce_mean(drewards * log_probs)
            entropy = tf.reduce_mean(probs * log_probs)
            loss += entropy * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )
        self.metric(loss)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
<li><a title="paiutils.reinforcement_agents.Continuous" href="#paiutils.reinforcement_agents.Continuous">Continuous</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.PGCAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value or list of values, which is the action
the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action (discarded)</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode
(discarded)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value or list of values, which is the action
                the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action (discarded)
        reward: A float, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
                  (discarded)
    &#34;&#34;&#34;
    self.states.add(np.array(state))
    self.actions.add(action)
    self.episode_rewards.append(reward)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.PGAgent.end_episode" href="reinforcement.html#paiutils.reinforcement.PGAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.learn" href="reinforcement.html#paiutils.reinforcement.PGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.load" href="reinforcement.html#paiutils.reinforcement.PGAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.save" href="reinforcement.html#paiutils.reinforcement.PGAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.select_action" href="reinforcement.html#paiutils.reinforcement.PGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.PGAgent.set_playing_data" href="reinforcement.html#paiutils.reinforcement.PGAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement_agents.PPOAgent"><code class="flex name class">
<span>class <span class="ident">PPOAgent</span></span>
<span>(</span><span>amodel, cmodel, discounted_rate, lambda_rate=0, clip_ratio=0.2, create_memory=&lt;function PPOAgent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class (Proximal Policy Optimization) is like the A2CAgent
but attempts to avoid taking large gradient steps that would
collapse the performacne of the agent. (this is the clip variant)</p>
<p>Initalizes the Policy Gradient Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>cmodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
the value of that state</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>lambda_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which if nonzero will enable
generalized advantage estimation</dd>
<dt><strong><code>clip_ratio</code></strong></dt>
<dd>A float, which is the ratio to clip the differences
between new and old action probabilities</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PPOAgent(A2CAgent):
    &#34;&#34;&#34;This class (Proximal Policy Optimization) is like the A2CAgent
       but attempts to avoid taking large gradient steps that would
       collapse the performacne of the agent. (this is the clip variant)
    &#34;&#34;&#34;

    def __init__(self, amodel, cmodel, discounted_rate,
                 lambda_rate=0, clip_ratio=.2,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the Policy Gradient Agent.

        Args:
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state as input and outputs
                    the value of that state
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            lambda_rate: A float within 0.0-1.0, which if nonzero will enable
                         generalized advantage estimation
            clip_ratio: A float, which is the ratio to clip the differences
                        between new and old action probabilities
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        A2CAgent.__init__(self, amodel, cmodel, discounted_rate,
                          lambda_rate=lambda_rate,
                          create_memory=create_memory)
        self.clip_ratio = clip_ratio
        self.old_probs = create_memory((None,),
                                       keras.backend.floatx())
        self.memory[&#39;old_probs&#39;] = self.old_probs
        self.prob = None
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None, None),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()))
        )

    def select_action(self, state, training=False):
        &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
           suited for the given state.

        Args:
            state: A value, which is the state to predict
                   the action for
            training: A boolean, which determines if the
                      agent is training

        Returns:
            A value, which is the selected action
        &#34;&#34;&#34;
        if (self.time_distributed_states is not None
                and state.shape == self.amodel.input_shape[2:]):
            self.time_distributed_states = np.roll(
                self.time_distributed_states, -1
            )
            self.time_distributed_states[-1] = state
            state = self.time_distributed_states

        actions = self.amodel(np.expand_dims(state, axis=0),
                              training=False)[0].numpy()
        action = np.random.choice(np.arange(self.action_size),
                                  p=actions)
        self.prob = actions[action]
        return action

    def add_memory(self, state, action, new_state, reward, terminal):
        &#34;&#34;&#34;Adds information from one step in the environment to the agent.

        Args:
            state: A value or list of values, which is the
                   state of the environment before the
                   action was performed
            action: A value, which is the action the agent took
            new_state: A value or list of values, which is the
                       state of the environment after performing
                       the action (discarded)
            reward: A float/integer, which is the evaluation of
                    the action performed
            terminal: A boolean, which determines if this call to
                      add memory is the last for the episode
                      (discarded)
        &#34;&#34;&#34;
        A2CAgent.add_memory(self, state, action, new_state, reward, terminal)
        if self.prob is None:
            # actions = self.amodel(np.expand_dims(state, axis=0),
            #                       training=False)[0].numpy()
            # prob = actions[action]
            # self.old_probs.add(prob)

            # Assuming a uniform distribution
            self.old_probs.add(1 / self.action_size)
        else:
            self.old_probs.add(self.prob)
            self.prob = None

    def _train_step(self, states, drewards, advantages, actions,
                    old_probs, entropy_coef):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            drewards: A tensor that contains the discounted reward
                      for the action performed in the environment
            advantages: A tensor, which if valid (lambda_rate &gt; 0) contains
                        advantages for the actions performed
            actions: A tensor that contains onehot encodings of
                     the action performed
            old_probs: A tensor of the old probs
            entropy_coef: A tensor constant float, which is the
                          coefficent of entropy to add to the
                          actor loss

        Returns:
            A tensor of the new probs
        &#34;&#34;&#34;
        with tf.GradientTape() as tape:
            value_pred = tf.squeeze(self.cmodel(states, training=True))
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss = self.cmodel.compiled_loss._losses[0].fn(drewards, value_pred)
            loss = loss + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        with tf.GradientTape() as tape:
            action_pred = self.amodel(states, training=True)
            probs = tf.reduce_sum(actions * action_pred, axis=1)
            ratio = probs / (old_probs + keras.backend.epsilon())
            clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_ratio,
                                             1.0 + self.clip_ratio)
            if self.lambda_rate == 0:
                advantages = drewards - value_pred
            loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            entropy = tf.reduce_sum(
                action_pred *
                tf.math.log(action_pred + keras.backend.epsilon()),
                axis=1
            )
            loss += tf.reduce_mean(entropy) * entropy_coef
        grads = tape.gradient(loss, self.amodel.trainable_variables)
        self.amodel.optimizer.apply_gradients(
            zip(grads, self.amodel.trainable_variables)
        )

        self.metric(loss)
        return probs

    def _train(self, states, drewards, advantages, actions,
               old_probs, epochs, batch_size, entropy_coef,
               verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            drewards: A numpy array that contains the discounted reward
                      for the action performed in the environment
            advantages: A numpy array, which if valid (lambda_rate &gt; 0)
                        contains advantages for the actions performed
            actions: A numpy array that contains onehot encodings of
                     the action performed
            old_probs: A numpy array of the old probs
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A tuple of a float (mean critic loss of the batches) and
                a numpy ndarray of probs
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             drewards.astype(float_type),
             advantages.astype(float_type),
             actions.astype(float_type),
             old_probs.astype(float_type))
        ).batch(batch_size)
        entropy_coef = tf.constant(entropy_coef,
                                   dtype=float_type)
        new_probs = []
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            count = 0
            for batch in batches:
                if epoch == epochs:
                    new_probs.append(self._tf_train_step(*batch,
                                                         entropy_coef))
                else:
                    self._tf_train_step(*batch, entropy_coef)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            actor_loss_results = self.metric.result()
            critic_loss_results = self.metric_c.result()
            self.metric.reset_states()
            self.metric_c.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;actor_loss: {actor_loss_results} - &#39;
                      f&#39;critic_loss: {critic_loss_results}&#39;)
        return critic_loss_results, np.hstack(new_probs)

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, entropy_coef=0, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled)
            entropy_coef: A float, which is the coefficent of entropy to add
                          to the actor loss
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            if self.lambda_rate == 0:
                advantages_arr = np.empty(length)
            else:
                # cmodel predict on batches if large?
                values = tf.squeeze(
                    self.cmodel(self.states.array())
                ).numpy()
                advantages = np.empty(len(self.rewards))
                for ndx in reversed(range(len(self.rewards))):
                    delta = self.rewards[ndx] - values[ndx]
                    if not self.terminals[ndx]:
                        delta += self.discounted_rate * values[ndx + 1]
                    if self.terminals[ndx]:
                        advantage = 0
                    advantage = (delta + self.discounted_rate *
                                 self.lambda_rate * advantage)
                    advantages[ndx] = advantage

            arrays, indexes = self.states.create_shuffled_subset(
                [self.states, self.drewards, self.actions, self.old_probs],
                length
            )

            if self.lambda_rate == 0:
                arrays = [arrays[0], arrays[1], advantages_arr,
                          arrays[2], arrays[3]]
                std = arrays[1].std()
                if std == 0:
                    return False
                arrays[1] = (arrays[1] - arrays[1].mean()) / std
            else:
                arrays = [arrays[0], arrays[1], advantages[indexes],
                          arrays[2], arrays[3]]
                std = arrays[2].std()
                if std == 0:
                    return False
                arrays[2] = (arrays[2] - arrays[2].mean()) / std

            loss, new_probs = self._train(*arrays, epochs, batch_size,
                                          entropy_coef, verbose=verbose)
            for ndx in range(length):
                self.old_probs[indexes[ndx]] = new_probs[ndx]

    def save(self, path, save_model=True,
             save_data=True, note=&#39;PPOAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, models, weights, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        path = A2CAgent.save(self, path, save_model=save_model,
                             save_data=save_data, note=note)
        return path</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement_agents.A2CAgent" href="#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></li>
<li><a title="paiutils.reinforcement.PGAgent" href="reinforcement.html#paiutils.reinforcement.PGAgent">PGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.PPOAgent.add_memory"><code class="name flex">
<span>def <span class="ident">add_memory</span></span>(<span>self, state, action, new_state, reward, terminal)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds information from one step in the environment to the agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment before the
action was performed</dd>
<dt><strong><code>action</code></strong></dt>
<dd>A value, which is the action the agent took</dd>
<dt><strong><code>new_state</code></strong></dt>
<dd>A value or list of values, which is the
state of the environment after performing
the action (discarded)</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>A float/integer, which is the evaluation of
the action performed</dd>
<dt><strong><code>terminal</code></strong></dt>
<dd>A boolean, which determines if this call to
add memory is the last for the episode
(discarded)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_memory(self, state, action, new_state, reward, terminal):
    &#34;&#34;&#34;Adds information from one step in the environment to the agent.

    Args:
        state: A value or list of values, which is the
               state of the environment before the
               action was performed
        action: A value, which is the action the agent took
        new_state: A value or list of values, which is the
                   state of the environment after performing
                   the action (discarded)
        reward: A float/integer, which is the evaluation of
                the action performed
        terminal: A boolean, which determines if this call to
                  add memory is the last for the episode
                  (discarded)
    &#34;&#34;&#34;
    A2CAgent.add_memory(self, state, action, new_state, reward, terminal)
    if self.prob is None:
        # actions = self.amodel(np.expand_dims(state, axis=0),
        #                       training=False)[0].numpy()
        # prob = actions[action]
        # self.old_probs.add(prob)

        # Assuming a uniform distribution
        self.old_probs.add(1 / self.action_size)
    else:
        self.old_probs.add(self.prob)
        self.prob = None</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.PPOAgent.select_action"><code class="name flex">
<span>def <span class="ident">select_action</span></span>(<span>self, state, training=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the action the Agent "believes" to be
suited for the given state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>A value, which is the state to predict
the action for</dd>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the
agent is training</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A value, which is the selected action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_action(self, state, training=False):
    &#34;&#34;&#34;Returns the action the Agent &#34;believes&#34; to be
       suited for the given state.

    Args:
        state: A value, which is the state to predict
               the action for
        training: A boolean, which determines if the
                  agent is training

    Returns:
        A value, which is the selected action
    &#34;&#34;&#34;
    if (self.time_distributed_states is not None
            and state.shape == self.amodel.input_shape[2:]):
        self.time_distributed_states = np.roll(
            self.time_distributed_states, -1
        )
        self.time_distributed_states[-1] = state
        state = self.time_distributed_states

    actions = self.amodel(np.expand_dims(state, axis=0),
                          training=False)[0].numpy()
    action = np.random.choice(np.arange(self.action_size),
                              p=actions)
    self.prob = actions[action]
    return action</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement_agents.A2CAgent" href="#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.end_episode" href="#paiutils.reinforcement_agents.A2CAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.learn" href="reinforcement.html#paiutils.reinforcement.PGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.load" href="#paiutils.reinforcement_agents.A2CAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.save" href="#paiutils.reinforcement_agents.A2CAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.set_playing_data" href="reinforcement.html#paiutils.reinforcement.PGAgent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="paiutils.reinforcement_agents.TD3Agent"><code class="flex name class">
<span>class <span class="ident">TD3Agent</span></span>
<span>(</span><span>policy, amodel, cmodel, discounted_rate, create_memory=&lt;function TD3Agent.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This class (Twin Delayed DDPG Agent) attempts to mitigate
the problems that a DDPGAgent faces through clipping Q targets
between two Q models, delaying policy updates, and adding noise
to target actions.</p>
<p>Initalizes the DDPG Agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>policy</code></strong></dt>
<dd>A noise policy instance, which used for exploring</dd>
<dt><strong><code>amodel</code></strong></dt>
<dd>A keras model, which takes the state as input and outputs
actions (regularization losses are not applied,
and compiled loss are not used)</dd>
<dt><strong><code>cmodel</code></strong></dt>
<dd>A keras model, which takes the state and a action as input
and outputs two seperate Q Values (a judgement)</dd>
<dt><strong><code>discounted_rate</code></strong></dt>
<dd>A float within 0.0-1.0, which is the rate that
future rewards should be counted for the current
reward</dd>
<dt><strong><code>create_memory</code></strong></dt>
<dd>A function, which returns a Memory instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TD3Agent(DDPGAgent):
    &#34;&#34;&#34;This class (Twin Delayed DDPG Agent) attempts to mitigate
       the problems that a DDPGAgent faces through clipping Q targets
       between two Q models, delaying policy updates, and adding noise
       to target actions.
    &#34;&#34;&#34;

    def __init__(self, policy, amodel, cmodel, discounted_rate,
                 create_memory=lambda shape, dtype: Memory()):
        &#34;&#34;&#34;Initalizes the DDPG Agent.

        Args:
            policy: A noise policy instance, which used for exploring
            amodel: A keras model, which takes the state as input and outputs
                    actions (regularization losses are not applied,
                    and compiled loss are not used)
            cmodel: A keras model, which takes the state and a action as input
                     and outputs two seperate Q Values (a judgement)
            discounted_rate: A float within 0.0-1.0, which is the rate that
                             future rewards should be counted for the current
                             reward
            create_memory: A function, which returns a Memory instance
        &#34;&#34;&#34;
        DDPGAgent.__init__(self, policy, amodel, cmodel, discounted_rate,
                           create_memory=create_memory,
                           enable_target=True)
        coutput_shape = self.cmodel.output_shape
        if not isinstance(coutput_shape, list) or len(coutput_shape) != 2:
            raise ValueError(&#39;cmodel should have two outputs&#39;)
        self._tf_train_step = tf.function(
            self._train_step,
            input_signature=(tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.input_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=self.amodel.output_shape,
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(None,),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(),
                                           dtype=keras.backend.floatx()),
                             tf.TensorSpec(shape=(), dtype=tf.int32))
        )
        self.gradient_step_count = 0

    def set_playing_data(self, training=False, memorizing=False,
                         learns_in_episode=False, batch_size=None,
                         mini_batch=0, epochs=1, repeat=1,
                         target_update_interval=1, tau=1.0,
                         policy_noise_std=.2, policy_noise_clip=.5,
                         actor_update_infreq=2,
                         verbose=True):
        &#34;&#34;&#34;Sets the playing data.

        Args:
            training: A boolean, which determines if the agent
                      should be treated as in a training mode
            memorizing: A boolean, which determines if the agent
                        should be adding the information obtained
                        through playing an episode to memory
            learns_in_episode: A boolean, which determines if the agent
                               learns during a episode or at the end
            batch_size: An integer, which is the size of each batch
                        within the mini-batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and qvalues repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                               the actor is updated compared to the critic
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        learning_params = {&#39;batch_size&#39;: batch_size,
                           &#39;mini_batch&#39;: mini_batch,
                           &#39;epochs&#39;: epochs,
                           &#39;repeat&#39;: repeat,
                           &#39;target_update_interval&#39;: target_update_interval,
                           &#39;tau&#39;: tau,
                           &#39;policy_noise_std&#39;: policy_noise_std,
                           &#39;policy_noise_clip&#39;: policy_noise_clip,
                           &#39;actor_update_infreq&#39;: actor_update_infreq,
                           &#39;verbose&#39;: verbose}
        self.playing_data = PlayingData(training, memorizing, epochs,
                                        learns_in_episode, learning_params)

    def _train_step(self, states, next_states, actions, terminals,
                    rewards, policy_noise_std, policy_noise_clip,
                    actor_update):
        &#34;&#34;&#34;Performs one gradient step with a batch of data.

        Args:
            states: A tensor that contains environment states
            next_states: A tensor that contains the states of
                         the environment after an action was performed
            actions: A tensor that contains the actions performed
            terminals: A tensor that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A tensor that contains the reward for the action
                     performed in the environment
            policy_noise_std: A tensor constant float, which is the
                              standard deviation of the noise to add
                              to the target actions for gradient steps
            policy_noise_clip: A tensor constant float, which is the
                               min and max value of the normal noise
                               added to target actions for gradient steps
            actor_update: A tensor constant integer, which is determines
                          if the actor should update
        &#34;&#34;&#34;
        next_actions = self.target_amodel(next_states, training=False)
        noise = tf.random.normal(tf.shape(next_actions),
                                 stddev=policy_noise_std)
        noise = tf.clip_by_value(noise, -policy_noise_clip, policy_noise_clip)
        next_actions = tf.clip_by_value(next_actions + noise,
                                        *self.policy.action_bounds)
        next_qvalues1, next_qvalues2 = self.target_cmodel(
            [next_states, next_actions], training=False
        )
        next_qvalues = tf.squeeze(tf.minimum(next_qvalues1, next_qvalues2))
        qvalues_true = (rewards +
                        self.discounted_rate * next_qvalues * terminals)
        # Critic
        with tf.GradientTape() as tape:
            qvalues_pred1, qvalues_pred2 = self.cmodel(
                [states, actions], training=True
            )
            qvalues_pred1 = tf.squeeze(qvalues_pred1)
            qvalues_pred2 = tf.squeeze(qvalues_pred2)
            if len(self.cmodel.losses) &gt; 0:
                reg_loss = tf.math.add_n(self.cmodel.losses)
            else:
                reg_loss = 0
            loss1 = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred1
            )
            loss2 = self.cmodel.compiled_loss._losses[0].fn(
                qvalues_true, qvalues_pred2
            )
            loss = tf.reduce_mean(loss1) + tf.reduce_mean(loss2) + reg_loss
        grads = tape.gradient(loss, self.cmodel.trainable_variables)
        self.cmodel.optimizer.apply_gradients(
            zip(grads, self.cmodel.trainable_variables)
        )
        self.metric_c(loss)

        # Actor
        if actor_update == 0:
            with tf.GradientTape() as tape:
                action_preds = self.amodel(states, training=True)
                loss = -tf.reduce_mean(
                    self.cmodel([states, action_preds], training=False)
                )
            grads = tape.gradient(loss, self.amodel.trainable_variables)
            self.amodel.optimizer.apply_gradients(
                zip(grads, self.amodel.trainable_variables)
            )
            self.metric_a(loss)

    def _train(self, states, next_states, actions, terminals, rewards,
               epochs, batch_size, policy_noise_std, policy_noise_clip,
               actor_update_infreq, verbose=True):
        &#34;&#34;&#34;Performs multiple gradient steps of all the data.

        Args:
            states: A numpy array that contains environment states
            next_states: A numpy array that contains the states of
                         the environment after an action was performed
            actions: A numpy array that contains the actions performed
            terminals: A numpy array that contains ones for nonterminal
                       states and zeros for terminal states
            rewards: A numpy array that contains the reward for the action
                     performed in the environment
            epochs: An integer, which is the number of complete gradient
                    steps to perform
            batch_size: An integer, which is the size of the batch for
                        each partial gradient step
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                               the actor is updated compared to the critic
            verbose: A boolean, which determines if information should
                     be printed to the screen

        Returns:
            A float, which is the mean critic loss of the batches
        &#34;&#34;&#34;
        length = states.shape[0]
        float_type = keras.backend.floatx()
        batches = tf.data.Dataset.from_tensor_slices(
            (states.astype(float_type),
             next_states.astype(float_type),
             actions.astype(float_type),
             terminals.astype(float_type),
             rewards.astype(float_type))
        ).batch(batch_size)
        policy_noise_std = tf.constant(policy_noise_std,
                                       dtype=float_type)
        policy_noise_clip = tf.constant(policy_noise_clip,
                                        dtype=float_type)
        for epoch in range(1, epochs + 1):
            if verbose:
                print(f&#39;Epoch {epoch}/{epochs}&#39;)
            self.gradient_step_count += 1
            count = 0
            actor_update = self.gradient_step_count % actor_update_infreq
            actor_update = tf.constant(actor_update, dtype=tf.int32)
            for batch in batches:
                self._tf_train_step(*batch, policy_noise_std,
                                    policy_noise_clip,
                                    actor_update)
                count += np.minimum(batch_size, length - count)
                if verbose:
                    print(f&#39;{count}/{length}&#39;, end=&#39;\r&#39;)
            critic_loss_results = self.metric_c.result()
            actor_loss_results = self.metric_a.result()
            self.metric_c.reset_states()
            if actor_update == 1:
                self.metric_a.reset_states()
            if verbose:
                print(f&#39;{count}/{length} - &#39;
                      f&#39;critic_loss: {critic_loss_results} - &#39;
                      f&#39;actor_loss: {actor_loss_results}&#39;)
        return critic_loss_results

    def learn(self, batch_size=None, mini_batch=0,
              epochs=1, repeat=1, target_update_interval=1,
              tau=1.0, policy_noise_std=.2, policy_noise_clip=.5,
              actor_update_infreq=2, verbose=True):
        &#34;&#34;&#34;Trains the agent on a sample of its experiences.

        Args:
            batch_size: An integer, which is the size of each batch
                        within the mini_batch during one training instance
            mini_batch: An integer, which is the entire batch size for
                        one training instance
            epochs: An integer, which is the number of epochs to train
                    in one training instance
            repeat: An integer, which is the times to repeat a training
                    instance in one training instance (similar to epochs,
                    but mini_batch is resampled and predictions are
                    repredicted)
            target_update_interval: An integer, which is the number of
                                    complete training instances
                                    (repeats do not count) until the
                                    target critic model weights are updated
            tau: A float, which is the strength of the copy from the
                 Actor or Critic model to the target models
                 (1.0 is a hard copy and less is softer)
            policy_noise_std: A float, which is the standard deviation of the
                              noise to add to the target actions for gradient
                              steps
            policy_noise_clip: A float, which is the min and max value of
                               the normal noise added to target actions
                               for gradient steps
            actor_update_infreq: An integer, which is the infrequency that
                                 the actor is updated compared to the critic
            verbose: A boolean, which determines if training
                     should be verbose (print information to the screen)
        &#34;&#34;&#34;
        self.total_steps += 1
        if batch_size is None:
            batch_size = len(self.states)
        if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
            length = mini_batch
        else:
            length = len(self.states)

        for count in range(1, repeat+1):
            if verbose:
                print(f&#39;Repeat {count}/{repeat}&#39;)

            arrays, _ = self.states.create_shuffled_subset(
                [self.states, self.next_states, self.actions,
                 self.terminals, self.rewards],
                length
            )

            self._train(*arrays, epochs, batch_size,
                        policy_noise_std, policy_noise_clip,
                        actor_update_infreq, verbose=verbose)

            if (self.enable_target
                    and self.total_steps % target_update_interval == 0):
                self.update_target(tau)

    def save(self, path, save_model=True, save_data=True,
             note=&#39;T3DAgent Save&#39;):
        &#34;&#34;&#34;Saves a note, weights of the models, and memory to a new folder.

        Args:
            path: A string, which is the path to a folder to save within
            save_model: A boolean, which determines if the model
                        architectures and weights
                        should be saved
            save_data: A boolean, which determines if the memory
                       should be saved
            note: A string, which is a note to save in the folder

        Returns:
            A string, which is the complete path of the save
        &#34;&#34;&#34;
        return DDPGAgent.save(self, path, save_model=save_model,
                              save_data=save_data, note=note)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="paiutils.reinforcement.DDPGAgent" href="reinforcement.html#paiutils.reinforcement.DDPGAgent">DDPGAgent</a></li>
<li><a title="paiutils.reinforcement.MemoryAgent" href="reinforcement.html#paiutils.reinforcement.MemoryAgent">MemoryAgent</a></li>
<li><a title="paiutils.reinforcement.Agent" href="reinforcement.html#paiutils.reinforcement.Agent">Agent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="paiutils.reinforcement_agents.TD3Agent.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, policy_noise_std=0.2, policy_noise_clip=0.5, actor_update_infreq=2, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the agent on a sample of its experiences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini_batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and predictions are
repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target critic model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
Actor or Critic model to the target models
(1.0 is a hard copy and less is softer)</dd>
<dt><strong><code>policy_noise_std</code></strong></dt>
<dd>A float, which is the standard deviation of the
noise to add to the target actions for gradient
steps</dd>
<dt><strong><code>policy_noise_clip</code></strong></dt>
<dd>A float, which is the min and max value of
the normal noise added to target actions
for gradient steps</dd>
<dt><strong><code>actor_update_infreq</code></strong></dt>
<dd>An integer, which is the infrequency that
the actor is updated compared to the critic</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, batch_size=None, mini_batch=0,
          epochs=1, repeat=1, target_update_interval=1,
          tau=1.0, policy_noise_std=.2, policy_noise_clip=.5,
          actor_update_infreq=2, verbose=True):
    &#34;&#34;&#34;Trains the agent on a sample of its experiences.

    Args:
        batch_size: An integer, which is the size of each batch
                    within the mini_batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and predictions are
                repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target critic model weights are updated
        tau: A float, which is the strength of the copy from the
             Actor or Critic model to the target models
             (1.0 is a hard copy and less is softer)
        policy_noise_std: A float, which is the standard deviation of the
                          noise to add to the target actions for gradient
                          steps
        policy_noise_clip: A float, which is the min and max value of
                           the normal noise added to target actions
                           for gradient steps
        actor_update_infreq: An integer, which is the infrequency that
                             the actor is updated compared to the critic
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    self.total_steps += 1
    if batch_size is None:
        batch_size = len(self.states)
    if mini_batch &gt; 0 and len(self.states) &gt; mini_batch:
        length = mini_batch
    else:
        length = len(self.states)

    for count in range(1, repeat+1):
        if verbose:
            print(f&#39;Repeat {count}/{repeat}&#39;)

        arrays, _ = self.states.create_shuffled_subset(
            [self.states, self.next_states, self.actions,
             self.terminals, self.rewards],
            length
        )

        self._train(*arrays, epochs, batch_size,
                    policy_noise_std, policy_noise_clip,
                    actor_update_infreq, verbose=verbose)

        if (self.enable_target
                and self.total_steps % target_update_interval == 0):
            self.update_target(tau)</code></pre>
</details>
</dd>
<dt id="paiutils.reinforcement_agents.TD3Agent.set_playing_data"><code class="name flex">
<span>def <span class="ident">set_playing_data</span></span>(<span>self, training=False, memorizing=False, learns_in_episode=False, batch_size=None, mini_batch=0, epochs=1, repeat=1, target_update_interval=1, tau=1.0, policy_noise_std=0.2, policy_noise_clip=0.5, actor_update_infreq=2, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the playing data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training</code></strong></dt>
<dd>A boolean, which determines if the agent
should be treated as in a training mode</dd>
<dt><strong><code>memorizing</code></strong></dt>
<dd>A boolean, which determines if the agent
should be adding the information obtained
through playing an episode to memory</dd>
<dt><strong><code>learns_in_episode</code></strong></dt>
<dd>A boolean, which determines if the agent
learns during a episode or at the end</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>An integer, which is the size of each batch
within the mini-batch during one training instance</dd>
<dt><strong><code>mini_batch</code></strong></dt>
<dd>An integer, which is the entire batch size for
one training instance</dd>
<dt><strong><code>epochs</code></strong></dt>
<dd>An integer, which is the number of epochs to train
in one training instance</dd>
<dt><strong><code>repeat</code></strong></dt>
<dd>An integer, which is the times to repeat a training
instance in one training instance (similar to epochs,
but mini_batch is resampled and qvalues repredicted)</dd>
<dt><strong><code>target_update_interval</code></strong></dt>
<dd>An integer, which is the number of
complete training instances
(repeats do not count) until the
target critic model weights are updated</dd>
<dt><strong><code>tau</code></strong></dt>
<dd>A float, which is the strength of the copy from the
Actor or Critic model to the target models
(1.0 is a hard copy and less is softer)</dd>
<dt><strong><code>policy_noise_std</code></strong></dt>
<dd>A float, which is the standard deviation of the
noise to add to the target actions for gradient
steps</dd>
<dt><strong><code>policy_noise_clip</code></strong></dt>
<dd>A float, which is the min and max value of
the normal noise added to target actions
for gradient steps</dd>
<dt><strong><code>actor_update_infreq</code></strong></dt>
<dd>An integer, which is the infrequency that
the actor is updated compared to the critic</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>A boolean, which determines if training
should be verbose (print information to the screen)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_playing_data(self, training=False, memorizing=False,
                     learns_in_episode=False, batch_size=None,
                     mini_batch=0, epochs=1, repeat=1,
                     target_update_interval=1, tau=1.0,
                     policy_noise_std=.2, policy_noise_clip=.5,
                     actor_update_infreq=2,
                     verbose=True):
    &#34;&#34;&#34;Sets the playing data.

    Args:
        training: A boolean, which determines if the agent
                  should be treated as in a training mode
        memorizing: A boolean, which determines if the agent
                    should be adding the information obtained
                    through playing an episode to memory
        learns_in_episode: A boolean, which determines if the agent
                           learns during a episode or at the end
        batch_size: An integer, which is the size of each batch
                    within the mini-batch during one training instance
        mini_batch: An integer, which is the entire batch size for
                    one training instance
        epochs: An integer, which is the number of epochs to train
                in one training instance
        repeat: An integer, which is the times to repeat a training
                instance in one training instance (similar to epochs,
                but mini_batch is resampled and qvalues repredicted)
        target_update_interval: An integer, which is the number of
                                complete training instances
                                (repeats do not count) until the
                                target critic model weights are updated
        tau: A float, which is the strength of the copy from the
             Actor or Critic model to the target models
             (1.0 is a hard copy and less is softer)
        policy_noise_std: A float, which is the standard deviation of the
                          noise to add to the target actions for gradient
                          steps
        policy_noise_clip: A float, which is the min and max value of
                           the normal noise added to target actions
                           for gradient steps
        actor_update_infreq: An integer, which is the infrequency that
                           the actor is updated compared to the critic
        verbose: A boolean, which determines if training
                 should be verbose (print information to the screen)
    &#34;&#34;&#34;
    learning_params = {&#39;batch_size&#39;: batch_size,
                       &#39;mini_batch&#39;: mini_batch,
                       &#39;epochs&#39;: epochs,
                       &#39;repeat&#39;: repeat,
                       &#39;target_update_interval&#39;: target_update_interval,
                       &#39;tau&#39;: tau,
                       &#39;policy_noise_std&#39;: policy_noise_std,
                       &#39;policy_noise_clip&#39;: policy_noise_clip,
                       &#39;actor_update_infreq&#39;: actor_update_infreq,
                       &#39;verbose&#39;: verbose}
    self.playing_data = PlayingData(training, memorizing, epochs,
                                    learns_in_episode, learning_params)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="paiutils.reinforcement.DDPGAgent" href="reinforcement.html#paiutils.reinforcement.DDPGAgent">DDPGAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="paiutils.reinforcement.DDPGAgent.add_memory" href="reinforcement.html#paiutils.reinforcement.DDPGAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.end_episode" href="reinforcement.html#paiutils.reinforcement.Agent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.forget" href="reinforcement.html#paiutils.reinforcement.Agent.forget">forget</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.load" href="reinforcement.html#paiutils.reinforcement.DDPGAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.save" href="reinforcement.html#paiutils.reinforcement.DDPGAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.select_action" href="reinforcement.html#paiutils.reinforcement.DDPGAgent.select_action">select_action</a></code></li>
<li><code><a title="paiutils.reinforcement.DDPGAgent.update_target" href="reinforcement.html#paiutils.reinforcement.DDPGAgent.update_target">update_target</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="paiutils" href="index.html">paiutils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="paiutils.reinforcement_agents.A2CAgent" href="#paiutils.reinforcement_agents.A2CAgent">A2CAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.add_memory" href="#paiutils.reinforcement_agents.A2CAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.end_episode" href="#paiutils.reinforcement_agents.A2CAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.load" href="#paiutils.reinforcement_agents.A2CAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.A2CAgent.save" href="#paiutils.reinforcement_agents.A2CAgent.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.A2CCAgent" href="#paiutils.reinforcement_agents.A2CCAgent">A2CCAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.A2CCAgent.add_memory" href="#paiutils.reinforcement_agents.A2CCAgent.add_memory">add_memory</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.Continuous" href="#paiutils.reinforcement_agents.Continuous">Continuous</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.Continuous.clip" href="#paiutils.reinforcement_agents.Continuous.clip">clip</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.Continuous.sample" href="#paiutils.reinforcement_agents.Continuous.sample">sample</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.Continuous.scale" href="#paiutils.reinforcement_agents.Continuous.scale">scale</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.DQNPGAgent" href="#paiutils.reinforcement_agents.DQNPGAgent">DQNPGAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.add_memory" href="#paiutils.reinforcement_agents.DQNPGAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.end_episode" href="#paiutils.reinforcement_agents.DQNPGAgent.end_episode">end_episode</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.learn" href="#paiutils.reinforcement_agents.DQNPGAgent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.load" href="#paiutils.reinforcement_agents.DQNPGAgent.load">load</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.save" href="#paiutils.reinforcement_agents.DQNPGAgent.save">save</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.use_dqn" href="#paiutils.reinforcement_agents.DQNPGAgent.use_dqn">use_dqn</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.DQNPGAgent.use_pg" href="#paiutils.reinforcement_agents.DQNPGAgent.use_pg">use_pg</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.PGCAgent" href="#paiutils.reinforcement_agents.PGCAgent">PGCAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.PGCAgent.add_memory" href="#paiutils.reinforcement_agents.PGCAgent.add_memory">add_memory</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.PPOAgent" href="#paiutils.reinforcement_agents.PPOAgent">PPOAgent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.PPOAgent.add_memory" href="#paiutils.reinforcement_agents.PPOAgent.add_memory">add_memory</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.PPOAgent.select_action" href="#paiutils.reinforcement_agents.PPOAgent.select_action">select_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="paiutils.reinforcement_agents.TD3Agent" href="#paiutils.reinforcement_agents.TD3Agent">TD3Agent</a></code></h4>
<ul class="">
<li><code><a title="paiutils.reinforcement_agents.TD3Agent.learn" href="#paiutils.reinforcement_agents.TD3Agent.learn">learn</a></code></li>
<li><code><a title="paiutils.reinforcement_agents.TD3Agent.set_playing_data" href="#paiutils.reinforcement_agents.TD3Agent.set_playing_data">set_playing_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>